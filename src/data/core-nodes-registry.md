# n8n Core Nodes Registry

Core nodes provide functionality such as logic, scheduling, or generic API calls. They can be actions or triggers and don't connect to specific external services.

## Core Nodes

### 1. Activation Trigger
**Status**: ⚠️ DEPRECATED (replaced by n8n Trigger and Workflow Trigger nodes)
**Category**: Core - Trigger
**Purpose**: Gets triggered when an event gets fired by n8n or a workflow

#### Parameters:
- **Events** (3 options):
  - **Activation**: Run when the workflow gets activated
  - **Start**: Run when n8n starts or restarts
  - **Update**: Run when the workflow gets saved while it's active

#### Usage Notes:
- Gets triggered for the workflow that it gets added to
- Can be used to trigger a workflow to notify the state of the workflow
- Add to the workflow you want to trigger (don't create separate workflow)

#### Migration:
- Use n8n Trigger node and Workflow Trigger node instead

---

### 2. Aggregate
**Status**: ✅ Active
**Category**: Core - Data Processing
**Purpose**: Take separate items, or portions of them, and group them together into individual items

#### Operation Types:
**1. Individual Fields**: Aggregate individual fields separately
- **Input Field Name**: Name of field in input data to aggregate
- **Rename Field**: Toggle to give field different name in output
  - **Output Field Name**: New field name for aggregated output data (required if renaming)

**2. All Item Data**: Aggregate all item data into a single list
- **Put Output in Field**: Name of field to output data in
- **Include**: Which fields to include (3 options):
  - **All fields**: Include all fields with no further parameters
  - **Specified Fields**: Include only listed fields
    - **Fields To Include**: Comma-separated list of fields to include
  - **All Fields Except**: Include all except listed fields
    - **Fields To Exclude**: Comma-separated list of fields to exclude

#### Node Options:
- **Disable Dot Notation** (Individual Fields only): Disallow referencing child fields using `parent.child` syntax
- **Merge Lists** (Individual Fields only): Output single flat list rather than list of lists when field is a list
- **Include Binaries** (Both types): Include binary data from input in output
- **Keep Missing And Null Values** (Individual Fields only): Add null entries for missing values instead of ignoring them

#### Use Cases:
- Group separate data items together
- Combine field data from multiple items
- Create lists from individual field values
- Aggregate data for reporting or analysis

---

### 3. AI Transform
**Status**: ✅ Active (Cloud plans only)
**Category**: Core - AI Processing
**Purpose**: Generate code snippets based on your prompt. The AI is context-aware, understanding the workflow's nodes and their data types.

#### Parameters:
**1. Instructions**
- Enter your prompt for the AI in plain English (under 500 characters)
- Click "Generate code" button to automatically populate the Transformation Code
- Specify how you want to process or categorize your data

**2. Transformation Code**
- Code snippet generated by the node (read-only)
- To edit: adjust your prompt in Instructions or copy/paste into a Code node

#### Requirements:
- **Feature availability**: Only available on Cloud plans

#### Use Cases:
- Transform data based on natural language instructions
- Generate custom data processing logic
- Categorize and process data with AI assistance
- Context-aware code generation for workflows

#### Usage Notes:
- AI understands workflow context and data types
- Generated code is read-only in this node
- For editing, use Code node instead

---

### 4. Code
**Status**: ✅ Active
**Category**: Core - Development
**Purpose**: Write custom JavaScript or Python and run it as a step in your workflow

#### Execution Modes:
**1. Run Once for All Items** (default)
- Code executes once regardless of input item count
- Process all items together

**2. Run Once for Each Item**
- Code runs for every input item individually
- Process items one by one

#### Language Support:

**JavaScript (Node.js)**
- **Supported Features**: Promises, console.log debugging
- **External Libraries**:
  - Self-hosted: Can import built-in and external npm modules (requires configuration)
  - Cloud: Limited to crypto (Node.js) and moment (npm) modules only
- **Built-in Methods**: Access via `$variableName` or `$methodName()` syntax

**Python** 
- **Implementation**: Uses Pyodide (CPython to WebAssembly port)
- **Performance**: Slower than JavaScript due to compilation steps
- **Packages**: Limited to packages included with Pyodide
- **Built-in Methods**: Access via `_variableName` or `_methodName()` syntax

#### AI Features:
- **Ask AI tab**: Generate code using ChatGPT (Cloud only)
- **JavaScript only**: AI assistance limited to JavaScript
- **Overwrites existing code**: AI generated code replaces current code

#### Restrictions:
- **No file system access**: Use Read/Write File From Disk node instead
- **No HTTP requests**: Use HTTP Request node instead

#### Key Concepts:
- Data structure understanding required
- Item linking when input/output counts don't match
- Built-in methods for workflow, execution, and environment data

#### Use Cases:
- Custom data transformation
- Complex business logic
- Data validation and processing
- Integration between different data formats

#### Additional Resources:
- Keyboard shortcuts available
- Common issues documentation
- Replaces legacy Function and Function Item nodes (v0.198.0+)

---

### 5. Compare Datasets
**Status**: ✅ Active
**Category**: Core - Data Processing
**Purpose**: Compare data from two input streams and identify differences/similarities

#### Core Parameters:
**1. Field Matching**
- **Input A Field**: Field name from first input stream to compare
- **Input B Field**: Field name from second input stream to compare
- **Add Fields to Match**: Optional multiple field comparisons

**2. Difference Handling** (When There Are Differences):
- **Use Input A Version**: Treat input A as source of truth
- **Use Input B Version**: Treat input B as source of truth
- **Use a Mix of Versions**: 
  - **Prefer**: Choose Input A or Input B as main source
  - **For Everything Except**: Comma-separated list of fields to pull from other source
- **Include Both Versions**: Include both inputs in output (more complex structure)

**3. Fuzzy Compare**
- When enabled: Tolerates small type differences (number 3 = string "3")
- When disabled: Strict type matching required

#### Comparison Process:
1. **Field Match Check**: Verifies selected fields match across inputs
2. **Full Item Compare**: If fields match, compares all fields within items

#### Node Options:
**Fields to Skip Comparing**
- Enter field names to ignore during comparison
- Example: Skip `person.name` to focus only on other fields

**Disable Dot Notation**
- Toggle: Allow/disallow `parent.child` field referencing
- Default: Enabled (dot notation allowed)

**Multiple Matches**
- **Include All Matches** (default): Return all duplicate matches
- **Include First Match Only**: Return only first occurrence of duplicates

#### Output Branches:
- **In A only Branch**: Data that exists only in first input
- **Same Branch**: Data that's identical in both inputs  
- **Different Branch**: Data that differs between inputs
- **In B only Branch**: Data that exists only in second input

#### Use Cases:
- Data synchronization between systems
- Identifying changes in datasets
- Two-way sync operations
- Data validation and quality checks
- Merge conflict resolution

#### Advanced Features:
- Multi-field comparison support
- Flexible difference resolution strategies
- Duplicate handling options
- Field exclusion capabilities

---

### 6. Compression
**Status**: ✅ Active  
**Category**: Core - Utilities
**Purpose**: Compress and decompress files. Supports Zip and Gzip formats.

#### AI Tool Capability:
- Can be used to enhance AI agent capabilities
- Many parameters can be set automatically or with AI direction

#### Operations:

**1. Compress**: Create a compressed file from input data
- **Input Binary Field(s)**: Names of fields containing binary files to compress (comma-separated for multiple files)
- **Output Format**: Choose between:
  - **Zip**: Standard zip compression
  - **Gzip**: Gzip compression format
- **File Name**: Name of the compressed file to create
- **Put Output File in Field**: Field name in output data to contain the compressed file

**2. Decompress**: Decompress an existing compressed file
- **Put Output File in Field**: Names of fields containing binary files to decompress (comma-separated for multiple)
- **Output Prefix**: Prefix to add to output file names

#### Use Cases:
- File compression for storage efficiency
- Batch file archiving
- Data transfer optimization
- Backup file creation
- Multi-file packaging

#### Templates Available:
- SQLite database with LangChain AI Agent
- Bank statement transcription with Gemini Vision AI
- Tax code assistant with Qdrant and Mistral.ai

---

### 7. Chat Trigger
**Status**: ✅ Active
**Category**: Core - Trigger/AI
**Purpose**: Building AI workflows for chatbots and other chat interfaces with configurable access methods and authentication

#### Key Requirements:
- Must connect either an agent or chain root node
- Every message to Chat Trigger executes workflow (affects execution allowance)
- Replaces Manual Chat Trigger node from version 1.24.0

#### Core Parameters:

**1. Make Chat Publicly Available**
- Toggle to make chat publicly available (on) or only through manual chat interface (off)
- Keep off while building, turn on when ready for users

**2. Mode** - Choose user access method:
- **Hosted Chat**: Use n8n's hosted chat interface (recommended)
  - Configurable via node options
  - No additional setup required
- **Embedded Chat**: Create your own chat interface
  - Requires custom development
  - Can use n8n's chat widget (@n8n/chat npm package)
  - Must call webhook URL shown in Chat URL

**3. Authentication** - Access restriction options:
- **None**: No authentication, anyone can use chat
- **Basic Auth**: Username/password authentication
  - Select/create credential for Basic Auth
  - All users share same credentials
- **n8n User Auth**: Only logged-in n8n users can access

**4. Initial Message(s)** (Hosted Chat only)
- Configure welcome message displayed when user arrives

#### Node Options:

**Hosted Chat Options:**
- **Allowed Origin (CORS)**: Set origins for cross-origin requests (default: "*" for all)
- **Input Placeholder, Title, and Subtitle**: Customize chat interface text
- **Load Previous Session**: Load messages from previous sessions
  - Requires memory sub-node connection for Chat trigger and Agent
- **Response Mode**: 
  - *When Last Node Finishes*: Returns response from last executed node
  - *Using 'Respond to Webhook' Node*: Uses Respond to Webhook node response
- **Require Button Click to Start Chat**: Show/hide "New Conversation" button

**Embedded Chat Options:**
- **Allowed Origin (CORS)**: Set cross-origin request permissions
- **Load Previous Session**: Previous session loading capability
- **Response Mode**: Same options as Hosted Chat

#### Advanced Features:

**Manual Response Configuration:**
- Create parameter named `text` or `output` for custom responses
- Without proper naming, entire object is sent as response
- Useful when modifying Agent/Chain output before sending to user

**Memory Integration:**
- When Load Previous Session is enabled, connect memory sub-node
- Recommend same memory sub-node for both Chat trigger and Agent
- Ensures single source of truth for both nodes

#### Use Cases:
- AI-powered customer support chatbots
- Interactive AI assistants
- Conversational workflow interfaces
- Public or private chat applications
- Embedded chat widgets in applications

#### Important Notes:
- Each user message = one workflow execution
- 10 messages = 10 executions from allowance
- Check payment plan for execution limits
- Agent/Chain nodes output `output` or `text` parameter by default

#### Common Issues & Troubleshooting:

**1. Pass data from a website to an embedded Chat Trigger node**
- **Problem**: Need to pass extra information (like user ID from cookies) to embedded Chat Trigger
- **Solution**: Use `metadata` field in JSON object passed to `createChat` function:
  ```javascript
  createChat({
    webhookUrl: 'YOUR_PRODUCTION_WEBHOOK_URL',
    metadata: {
      'YOUR_KEY': 'YOUR_DATA'
    }
  });
  ```
- **Result**: Metadata appears in Chat Trigger output alongside other data for downstream processing

**2. Chat Trigger node doesn't fetch previous messages**
- **Problem**: "workflow could not be started!" error when trying to load previous sessions
- **Root Cause**: Improper Load Previous Session configuration
- **Best Practice Solution**:
  1. In Chat Trigger: Set "Load Previous Session" to "From Memory" (only visible when publicly available)
  2. Attach Simple Memory node to Chat Trigger's Memory connector
  3. Attach the *same* Simple Memory node to Agent's Memory connector  
  4. In Simple Memory node: Set "Session ID" to "Connected Chat Trigger Node"
- **Exception**: When using "Define below" for Session ID, may need separate memory nodes if expressions aren't compatible across nodes
- **Key Point**: Same expression must work for each node attached to memory

**General Troubleshooting Notes:**
- Memory connector appears when Load Previous Session is set to "From Memory"
- Response parameter naming critical for proper chat responses
- Each user message = one workflow execution (impacts usage allowance)

---

[Continue documenting remaining core nodes...]

## Index of Core Nodes

1. ✅ Activation Trigger (deprecated)
2. ✅ Aggregate  
3. ✅ AI Transform
4. ✅ Code
5. ✅ Compare Datasets
6. ✅ Compression
7. ✅ Chat Trigger
### 8. Convert to File
**Status**: ✅ Active
**Category**: Core - File Processing
**Purpose**: Take input data and output it as a file, converting JSON data into binary format

#### Operations (10 total):

**1. Convert to CSV**
- **Put Output File in Field**: Name of field to contain the file
- **Options**:
  - **File Name**: Generated output file name
  - **Header Row**: Toggle for header names in first row

**2. Convert to HTML**
- **Put Output File in Field**: Name of field to contain the file
- **Options**:
  - **File Name**: Generated output file name
  - **Header Row**: Toggle for header names in first row

**3. Convert to ICS** (Calendar format)
- **Put Output File in Field**: Name of field to contain the file
- **Event Title**: Title for the event
- **Start**: Event start date and time
- **End**: Event end date and time
- **All Day**: Whether event is all-day
- **Options**:
  - **File Name**: Generated output file name
  - **Attendees**: Name, Email, RSVP settings
  - **Busy Status**: Busy or Tentative (for Microsoft apps)
  - **Calendar Name**: For Apple and Microsoft calendars
  - **Description**: Event description
  - **Geolocation**: Latitude and Longitude
  - **Location**: Event venue/location
  - **Recurrence Rule**: RRULE for repeat patterns
  - **Organizer**: Name and Email
  - **Sequence**: Revision sequence number
  - **Status**: Confirmed, Cancelled, or Tentative
  - **UID**: Universally unique ID (auto-generated if not provided)
  - **URL**: Associated URL
  - **Use Workflow Timezone**: UTC vs workflow timezone

**4. Convert to JSON**
- **Mode Options**:
  - **All Items to One File**: Single file for all input items
  - **Each Item to Separate File**: Individual file per input item
- **Options**:
  - **File Name**: Generated output file name
  - **Format**: Format JSON for readability toggle
  - **Encoding**: Character set (default: utf8)

**5. Convert to ODS** (OpenDocument Spreadsheet)
- **Put Output File in Field**: Name of field to contain the file
- **Options**:
  - **File Name**: Generated output file name
  - **Compression**: Compress to reduce file size
  - **Header Row**: Toggle for header names
  - **Sheet Name**: Sheet name in spreadsheet

**6. Convert to RTF** (Rich Text Format)
- **Put Output File in Field**: Name of field to contain the file
- **Options**:
  - **File Name**: Generated output file name
  - **Header Row**: Toggle for header names

**7. Convert to Text File**
- **Text Input Field**: Name of field containing string to convert (supports dot-notation)
- **Options**:
  - **File Name**: Generated output file name
  - **Encoding**: Character set (default: utf8)

**8. Convert to XLS** (Excel 97-2003)
- **Put Output File in Field**: Name of field to contain the file
- **Options**:
  - **File Name**: Generated output file name
  - **Header Row**: Toggle for header names
  - **Sheet Name**: Sheet name in spreadsheet

**9. Convert to XLSX** (Excel 2007+)
- **Put Output File in Field**: Name of field to contain the file
- **Options**:
  - **File Name**: Generated output file name
  - **Compression**: Compress to reduce file size
  - **Header Row**: Toggle for header names
  - **Sheet Name**: Sheet name in spreadsheet

**10. Move Base64 String to File**
- **Base64 Input Field**: Field containing Base64 string (supports dot-notation)
- **Options**:
  - **File Name**: Generated output file name
  - **MIME Type**: MIME type of output file

#### Use Cases:
- Data export and file generation
- Report creation in multiple formats
- Calendar event file creation
- Document format conversion
- Binary data handling
- Email attachment preparation
- File format standardization

#### Templates Available:
- Automated Web Scraping with CSV/Excel export
- Telegram Messaging Agent for files
- Ultimate Scraper Workflow

#### Related Nodes:
- Extract From File (opposite operation)
- Read/Write Files from Disk
- Compression

---

8. Convert to File (to be documented)
### 9. Crypto
**Status**: ✅ Active
**Category**: Core - Security/Encryption
**Purpose**: Encrypt data in workflows using various cryptographic operations

#### AI Tool Capability:
- Can be used to enhance AI agent capabilities
- Many parameters can be set automatically or with AI direction

#### Actions (4 total):

**1. Generate** - Generate a random string
- **Property Name**: Name of property to write random string to
- **Type**: Encoding type options:
  - **ASCII**: Standard ASCII encoding
  - **BASE64**: Base64 encoding
  - **HEX**: Hexadecimal encoding
  - **UUID**: Universally unique identifier

**2. Hash** - Hash a text or file in specified format
- **Type**: Hash algorithm options:
  - **MD5**: MD5 hash algorithm
  - **SHA256**: SHA-256 hash algorithm
  - **SHA3-256**: SHA3-256 hash algorithm
  - **SHA3-384**: SHA3-384 hash algorithm
  - **SHA3-512**: SHA3-512 hash algorithm
  - **SHA385**: SHA-385 hash algorithm (likely SHA-384)
  - **SHA512**: SHA-512 hash algorithm
- **Binary File**: Toggle for binary file data
  - **Value**: Text value to hash (if not binary)
  - **Binary Property Name**: Binary property containing data (if binary)
- **Property Name**: Name of property to write hash to
- **Encoding**: Output encoding options:
  - **BASE64**: Base64 encoding
  - **HEX**: Hexadecimal encoding

**3. Hmac** - Hmac a text or file in specified format
- **Binary File**: Toggle for binary file data
  - **Value**: Text value to encrypt (if not binary)
  - **Binary Property Name**: Binary property containing data (if binary)
- **Type**: Encryption algorithm options:
  - **MD5**: MD5 HMAC
  - **SHA256**: SHA-256 HMAC
  - **SHA3-256**: SHA3-256 HMAC
  - **SHA3-384**: SHA3-384 HMAC
  - **SHA3-512**: SHA3-512 HMAC
  - **SHA385**: SHA-385 HMAC
  - **SHA512**: SHA-512 HMAC
- **Property Name**: Name of property to write hash to
- **Secret**: Secret or secret key for decoding
- **Encoding**: Output encoding options:
  - **BASE64**: Base64 encoding
  - **HEX**: Hexadecimal encoding

**4. Sign** - Sign a string using a private key
- **Value**: Value to sign
- **Property Name**: Name of property to write signed value to
- **Algorithm Name or ID**: Algorithm selection or expression-based ID
- **Encoding**: Output encoding options:
  - **BASE64**: Base64 encoding
  - **HEX**: Hexadecimal encoding
- **Private Key**: Private key for signing

#### Use Cases:
- Data encryption and security
- Password hashing and verification
- Digital signatures and authentication
- API request signing
- Token generation and validation
- Data integrity verification
- Secure random string generation

#### Templates Available:
- Conversational Interviews with AI Agents
- AI-Powered CoinMarketCap Data Analyst
- ChatGPT email reply with Google Sheets

#### Related Nodes:
- JWT (JSON Web Token operations)
- TOTP (Time-based One-Time Password)
- HTTP Request (for signed API requests)

---

9. Crypto (to be documented)
### 10. Date & Time
**Status**: ✅ Active
**Category**: Core - Date Processing
**Purpose**: Manipulate date and time data and convert it to different formats

#### Timezone Configuration:
- Uses workflow timezone (if set) or n8n instance timezone
- Default: America/New_York (self-hosted), auto-detect/GMT (Cloud)
- Configurable via Environment variables or Admin dashboard

#### Technology:
- Built on **Luxon** library
- Supports all Luxon date formats (case-sensitive tokens)
- Integrates with Code node and expressions

#### Operations (7 total):

**1. Add to a Date** - Add specified time to a date
- **Date to Add To**: Date to modify
- **Time Unit to Add**: Unit for duration (Year/Month/Week/Day/Hour/Minute/Second)
- **Duration**: Number of time units to add
- **Output Field Name**: Field name for result
- **Options**:
  - **Include Input Fields**: Toggle to include all input fields in output

**2. Extract Part of a Date** - Extract specific date component
- **Date**: Date to extract from
- **Part**: Component to extract:
  - **Year**: Extract year
  - **Month**: Extract month
  - **Week**: Extract week
  - **Day**: Extract day
  - **Hour**: Extract hour
  - **Minute**: Extract minute
  - **Second**: Extract second
- **Output Field Name**: Field name for extracted part
- **Options**:
  - **Include Input Fields**: Toggle to include all input fields in output

**3. Format a Date** - Transform date format using presets or custom expressions
- **Date**: Date to format
- **Format**: Output format options:
  - **Custom Format**: Custom Luxon format using special tokens
  - **MM/DD/YYYY**: US format (09/04/1986)
  - **YYYY/MM/DD**: ISO-like format (1986/09/04)
  - **MMMM DD YYYY**: Long format (September 04 1986)
  - **MM-DD-YYYY**: Dash-separated US (09-04-1986)
  - **YYYY-MM-DD**: ISO format (1986-09-04)
- **Output Field Name**: Field name for formatted date
- **Options**:
  - **Include Input Fields**: Toggle to include all input fields
  - **From Date Format**: Input format specification for parsing
  - **Use Workflow Timezone**: Use workflow vs input timezone

**4. Get Current Date** - Retrieve current date/time
- **Include Current Time**: Include time (on) or set to midnight (off)
- **Output Field Name**: Field name for current date
- **Options**:
  - **Include Input Fields**: Toggle to include all input fields
  - **Timezone**: Specific timezone (blank = instance default, GMT for +00:00)

**5. Get Time Between Dates** - Calculate time difference between dates
- **Start Date**: Earlier date for comparison
- **End Date**: Later date for comparison
- **Units**: Time units to calculate (multiple selection):
  - **Year/Month/Week/Day/Hour/Minute/Second/Millisecond**
- **Output Field Name**: Field name for time difference
- **Options**:
  - **Include Input Fields**: Toggle to include all input fields
  - **Output as ISO String**: Format as single ISO duration (P1Y3M13D) vs separate units

**ISO Duration Format**: `P<n>Y<n>M<n>DT<n>H<n>M<n>S`
- **P**: Period (duration prefix)
- **Y**: Years, **M**: Months, **W**: Weeks, **D**: Days
- **T**: Time delimiter (separates date/time)
- **H**: Hours, **M**: Minutes, **S**: Seconds
- Milliseconds: Decimal seconds (2.1ms = 0.0021S)

**6. Round a Date** - Round date up/down to nearest unit
- **Date**: Date to round
- **Mode**: Rounding direction:
  - **Round Down**: Floor to unit
  - **Round Up**: Ceiling to unit
- **To Nearest**: Target unit (Year/Month/Week/Day/Hour/Minute/Second)
- **Output Field Name**: Field name for rounded date
- **Options**:
  - **Include Input Fields**: Toggle to include all input fields

**7. Subtract From a Date** - Subtract specified time from date
- **Date to Subtract From**: Date to modify
- **Time Unit to Subtract**: Unit for duration
- **Duration**: Number of time units to subtract
- **Output Field Name**: Field name for result
- **Options**:
  - **Include Input Fields**: Toggle to include all input fields

#### Key Features:
- **Luxon Integration**: Full Luxon date library support
- **Timezone Awareness**: Configurable timezone handling
- **Flexible Formatting**: Custom and preset format options
- **Multiple Operations**: Comprehensive date manipulation suite
- **ISO Standards**: Support for ISO duration format

#### Use Cases:
- Date arithmetic and calculations
- Date format standardization
- Time zone conversions
- Duration calculations
- Date component extraction
- Workflow scheduling logic
- Time-based conditional processing

#### Templates Available:
- Working with dates and times
- RSS feed creation based on website content
- Monthly Financial Reports with Gemini AI

#### Related Resources:
- **Luxon Documentation**: Full token reference
- **Code Node**: Luxon available in JavaScript
- **Expressions**: Date/time expressions throughout n8n
- **Date and time with Luxon**: Comprehensive cookbook

#### Related Nodes:
- Schedule Trigger (time-based triggers)
- Wait (time-based delays)
- Code (custom date logic)

---

10. Date & Time (to be documented)
### 11. Debug Helper
**Status**: ✅ Active
**Category**: Core - Development/Testing
**Purpose**: Trigger different error types or generate random datasets to help test n8n workflows

#### Operations (4 total):

**1. Do Nothing** - Performs no operation (pass-through)
- No additional parameters
- Useful for workflow testing and placeholders

**2. Throw Error** - Trigger specific error types for testing error handling
- **Error Type**: Type of error to throw:
  - **NodeApiError**: API-related error type
  - **NodeOperationError**: Operation-specific error type  
  - **Error**: Generic error type
- **Error Message**: Custom error message text

**3. Out Of Memory** - Simulate memory exhaustion scenarios
- **Memory Size to Generate**: Approximate amount of memory to allocate
- Used for testing memory limits and out-of-memory error handling

**4. Generate Random Data** - Create test datasets in various formats
- **Data Type**: Type of random data to generate:
  - **Address**: Random postal addresses
  - **Coordinates**: Geographic latitude/longitude pairs
  - **Credit Card**: Credit card numbers (test format)
  - **Email**: Email addresses
  - **IPv4**: IPv4 addresses
  - **IPv6**: IPv6 addresses
  - **MAC**: MAC addresses
  - **Nanoids**: Custom nanoid generation
    - **Nanoid Alphabet**: Character set for nanoid generation
    - **Nanoid Length**: Length of each nanoid
  - **URL**: Random URLs
  - **User Data**: User profile information
  - **UUID**: Universally unique identifiers
  - **Version**: Version numbers
- **Seed**: Optional seed for consistent random generation (reproducible results)
- **Number of Items to Generate**: Quantity of random items to create
- **Output as Single Array**: Format output as:
  - **On**: Single array containing all items
  - **Off**: Multiple separate items

#### Key Features:
- **Error Testing**: Comprehensive error simulation capabilities
- **Memory Testing**: Out-of-memory condition simulation
- **Data Generation**: Multiple realistic data formats
- **Reproducible Results**: Seed-based consistent random generation
- **Flexible Output**: Array or individual item formats

#### Use Cases:
- **Workflow Testing**: Test error handling and edge cases
- **Load Testing**: Simulate memory constraints
- **Data Mocking**: Generate test datasets for development
- **Error Recovery**: Test workflow resilience
- **Performance Testing**: Stress test with large datasets
- **Development**: Create placeholder data during workflow building

#### Templates Available:
- Build an MCP Server with Google Calendar and Custom Functions
- Extract Domain and verify email syntax on the go
- Test Webhooks in n8n Without Changing WEBHOOK_URL (PostBin & BambooHR Example)

#### Related Nodes:
- No Operation, do nothing (similar pass-through functionality)
- Code (for custom test logic)
- Stop And Error (for intentional workflow termination)

---

11. Debug Helper (to be documented)
### 12. Edit Fields (Set)
**Status**: ✅ Active
**Category**: Core - Data Processing
**Purpose**: Set workflow data by creating new fields or overwriting existing data. Crucial for workflows that expect specific data formats for downstream nodes.

#### Core Parameters:

**1. Mode** - Choose data editing approach:
- **Manual Mapping**: GUI-based field editing with drag-and-drop interface
  - Drag values from INPUT panel to create fields
  - Default behavior: field name = value name, field value = expression accessing the value
  - **Fixed | Expressions** toggle: Switch between fixed values and expressions
  - Visual interface for field mapping
- **JSON Output**: Write JSON that gets added to input data
  - Direct JSON editing for complex data structures
  - Support for expressions within JSON
  - Arrays and nested objects supported

**2. Fields to Set** (Manual Mapping mode only)
- **Drag and Drop Interface**: Visual field mapping from INPUT panel
- **Field Name**: Set field names (supports expressions)
- **Field Value**: Set field values (supports expressions or fixed values)
- **Expression Toggle**: Switch between expression and fixed modes for names and values

**3. Keep Only Set Fields**
- **Toggle**: Discard unused input data vs include all input data
- **On**: Output contains only fields defined in "Fields to Set"
- **Off**: Output includes both set fields and original input data

**4. Include in Output** - Control input data inclusion:
- **All Input Fields**: Include all original input data plus new fields
- **No Input Fields**: Include only the fields you set
- **Selected Input Fields**: Choose specific input fields to include

#### Node Options:

**1. Include Binary Data**
- **Toggle**: Whether to include binary data from input in output
- **Important**: Binary data handling for file operations

**2. Ignore Type Conversion Errors** (Manual Mapping only)
- **Toggle**: Allow n8n to ignore data type errors during field mapping
- **Use Case**: Handle mixed data types gracefully

**3. Support Dot Notation**
- **Default**: Enabled (allows `parent.child` field referencing)
- **Enabled**: `number.one` creates nested structure `{"number": {"one": value}}`
- **Disabled**: `number.one` creates flat field `{"number.one": value}`

#### Advanced Features:

**JSON Output Mode Capabilities:**
- **Arrays**: Create arrays using `[value1, "value2", {{ $json.field }}]`
- **Nested Objects**: Create complex object structures
- **Expressions**: Use n8n expressions within JSON `{{ $json.fieldName }}`
- **Mixed Data Types**: Combine fixed values and dynamic expressions

**Example JSON Output:**
```json
{
  "newKey": "new value",
  "array": [{{ $json.id }}, "{{ $json.name }}"],
  "object": {
    "innerKey1": "new value",
    "innerKey2": "{{ $json.id }}",
    "innerKey3": "{{ $json.name }}"
  }
}
```

#### Key Features:
- **Visual Field Mapping**: Drag-and-drop interface for easy field creation
- **Flexible Data Modes**: Manual mapping or JSON output approaches
- **Expression Support**: Full n8n expression capabilities
- **Dot Notation Control**: Nested vs flat field structures
- **Data Filtering**: Control over which input data to preserve
- **Type Safety**: Optional type conversion error handling

#### Use Cases:
- **Data Format Preparation**: Structure data for downstream nodes (Google Sheets, databases)
- **Field Transformation**: Rename, restructure, or combine fields
- **Data Enrichment**: Add calculated or static fields to existing data
- **API Response Formatting**: Format data for API endpoints
- **Database Inserts**: Prepare data with required field names and types
- **Report Generation**: Structure data for reporting tools
- **Data Cleaning**: Remove unwanted fields and add processed fields

#### Common Patterns:

**1. Database Insert Preparation:**
- Map API response fields to database column names
- Add timestamps, user IDs, or calculated fields
- Remove unnecessary fields to reduce payload size

**2. API Response Formatting:**
- Transform internal data structure to API schema
- Add metadata fields (timestamps, request IDs)
- Standardize field naming conventions

**3. Data Aggregation:**
- Combine multiple fields into summary fields
- Create formatted display names from separate fields
- Calculate derived values from existing data

#### Templates Available:
- Creating an API endpoint
- Scrape and summarize webpages with AI
- Very quick quickstart

#### Integration Examples:
- **Google Sheets**: Set field names to match spreadsheet columns
- **Databases**: Map data to required table schema
- **Email**: Format data for email templates
- **Webhooks**: Structure response data
- **File Operations**: Prepare data for CSV/JSON export

#### Related Nodes:
- Rename Keys (field renaming only)
- Code (complex data transformations)
- JSON (JSON parsing and manipulation)
- Convert to File (data export)
- Merge (combining data from multiple sources)

#### Best Practices:
- Use Manual Mapping for simple field operations
- Use JSON Output for complex nested structures
- Enable "Keep Only Set Fields" to reduce data payload
- Use expressions for dynamic field values
- Test dot notation behavior with your data structure

---

12. Edit Fields (Set) (to be documented)
### 13. Edit Image
**Status**: ✅ Active
**Category**: Core - Image Processing
**Purpose**: Manipulate and edit images using GraphicsMagick operations

#### Dependencies:
- **GraphicsMagick**: Required if not running n8n on Docker
- **Data Source**: Need nodes like Read/Write Files from Disk or HTTP Request to pass image file as binary data property

#### Operations (13 total):

**1. Blur** - Add blur to reduce image sharpness
- **Property Name**: Name of binary property storing image data
- **Blur**: Blur strength (0-1000, higher = more blur)
- **Sigma**: Sigma value for blur (0-1000, higher = more blur)

**2. Border** - Add border around image
- **Property Name**: Name of binary property storing image data
- **Border Width**: Width of the border
- **Border Height**: Height of the border
- **Border Color**: Border color (hex or color picker)

**3. Composite** - Overlay one image on top of another
- **Property Name**: Base image binary property name
- **Composite Image Property**: Overlay image binary property name
- **Operator**: Composite blend mode options:
  - Add, Atop, Bumpmap, Copy, Copy Black/Blue/Cyan/Green/Magenta/Opacity/Red/Yellow
  - Difference, Divide, In, Minus, Multiply, Out, Over, Plus, Subtract, Xor
- **Position X**: Horizontal position of overlay image
- **Position Y**: Vertical position of overlay image

**4. Create** - Generate new blank image
- **Property Name**: Name for created image binary property
- **Background Color**: Background color (hex or color picker)
- **Image Width**: Width of new image
- **Image Height**: Height of new image

**5. Crop** - Extract portion of image
- **Property Name**: Name of binary property storing image data
- **Width**: Crop width
- **Height**: Crop height
- **Position X**: Horizontal start position for crop
- **Position Y**: Vertical start position for crop

**6. Draw** - Draw primitive shapes on image
- **Property Name**: Name of binary property storing image data
- **Primitive**: Shape to draw:
  - **Circle**: Draw circle
  - **Line**: Draw line
  - **Rectangle**: Draw rectangle
- **Color**: Shape color (hex or color picker)
- **Start Position X**: Horizontal start position
- **Start Position Y**: Vertical start position
- **End Position X**: Horizontal end position
- **End Position Y**: Vertical end position
- **Corner Radius**: Rounded corners for shapes

**7. Get Information** - Extract image metadata
- **Property Name**: Name of binary property storing image data
- Returns: Image dimensions, format, properties

**8. Multi Step** - Chain multiple operations together
- **Property Name**: Name of binary property storing image data
- **Operations**: List of operations to perform sequentially
- Can use any other Edit Image operations in sequence

**9. Resize** - Change image dimensions
- **Property Name**: Name of binary property storing image data
- **Width**: New width
- **Height**: New height
- **Option**: Resize behavior:
  - **Ignore Aspect Ratio**: Exact dimensions (may distort)
  - **Maximum Area**: Fit within bounds (maintain aspect ratio)
  - **Minimum Area**: Fill bounds (maintain aspect ratio)
  - **Only if Larger**: Resize only if image exceeds bounds
  - **Only if Smaller**: Resize only if image is smaller than bounds
  - **Percent**: Resize using width/height as percentages

**10. Rotate** - Rotate image by degrees
- **Property Name**: Name of binary property storing image data
- **Rotate**: Degrees to rotate (-360 to 360)
- **Background Color**: Fill color for rotation background (hex or color picker)
- Note: Background color used for non-90-degree rotations

**11. Shear** - Skew image along axes
- **Property Name**: Name of binary property storing image data
- **Degrees X**: Shear degrees along X-axis
- **Degrees Y**: Shear degrees along Y-axis

**12. Text** - Add text overlay to image
- **Property Name**: Name of binary property storing image data
- **Text**: Text content to add
- **Font Size**: Text font size
- **Font Color**: Text color (hex or color picker)
- **Position X**: Horizontal text position
- **Position Y**: Vertical text position
- **Max Line Length**: Characters per line before line break
- **Font Name or ID**: Font selection (dropdown or expression)

**13. Transparent** - Make specific color transparent
- **Property Name**: Name of binary property storing image data
- **Color**: Color to make transparent (hex or color picker)

#### Node Options:

**1. File Name**
- **Output filename**: Name of processed image file

**2. Format**
- **Output format**: Image format options:
  - **bmp**: Bitmap format
  - **gif**: GIF format
  - **jpeg**: JPEG format
  - **png**: PNG format
  - **tiff**: TIFF format
  - **WebP**: WebP format

#### Key Features:
- **GraphicsMagick Integration**: Professional-grade image processing
- **Multiple Operations**: 13 different image manipulation capabilities
- **Composite Support**: Complex image overlaying and blending
- **Format Flexibility**: Multiple input/output format support
- **Multi-Step Processing**: Chain operations for complex workflows
- **Precise Positioning**: Pixel-level control for overlays and crops

#### Technical Requirements:
- **Binary Data Input**: Images must be passed as binary properties
- **GraphicsMagick**: Dependency for image processing operations
- **Memory Usage**: Large images require significant processing memory

#### Use Cases:

**1. Image Processing Workflows:**
- Resize images for web optimization
- Add watermarks and branding
- Create thumbnails and previews
- Batch image processing

**2. Content Generation:**
- Generate social media graphics
- Add text overlays to images
- Create composite images from multiple sources
- Process user-uploaded images

**3. E-commerce Applications:**
- Product image standardization
- Create image variants and sizes
- Add price or promotional overlays
- Background removal and replacement

**4. Report and Document Generation:**
- Add charts and graphs to images
- Create visual reports
- Combine multiple images into layouts
- Add annotations and callouts

#### Common Patterns:

**1. Watermarking Workflow:**
- Load base image and watermark
- Use Composite operation to overlay watermark
- Position watermark in corner with transparency

**2. Social Media Image Generation:**
- Create base image with specific dimensions
- Add text overlays with branding
- Resize for different platform requirements

**3. Product Image Processing:**
- Standardize image sizes with Resize
- Add borders for consistency
- Crop to focus on product
- Convert to web-optimized format

#### Templates Available:
- Flux AI Image Generator
- Generate Instagram Content from Top Trends with AI Image Generation
- AI-Powered WhatsApp Chatbot for Text, Voice, Images & PDFs with memory

#### Integration Examples:
- **E-commerce**: Product image processing pipelines
- **Social Media**: Automated content generation
- **Content Management**: Image optimization workflows
- **Marketing**: Branded image creation

#### Related Nodes:
- Read/Write Files from Disk (image input/output)
- HTTP Request (image download)
- Convert to File (image format conversion)
- Compression (image file optimization)

#### Best Practices:
- Test memory requirements with large images
- Use appropriate formats for use case (PNG for transparency, JPEG for photos)
- Chain operations with Multi Step for complex processing
- Consider image quality vs file size trade-offs
- Use expressions for dynamic text and positioning

#### Error Handling:
- Ensure GraphicsMagick is properly installed
- Validate image formats are supported
- Handle memory limitations for large images
- Check binary property names match exactly

---

### 14. Email Trigger (IMAP)
**Status**: ✅ Active
**Category**: Core - Trigger/Email
**Purpose**: Receive emails using an IMAP email server. This is a trigger node that monitors email inboxes.

#### Authentication:
- **Credential Required**: IMAP credential (username/password, host, port, SSL/TLS settings)
- **Credential Setup**: [IMAP Credential Documentation](../../credentials/imap/)
- **Common Providers**: Gmail, Outlook, Yahoo, custom IMAP servers

#### Operations:
**1. Receive an Email** - Monitor and trigger on incoming emails

#### Node Parameters:

**1. Credential to connect with**
- **Required**: Select or create IMAP credential
- **Purpose**: Authenticate with IMAP email server
- **Setup**: Configure server settings, authentication method

**2. Mailbox Name**
- **Required**: Specify mailbox/folder to monitor
- **Default**: Usually "INBOX"
- **Examples**: "INBOX", "Sent", "Drafts", custom folder names
- **Case Sensitive**: Depends on email server configuration

**3. Action**
- **Purpose**: Control email read status after processing
- **Options**:
  - **None**: Leave emails marked as unread (default)
  - **Mark as Read**: Mark processed emails as read
- **Impact**: Prevents re-processing same emails

**4. Download Attachments**
- **Toggle**: Enable/disable attachment download
- **Performance Impact**: Increases processing time and memory usage
- **Recommendation**: Only enable when attachments are needed
- **Binary Data**: Attachments saved as binary data in workflow

**5. Format**
- **Purpose**: Choose email data format for processing
- **Options**:
  - **RAW**: Full email as base64url encoded string in raw field
    - **Use Case**: Need complete email headers and raw content
    - **Structure**: No payload field, all data in raw field
  - **Resolved**: Full email with all data resolved, attachments as binary
    - **Use Case**: Complete email processing with attachment handling
    - **Structure**: Parsed headers, body, attachments as binary data
  - **Simple**: Full email without inline attachment processing
    - **Use Case**: Basic email processing without complex attachments
    - **Limitation**: Don't use for inline attachments

#### Node Options:

**1. Custom Email Rules**
- **Purpose**: Advanced email filtering using IMAP search criteria
- **Reference**: [node-imap search function criteria](https://github.com/mscdex/node-imap)
- **Examples**:
  - `['UNSEEN']` - Only unread emails
  - `['FROM', 'sender@example.com']` - From specific sender
  - `['SUBJECT', 'Important']` - Subject contains "Important"
  - `['SINCE', 'May 20, 2023']` - Since specific date
- **Complex Rules**: Combine multiple criteria for advanced filtering

**2. Force Reconnect Every Minutes**
- **Purpose**: Prevent connection timeouts and ensure reliability
- **Default**: Connection maintained until timeout
- **Use Case**: Unstable connections, firewall limitations
- **Recommendation**: Set if experiencing connection issues

#### Key Features:
- **Real-time Monitoring**: Trigger workflows immediately on email receipt
- **Flexible Filtering**: Custom IMAP search rules for targeted email processing
- **Attachment Support**: Download and process email attachments as binary data
- **Multiple Formats**: Choose optimal data format for workflow needs
- **Connection Management**: Automatic reconnection and stability features

#### Use Cases:

**1. Email Automation:**
- Process customer support tickets
- Auto-categorize and route emails
- Extract data from structured emails
- Monitor specific senders or subjects

**2. Document Processing:**
- Process invoice emails with attachments
- Extract data from PDF attachments
- Save email attachments to cloud storage
- OCR processing of image attachments

**3. Notification Systems:**
- Trigger workflows on specific email patterns
- Forward important emails to team channels
- Create tasks from email requests
- Alert systems based on email content

**4. Data Integration:**
- Import data from email reports
- Process CSV attachments into databases
- Sync email data with CRM systems
- Archive important communications

#### Common Patterns:

**1. Customer Support Automation:**
- Monitor support mailbox with Custom Email Rules
- Extract ticket information from email content
- Download attachments for processing
- Create support tickets in external systems

**2. Invoice Processing:**
- Filter emails with "invoice" in subject
- Download PDF attachments using Resolved format
- Extract invoice data using OCR or parsing
- Update accounting systems

**3. Newsletter Processing:**
- Monitor specific sender addresses
- Extract content and links
- Process unsubscribe requests
- Archive newsletter content

#### Technical Details:

**IMAP Protocol Features:**
- **Real-time**: IDLE command support for instant notifications
- **Selective Sync**: Download only required email parts
- **Server-side Search**: Efficient filtering using IMAP search
- **Connection Persistence**: Maintain connection for continuous monitoring

**Performance Considerations:**
- **Memory Usage**: Attachments loaded into memory as binary data
- **Network Traffic**: Raw format requires full email download
- **Processing Time**: Attachment download increases execution time
- **Connection Limits**: IMAP servers may limit concurrent connections

#### Error Handling:

**Common Issues:**
- **Authentication Failures**: Check credential configuration
- **Connection Timeouts**: Use Force Reconnect option
- **Mailbox Not Found**: Verify mailbox name case-sensitivity
- **Memory Issues**: Limit attachment downloads for large files

**Troubleshooting:**
- Test credentials with email client first
- Check firewall and network connectivity
- Verify IMAP is enabled on email account
- Monitor connection logs for specific errors

#### Templates Available:
- **Effortless Email Management with AI-Powered Summarization & Review**
- **AI Email Analyzer: Process PDFs, Images & Save to Google Drive + Telegram**
- **A Very Simple "Human in the Loop" Email Response System Using AI and IMAP**

#### Integration Examples:
- **AI Processing**: Email content analysis and summarization
- **Document Management**: Attachment processing and storage
- **CRM Integration**: Customer communication tracking
- **Task Management**: Email-to-task conversion workflows

#### Related Nodes:
- Send Email (outbound email)
- HTTP Request (API integrations)
- Edit Fields (Set) (data formatting)
- Convert to File (attachment processing)
- Extract From File (attachment content extraction)

#### Best Practices:
- Use specific Custom Email Rules to reduce processing overhead
- Only enable Download Attachments when necessary
- Choose appropriate Format based on data requirements
- Implement error handling for connection issues
- Monitor memory usage with large attachments
- Test thoroughly with representative email samples

#### Security Considerations:
- Store IMAP credentials securely
- Use application-specific passwords when available
- Consider OAuth2 for supported providers
- Validate email content before processing
- Sanitize attachment filenames for security

---

### 15. Error Trigger
**Status**: ✅ Active
**Category**: Core - Trigger/Error Handling
**Purpose**: Create error workflows that run when another linked workflow fails. Provides detailed error information and enables automated error handling.

#### Authentication:
- **No credentials required**: Works with n8n's internal workflow execution system
- **Setup**: Link error workflow to main workflow through workflow settings

#### Operations:
**1. Error Detection** - Automatically triggers when linked workflow fails or encounters errors

#### Setup Process:

**Step 1: Create Error Workflow**
1. Create new workflow with Error Trigger as first node
2. Give workflow descriptive name (e.g., "Error Handler")
3. Save the workflow

**Step 2: Link to Main Workflow**
1. In the main workflow: Select **Options** > **Settings**
2. In **Error workflow** field: Select the error workflow you created
3. Save the main workflow
4. Now when main workflow errors, the error workflow runs automatically

#### Key Characteristics:

**1. Automatic Activation**
- Error workflows don't need to be manually activated
- They activate automatically when linked to main workflows

**2. Self-Referencing**
- Workflows containing Error Trigger nodes use themselves as error workflows by default
- Can override by setting different error workflow in settings

**3. Testing Limitations**
- Cannot test error workflows manually
- Error Trigger only runs when automatic workflows error
- Manual workflow errors don't trigger error workflows

#### Error Data Structures:

**1. Standard Execution Errors**
Default error data structure when workflow fails during execution:

```json
[
  {
    "execution": {
      "id": "231",
      "url": "https://n8n.example.com/execution/231",
      "retryOf": "34",
      "error": {
        "message": "Example Error Message",
        "stack": "Stacktrace"
      },
      "lastNodeExecuted": "Node With Error",
      "mode": "manual"
    },
    "workflow": {
      "id": "1",
      "name": "Example Workflow"
    }
  }
]
```

**2. Trigger Node Errors**
Different structure when error occurs in trigger node:

```json
{
  "trigger": {
    "error": {
      "context": {},
      "name": "WorkflowActivationError",
      "cause": {
        "message": "",
        "stack": ""
      },
      "timestamp": 1654609328787,
      "message": "",
      "node": {
        // Node details
      }
    },
    "mode": "trigger"
  },
  "workflow": {
    "id": "",
    "name": ""
  }
}
```

#### Error Data Fields:

**Execution Object:**
- **id**: Execution ID (only if execution saved to database)
- **url**: Direct link to execution (only if execution saved)
- **retryOf**: Present only when execution is a retry of failed execution
- **error.message**: Human-readable error description
- **error.stack**: Full error stack trace
- **lastNodeExecuted**: Name of node where error occurred
- **mode**: Execution mode (manual, automatic, webhook, etc.)

**Workflow Object:**
- **id**: Workflow identifier
- **name**: Workflow display name

**Conditional Fields:**
- **execution.id**: Missing if error in trigger node (workflow doesn't execute)
- **execution.url**: Missing if error in trigger node (workflow doesn't execute)
- **execution.retryOf**: Only present for retry executions

#### Use Cases:

**1. Error Notification Systems:**
- Send email/Slack alerts when workflows fail
- Include error details and workflow context
- Escalate critical errors to support teams
- Log errors to external monitoring systems

**2. Automated Error Recovery:**
- Retry failed operations with different parameters
- Switch to backup data sources or API endpoints
- Rollback transactions or data changes
- Implement fallback workflows

**3. Error Analysis and Reporting:**
- Collect error statistics and patterns
- Create error dashboards and reports
- Track workflow reliability metrics
- Identify recurring issues for optimization

**4. Operational Monitoring:**
- Monitor workflow health and performance
- Alert on workflow failures or timeouts
- Track error rates and resolution times
- Maintain workflow uptime statistics

#### Common Error Handling Patterns:

**1. Email Notification Pattern:**
```
Error Trigger → Edit Fields (Set) → Send Email
```
- Format error data for email
- Include workflow name, error message, execution link
- Send to operations team or workflow owner

**2. Slack Alert Pattern:**
```
Error Trigger → Edit Fields (Set) → Slack → HTTP Request
```
- Format error for Slack message
- Include workflow details and error context
- Optionally create support ticket

**3. Retry Pattern:**
```
Error Trigger → Wait → HTTP Request (retry original operation)
```
- Implement exponential backoff delays
- Retry with different parameters or endpoints
- Log retry attempts and outcomes

**4. Rollback Pattern:**
```
Error Trigger → Database Query → HTTP Request (undo operations)
```
- Identify incomplete transactions
- Reverse data changes
- Clean up temporary resources

#### Advanced Features:

**1. Error Classification:**
- Use error message patterns to categorize errors
- Route different error types to different handlers
- Implement priority-based error handling

**2. Error Context Enrichment:**
- Add environment information (prod/staging/dev)
- Include user context and request details
- Append workflow execution history

**3. Custom Error Messages:**
- Use Stop And Error node to send custom error data
- Include business-specific error codes
- Provide actionable error descriptions

#### Integration Examples:

**1. Production Monitoring:**
- PagerDuty integration for critical errors
- DataDog/New Relic for error metrics
- Jira for error ticket creation

**2. Communication Platforms:**
- Slack for team notifications
- Microsoft Teams for enterprise alerts
- Email for executive summaries

**3. Database Logging:**
- PostgreSQL for error history
- MongoDB for error document storage
- InfluxDB for time-series error metrics

#### Templates Available:
- Browse Error Trigger integration templates
- Search all templates for error handling patterns

#### Related Nodes:
- **Stop And Error**: Send custom error messages to Error Trigger
- **Send Email**: Error notification emails
- **Slack**: Team error notifications
- **HTTP Request**: API-based error reporting
- **Database nodes**: Error logging and tracking

#### Best Practices:

**1. Error Workflow Design:**
- Keep error workflows simple and reliable
- Avoid complex logic that might fail
- Test error workflows thoroughly
- Include fallback notification methods

**2. Error Data Processing:**
- Extract relevant error information only
- Format error messages for human readability
- Include enough context for debugging
- Avoid exposing sensitive information

**3. Notification Strategy:**
- Use appropriate urgency levels
- Avoid notification fatigue
- Include actionable information
- Provide clear next steps

**4. Error Recovery:**
- Implement intelligent retry logic
- Use exponential backoff for retries
- Set maximum retry limits
- Log all recovery attempts

#### Troubleshooting:

**Common Issues:**
- **Error workflow not triggering**: Check workflow settings configuration
- **Missing execution data**: Verify execution is saved to database
- **Incomplete error data**: Understand trigger vs execution error differences
- **Test failures**: Remember error workflows only work with automatic executions

**Debugging Tips:**
- Check workflow settings for proper error workflow assignment
- Verify error workflow is properly saved
- Test with actual failing workflows, not manual tests
- Monitor error workflow execution logs

#### Security Considerations:
- Sanitize error messages before external transmission
- Avoid exposing internal system details in error notifications
- Implement proper access controls for error data
- Use secure channels for error notifications

#### Performance Considerations:
- Keep error workflows lightweight and fast
- Avoid blocking operations in error handling
- Use async operations for external notifications
- Implement timeout handling for error workflows

---

13. Edit Image (to be documented)
14. Email Trigger (IMAP) (to be documented)
16. Evaluation (to be documented)
### 16. Evaluation
**Status**: ✅ Active
**Category**: Core - AI/Evaluation
**Purpose**: Perform various operations related to AI workflow evaluations to validate AI workflow reliability and performance

#### Authentication:
- **Google Sheets Credential**: Required for Set Outputs operation only
- **No credentials required**: For Set Metrics and Check If Evaluating operations

#### Operations (3 total):

**1. Set Outputs** - Write evaluation results back to Google Sheet dataset
**2. Set Metrics** - Record metrics scoring evaluation performance to n8n's Evaluations tab
**3. Check If Evaluating** - Branch workflow execution based on whether current execution is an evaluation

#### Operation Details:

**1. Set Outputs Operation**

**Purpose**: Write evaluation results back to Google Sheet dataset (typically same sheet used in Evaluation Trigger)

**Parameters**:
- **Credential to connect with**: Google Sheets credentials (required)
- **Document Containing Dataset**: Target spreadsheet selection
  - **From list**: Choose from dropdown of available spreadsheets
  - **By URL**: Enter complete spreadsheet URL
  - **By ID**: Enter spreadsheetId from URL
    - Format: `https://docs.google.com/spreadsheets/d/spreadsheetId/edit#gid=0`
- **Sheet Containing Dataset**: Target sheet selection within spreadsheet
  - **From list**: Choose from dropdown of available sheets
  - **By URL**: Enter complete sheet URL
  - **By ID**: Enter sheetId from URL
    - Format: `https://docs.google.com/spreadsheets/d/aBC-123_xYz/edit#gid=sheetId`
  - **By Name**: Enter sheet title directly

**Outputs Configuration**:
- **Name**: Google Sheet column name to write evaluation results to
- **Value**: Actual value to write to the specified column
- **Multiple Outputs**: Add multiple name/value pairs for different columns

**2. Set Metrics Operation**

**Purpose**: Record and track evaluation metrics that appear in workflow's Evaluations tab

**Parameters**:
- **Metrics to Return**: Section for defining metrics to record and track

**Metric Configuration**:
- **Name**: Display name for the metric in Evaluations tab
- **Value**: Numeric value to record (must be numeric)
  - Supports drag-and-drop values from previous nodes
  - Used for tracking evaluation performance over time

**3. Check If Evaluating Operation**

**Purpose**: Conditionally execute workflow logic based on evaluation status

**Features**:
- **No parameters required**: Simple boolean check operation
- **Branching output connectors**: Separate paths for evaluation vs normal execution
- **Conditional logic**: Enable different behavior during evaluations vs normal runs

#### Key Features:

**Evaluation Integration**:
- **Complete evaluation workflow support**: Works with Evaluation Trigger for full evaluation cycles
- **Google Sheets integration**: Direct write-back to evaluation datasets
- **Metrics tracking**: Built-in performance monitoring and reporting
- **Conditional execution**: Smart branching based on execution context

**Workflow Flexibility**:
- **Mixed execution modes**: Same workflow can handle both evaluations and normal operations
- **Results persistence**: Automatic logging to Google Sheets for analysis
- **Performance monitoring**: Track evaluation metrics over time
- **Quality assurance**: Validate AI workflow reliability systematically

#### Use Cases:

**1. AI Model Evaluation:**
- Test AI model performance with standardized datasets
- Record accuracy, precision, recall metrics
- Compare model versions over time
- Track evaluation results in structured format

**2. Workflow Quality Assurance:**
- Validate workflow reliability with test datasets
- Monitor performance degradation over time
- A/B test different workflow configurations
- Ensure consistent output quality

**3. Data Science Workflows:**
- Evaluate machine learning model performance
- Track experiment results and metrics
- Compare different algorithm approaches
- Document evaluation outcomes for reporting

**4. Content Quality Assessment:**
- Evaluate AI-generated content quality
- Score content against quality criteria
- Track improvement over time
- Maintain quality standards documentation

#### Common Evaluation Patterns:

**1. Complete Evaluation Cycle:**
```
Evaluation Trigger → [AI Processing Nodes] → Evaluation (Set Outputs) → Evaluation (Set Metrics)
```
- Load test data from Google Sheets
- Process through AI workflow
- Write results back to dataset
- Record performance metrics

**2. Conditional Evaluation Behavior:**
```
Manual Trigger → Evaluation (Check If Evaluating) → [Different logic paths]
```
- Use same workflow for evaluation and production
- Branch execution based on evaluation status
- Apply different processing logic as needed

**3. A/B Testing Pattern:**
```
Evaluation Trigger → Switch → [Multiple AI approaches] → Evaluation (Set Metrics)
```
- Test multiple AI model versions
- Compare performance metrics
- Identify best-performing approach

**4. Quality Monitoring:**
```
Schedule Trigger → [AI Processing] → Evaluation (Set Metrics)
```
- Regular quality checks on production data
- Monitor performance trends
- Alert on quality degradation

#### Integration Examples:

**1. AI Model Performance Testing:**
- Load test datasets from Google Sheets
- Process through OpenAI, Anthropic, or other AI nodes
- Score accuracy, relevance, coherence
- Track metrics in Evaluations tab

**2. Content Generation Quality:**
- Generate content with AI models
- Evaluate against quality criteria
- Record scores for different content types
- Optimize generation parameters

**3. Customer Support Automation:**
- Test automated response quality
- Evaluate against human-written responses
- Track response accuracy and helpfulness
- Improve automation over time

**4. Data Processing Validation:**
- Test data transformation accuracy
- Validate against known good results
- Track processing error rates
- Ensure data quality standards

#### Advanced Features:

**1. Multi-Metric Evaluation:**
- Record multiple metrics per evaluation run
- Track different quality dimensions
- Compare metrics across evaluation runs
- Identify improvement opportunities

**2. Dataset Management:**
- Use same Google Sheets for data input and output
- Maintain evaluation history
- Version control for evaluation datasets
- Collaborate on evaluation criteria

**3. Performance Tracking:**
- Historical metric trends in Evaluations tab
- Performance comparison over time
- Regression detection and alerting
- Continuous improvement monitoring

#### Technical Details:

**Google Sheets Integration:**
- **OAuth2 authentication**: Secure access to Google Sheets
- **Real-time writing**: Immediate results recording
- **Flexible column mapping**: Map to any sheet columns
- **Multiple sheet support**: Write to different sheets in same document

**Metrics System:**
- **Numeric values only**: Metrics must be numeric for proper tracking
- **Time-series data**: Historical tracking in Evaluations tab
- **Aggregation support**: View trends and averages
- **Export capabilities**: Download evaluation data

**Execution Context:**
- **Automatic detection**: Check If Evaluating automatically detects evaluation mode
- **Seamless integration**: No special configuration required
- **Performance impact**: Minimal overhead for evaluation detection

#### Templates Available:
- **AI Automated HR Workflow for CV Analysis and Candidate Evaluation**
- **HR Job Posting and Evaluation with AI**
- **AI-Powered Candidate Screening and Evaluation Workflow using OpenAI and Airtable**

#### Related Nodes:
- **Evaluation Trigger**: Starts evaluation workflows with test data
- **Google Sheets**: Data source and destination for evaluations
- **AI nodes**: OpenAI, Anthropic, etc. for AI model evaluation
- **Code**: Custom evaluation logic and scoring
- **Switch**: A/B testing different approaches

#### Best Practices:

**1. Evaluation Design:**
- Use consistent evaluation datasets
- Define clear evaluation criteria
- Record comprehensive metrics
- Document evaluation methodology

**2. Data Management:**
- Keep evaluation data organized in Google Sheets
- Use clear column naming conventions
- Version control evaluation datasets
- Backup evaluation results

**3. Metrics Selection:**
- Choose meaningful, actionable metrics
- Balance comprehensiveness with simplicity
- Track both accuracy and performance metrics
- Include business-relevant measures

**4. Workflow Organization:**
- Separate evaluation and production logic when needed
- Use Check If Evaluating for conditional behavior
- Keep evaluation workflows simple and reliable
- Test evaluation workflows thoroughly

#### Troubleshooting:

**Common Issues:**
- **Google Sheets authentication**: Ensure proper credential setup
- **Metric values**: Must be numeric, not text
- **Sheet/document not found**: Verify IDs and permissions
- **Permission errors**: Check Google Sheets sharing settings

**Debugging Tips:**
- Test Google Sheets connection independently
- Verify evaluation trigger setup
- Check numeric format for metrics
- Monitor Evaluations tab for results

#### Security Considerations:
- Secure Google Sheets credentials storage
- Limit evaluation data access appropriately
- Sanitize evaluation data before processing
- Use proper sharing permissions for collaborative evaluation

#### Performance Considerations:
- Minimize Google Sheets write operations
- Batch multiple outputs when possible
- Consider rate limits for Google Sheets API
- Monitor evaluation execution time

---

15. Error Trigger (to be documented)
### 17. Evaluation Trigger
**Status**: ✅ Active
**Category**: Core - Trigger/AI/Evaluation
**Purpose**: Start evaluation workflows that read test datasets from Google Sheets and send items through the workflow sequentially for AI workflow reliability validation

#### Authentication:
- **Google Sheets Credential**: Required for accessing evaluation datasets
- **Credential Setup**: [Google Sheets Credential Documentation](../../credentials/google/)
- **OAuth2 Support**: Secure authentication with Google Sheets API

#### Operations:
**1. Read Dataset** - Read evaluation data from Google Sheets and trigger workflow for each row

#### Core Parameters:

**1. Credential to connect with**
- **Required**: Google Sheets credentials
- **Purpose**: Authenticate with Google Sheets API
- **Setup**: Configure OAuth2 or service account credentials

**2. Document Containing Dataset**
- **Purpose**: Select spreadsheet containing evaluation test data
- **Selection Methods**:
  - **From list**: Choose from dropdown of available spreadsheets
  - **By URL**: Enter complete spreadsheet URL
  - **By ID**: Enter spreadsheetId from URL
    - **Format**: `https://docs.google.com/spreadsheets/d/spreadsheetId/edit#gid=0`
    - **Location**: Extract spreadsheetId from URL path

**3. Sheet Containing Dataset**
- **Purpose**: Select specific sheet within spreadsheet
- **Selection Methods**:
  - **From list**: Choose from dropdown of available sheets
  - **By URL**: Enter complete sheet URL
  - **By ID**: Enter sheetId from URL
    - **Format**: `https://docs.google.com/spreadsheets/d/aBC-123_xYz/edit#gid=sheetId`
    - **Location**: Extract sheetId from URL fragment
  - **By Name**: Enter sheet title directly

**4. Limit Rows**
- **Purpose**: Control dataset size for evaluation runs
- **Toggle**: Enable/disable row limiting
- **Max Rows to Process**: Maximum number of rows to read and process
  - **Use Case**: Test with subset of data during development
  - **Performance**: Reduce evaluation time for large datasets

#### Advanced Features:

**Dataset Filtering**
- **Purpose**: Filter evaluation dataset based on column values
- **Column Selection**: Choose sheet column for filtering
  - **From list**: Select from dropdown of available columns
  - **By ID**: Specify column using expressions
- **Value Matching**: Filter rows by specific column values
  - **Exact Match**: Only process rows with specified column value
  - **Use Case**: Run evaluations on specific data segments

#### Key Features:

**Sequential Processing**:
- **One-by-one execution**: Sends dataset items through workflow sequentially
- **Order preservation**: Maintains data order from Google Sheets
- **Individual executions**: Each row triggers separate workflow execution
- **Evaluation context**: Automatically sets evaluation mode for workflow

**Google Sheets Integration**:
- **Real-time access**: Reads current data from Google Sheets
- **Dynamic datasets**: Support for changing evaluation data
- **Column mapping**: Automatic field mapping from sheet headers
- **Large dataset support**: Handle extensive evaluation datasets efficiently

**Evaluation Workflow Setup**:
- **First node requirement**: Must be first node in evaluation workflows
- **Automatic activation**: Evaluation workflows don't need manual activation
- **Integration ready**: Works seamlessly with Evaluation node for results tracking

#### Use Cases:

**1. AI Model Testing:**
- Load standardized test datasets from Google Sheets
- Test AI model performance across various inputs
- Compare different model versions systematically
- Track evaluation results with consistent datasets

**2. Workflow Quality Assurance:**
- Validate workflow reliability with known test cases
- Regression testing for workflow changes
- Performance benchmarking over time
- A/B testing different workflow configurations

**3. Content Generation Evaluation:**
- Test AI content generation quality
- Evaluate against human-written examples
- Score content across multiple quality dimensions
- Optimize content generation parameters

**4. Data Processing Validation:**
- Verify data transformation accuracy
- Test against known good results
- Validate business logic implementations
- Ensure consistent processing quality

#### Common Evaluation Patterns:

**1. Complete AI Evaluation Workflow:**
```
Evaluation Trigger → [AI Processing Nodes] → Evaluation (Set Outputs) → Evaluation (Set Metrics)
```
- Load test data from Google Sheets
- Process through AI workflow
- Write results back to spreadsheet
- Track performance metrics

**2. A/B Testing Pattern:**
```
Evaluation Trigger → Switch → [Multiple AI Models] → Evaluation (Set Metrics)
```
- Test multiple AI approaches
- Compare performance metrics
- Identify best-performing configurations
- Data-driven model selection

**3. Quality Monitoring:**
```
Evaluation Trigger → [Processing Logic] → Evaluation (Check If Evaluating) → [Conditional Logic]
```
- Run same workflow for evaluation and production
- Branch logic based on evaluation mode
- Quality checks with evaluation datasets

**4. Regression Testing:**
```
Evaluation Trigger → [Updated Workflow] → Compare Datasets → Evaluation (Set Metrics)
```
- Test workflow changes against baseline
- Compare new results with expected outputs
- Detect performance regressions
- Validate workflow improvements

#### Dataset Management:

**1. Test Data Organization:**
- **Column Headers**: Use as field names in workflow
- **Data Types**: Support text, numbers, dates, JSON
- **Validation**: Ensure data quality and consistency
- **Versioning**: Track evaluation dataset changes

**2. Google Sheets Structure:**
- **Header Row**: First row contains field names
- **Data Rows**: Subsequent rows contain test cases
- **Filtering Columns**: Special columns for data segmentation
- **Results Columns**: Columns for writing evaluation outcomes

**3. Dataset Filtering Examples:**
- **Environment Filter**: `environment = "production"`
- **Test Type Filter**: `test_type = "regression"`
- **Priority Filter**: `priority = "high"`
- **Model Version**: `model_version = "v2.1"`

#### Integration Examples:

**1. OpenAI Model Evaluation:**
- Load prompts and expected responses from Google Sheets
- Process through OpenAI API
- Compare actual vs expected outputs
- Score accuracy and relevance

**2. Customer Support Automation:**
- Test automated response quality
- Evaluate against human-written responses
- Track response time and accuracy
- Improve automation over time

**3. Data Pipeline Testing:**
- Validate data transformation accuracy
- Test against known input/output pairs
- Monitor processing error rates
- Ensure data quality standards

**4. Content Moderation:**
- Test content classification accuracy
- Evaluate against manually labeled data
- Track false positive/negative rates
- Optimize moderation algorithms

#### Technical Details:

**Execution Behavior:**
- **Trigger Mode**: Starts evaluation workflows automatically
- **Sequential Processing**: Processes rows one at a time
- **Evaluation Context**: Sets workflow execution as evaluation mode
- **Memory Usage**: Optimized for large datasets

**Performance Considerations:**
- **API Rate Limits**: Respects Google Sheets API quotas
- **Large Datasets**: Efficient processing for thousands of rows
- **Network Optimization**: Minimizes API calls
- **Error Handling**: Graceful handling of sheet access issues

**Data Processing:**
- **Type Conversion**: Automatic conversion of sheet data types
- **Header Mapping**: Sheet headers become workflow field names
- **Empty Cells**: Handle missing data appropriately
- **Formula Values**: Reads calculated values, not formulas

#### Templates Available:
- **AI Automated HR Workflow for CV Analysis and Candidate Evaluation**
- **HR Job Posting and Evaluation with AI**
- **AI-Powered Candidate Screening and Evaluation Workflow using OpenAI and Airtable**

#### Related Nodes:
- **Evaluation**: Write results back and track metrics
- **Google Sheets**: Direct sheet operations
- **Compare Datasets**: Compare evaluation results
- **Switch**: A/B testing different approaches
- **Code**: Custom evaluation logic

#### Best Practices:

**1. Dataset Design:**
- Use clear, descriptive column headers
- Include expected outputs for comparison
- Organize test cases by priority or type
- Maintain dataset version control

**2. Evaluation Setup:**
- Start with small datasets during development
- Use row limiting for quick iterations
- Implement proper error handling
- Document evaluation criteria clearly

**3. Performance Optimization:**
- Limit rows for development testing
- Use filtering to focus on specific test cases
- Monitor Google Sheets API usage
- Batch related test cases when possible

**4. Quality Assurance:**
- Validate test data quality regularly
- Update datasets as requirements change
- Track evaluation history and trends
- Document evaluation methodology

#### Troubleshooting:

**Common Issues:**
- **Authentication Failures**: Check Google Sheets credential setup
- **Sheet Not Found**: Verify spreadsheet and sheet names/IDs
- **Permission Errors**: Ensure proper Google Sheets sharing permissions
- **Empty Results**: Check for proper column headers and data

**Debugging Tips:**
- Test Google Sheets access independently
- Verify spreadsheet/sheet IDs in URLs
- Check credential permissions and scopes
- Monitor workflow execution logs

**Error Handling:**
- Implement fallback for missing data
- Handle Google Sheets API rate limits
- Validate dataset structure before evaluation
- Log evaluation progress and errors

#### Security Considerations:
- Secure Google Sheets credentials storage
- Limit evaluation data access appropriately
- Use proper sharing permissions for collaborative evaluation
- Sanitize evaluation data before processing

#### Performance Considerations:
- Respect Google Sheets API rate limits
- Use row limiting for large datasets during development
- Monitor memory usage with extensive datasets
- Consider dataset size vs evaluation time trade-offs

#### Advanced Configuration:

**1. Dynamic Dataset Selection:**
- Use expressions for spreadsheet/sheet selection
- Environment-based dataset switching
- Automated dataset rotation
- Conditional evaluation triggers

**2. Complex Filtering:**
- Multiple column filters
- Date range filtering
- Conditional logic in filters
- Expression-based filter values

**3. Large Dataset Handling:**
- Chunk processing for very large datasets
- Progress tracking and resumption
- Memory optimization techniques
- Parallel evaluation strategies

---

### 18. Execute Command
**Status**: ✅ Active (Self-hosted only)
**Category**: Core - System/Shell
**Purpose**: Run shell commands on the host machine that runs n8n

#### Authentication:
- **No credentials required**: Direct access to host system shell
- **System Requirements**: Requires self-hosted n8n instance
- **Shell Access**: Uses default shell of host machine (cmd on Windows, zsh on macOS, bash on Linux)

#### Availability:
- **Self-hosted only**: Not available on n8n Cloud for security reasons
- **Docker Considerations**: Commands run inside n8n container, not Docker host
- **Shell Environment**: Uses default shell of the operating system

#### Operations:
**1. Execute Shell Command** - Run any shell command on the host system

#### Core Parameters:

**1. Execute Once**
- **Purpose**: Control execution frequency based on input data
- **Options**:
  - **On (true)**: Execute command only once regardless of input item count
  - **Off (false)**: Execute command once for every input item received
- **Use Cases**: 
  - Single execution for system operations (backups, updates)
  - Per-item execution for data processing

**2. Command**
- **Purpose**: Specify the shell command(s) to execute
- **Input Type**: Text field supporting expressions
- **Expression Support**: Can use n8n expressions to build dynamic commands
- **Multi-line Support**: Enter commands on multiple lines or single line

#### Advanced Command Features:

**Multiple Commands Execution:**

**Method 1: Single Line with &&**
- **Syntax**: `command1 && command2 && command3`
- **Behavior**: Commands execute sequentially, stops if any command fails
- **Example**: `cd bin && ls` (change directory then list contents)
- **Advantage**: Clear dependency chain

**Method 2: Multi-line Commands**
- **Syntax**: Enter each command on separate line
- **Behavior**: Commands execute in sequence
- **Example**:
  ```
  cd bin
  ls
  ```
- **Advantage**: Better readability for complex sequences

#### Docker Integration:

**cURL Command Setup:**
When using n8n in Docker and needing cURL functionality:

**Custom Dockerfile:**
```dockerfile
FROM docker.n8n.io/n8nio/n8n
USER root
RUN apk --update add curl
USER node
```

**Build Process:**
1. Create Dockerfile with above content
2. Build image: `docker build -t n8n-curl .`
3. Replace original image with `n8n-curl` in docker-compose
4. Now cURL commands available in Execute Command

#### Key Features:

**System Integration:**
- **Full shell access**: Run any command available on host system
- **Environment variables**: Access to system environment variables
- **File system operations**: Create, read, write, delete files and directories
- **Process management**: Start/stop services, monitor processes

**Workflow Integration:**
- **Dynamic commands**: Build commands using workflow data
- **Output processing**: Command output becomes node output data
- **Error handling**: Command failures trigger workflow errors
- **Expression support**: Use n8n expressions in command strings

#### Use Cases:

**1. System Administration:**
- File backups and archiving
- Log rotation and cleanup
- System monitoring and health checks
- Service management and restarts
- Database backups and maintenance

**2. Data Processing:**
- File format conversions using command-line tools
- Data compression and decompression
- Image/video processing with FFmpeg
- PDF manipulation with command-line tools
- CSV/JSON processing with awk, sed, jq

**3. Development Operations:**
- Git operations (clone, pull, push, commit)
- Build and deployment scripts
- Package installations and updates
- Environment setup and configuration
- Test execution and reporting

**4. Integration Tasks:**
- FTP/SFTP file transfers
- Database operations using CLI tools
- API testing with cURL
- Network operations and diagnostics
- Certificate management and SSL operations

#### Common Patterns:

**1. File Operations:**
```bash
# Create backup directory
mkdir -p /backups/$(date +%Y%m%d)

# Copy files with timestamp
cp -r /data/* /backups/$(date +%Y%m%d)/

# Compress backup
tar -czf /backups/backup_$(date +%Y%m%d).tar.gz /backups/$(date +%Y%m%d)
```

**2. Git Workflow:**
```bash
# Navigate to repository
cd /path/to/repo

# Pull latest changes
git pull origin main

# Add and commit changes
git add .
git commit -m "Automated update: $(date)"

# Push to repository
git push origin main
```

**3. Data Processing:**
```bash
# Process CSV file
awk -F',' '{print $1,$3}' input.csv > output.csv

# JSON manipulation
jq '.items[] | {name: .name, value: .value}' input.json > processed.json

# Log analysis
grep "ERROR" /var/log/application.log | tail -50 > recent_errors.log
```

**4. System Monitoring:**
```bash
# Check disk space
df -h > disk_usage.txt

# Monitor processes
ps aux | grep myapp > process_status.txt

# Network connectivity
ping -c 5 google.com > connectivity_test.txt
```

#### Integration Examples:

**1. Automated Backup System:**
- Schedule Trigger → Execute Command (create backup) → Send Email (notify completion)
- Git repository backups with timestamps
- Database dumps with compression
- Log file archival and cleanup

**2. Data Pipeline Operations:**
- HTTP Request (download file) → Execute Command (process file) → Upload to cloud
- File format conversions
- Data validation and cleanup
- Report generation

**3. Development Workflow:**
- Webhook → Execute Command (run tests) → Slack (notify results)
- Continuous integration triggers
- Deployment automation
- Code quality checks

**4. System Monitoring:**
- Schedule Trigger → Execute Command (system check) → Conditional alerts
- Performance monitoring
- Security scanning
- Health checks and diagnostics

#### Technical Details:

**Command Execution Environment:**
- **Working Directory**: Defaults to n8n process working directory
- **User Context**: Runs as n8n process user
- **Environment Variables**: Access to system and n8n environment variables
- **Shell Features**: Full shell capabilities (pipes, redirects, variables)

**Output Handling:**
- **stdout**: Captured as node output data
- **stderr**: Included in error messages
- **Exit Codes**: Non-zero exit codes trigger workflow errors
- **Binary Data**: Text output only, binary data requires file operations

**Performance Considerations:**
- **Blocking Execution**: Commands run synchronously
- **Memory Usage**: Large command outputs consume workflow memory
- **Timeout Handling**: Long-running commands may timeout
- **Resource Limits**: Subject to system resource constraints

#### Security Considerations:

**Access Control:**
- **Host System Access**: Full access to host file system and processes
- **User Permissions**: Limited by n8n process user permissions
- **Command Injection**: Validate and sanitize dynamic command inputs
- **Privilege Escalation**: Be cautious with sudo and elevated permissions

**Best Practices:**
- **Input Validation**: Always validate and sanitize user inputs in commands
- **Least Privilege**: Run n8n with minimal required permissions
- **Command Whitelisting**: Consider restricting allowed commands
- **Audit Logging**: Log all executed commands for security monitoring

#### Templates Available:
- **Scrape and store data from multiple website pages**
- **Git backup of workflows and credentials**  
- **Track changes of product prices**

#### Related Nodes:
- **SSH**: Remote command execution
- **HTTP Request**: Alternative for cURL operations
- **Read/Write Files from Disk**: File system operations
- **Code**: JavaScript/Python execution (safer alternative)

#### Best Practices:

**1. Command Safety:**
- Validate all dynamic inputs
- Use absolute paths when possible
- Handle special characters in file names
- Test commands in safe environment first

**2. Error Handling:**
- Check exit codes and handle failures gracefully
- Implement fallback procedures for critical operations
- Log command outputs for debugging
- Use error workflows for failure notifications

**3. Performance Optimization:**
- Minimize command output size when possible
- Use streaming for large data operations
- Implement timeouts for long-running commands
- Consider async operations for non-blocking execution

**4. Security Best Practices:**
- Never trust user input in commands
- Use parameterized commands when possible
- Implement command whitelisting for public workflows
- Monitor and audit command execution

#### Troubleshooting:

**Common Issues:**
- **Command not found**: Check PATH and command availability
- **Permission denied**: Verify user permissions and file access
- **Path issues**: Use absolute paths or verify working directory
- **Special characters**: Properly escape quotes and special characters

**Debugging Tips:**
- Test commands manually in terminal first
- Check n8n logs for detailed error messages
- Verify file paths and permissions
- Use echo commands to debug variable expansion

**Docker-specific Issues:**
- Commands run in container, not host
- Limited packages available in base image
- Need custom Dockerfile for additional tools
- Volume mounts required for host file access

#### Common Issues Documentation:
- **Dedicated troubleshooting page**: [Common Issues](common-issues/) with detailed solutions
- **Platform-specific guidance**: Windows, macOS, Linux considerations
- **Docker setup**: Container configuration and package installation
- **Security considerations**: Safe command execution practices

---

### 19. Execute Sub-workflow
**Status**: ✅ Active
**Category**: Core - Workflow Orchestration
**Purpose**: Run a different workflow on the host machine that runs n8n, enabling modular workflow design and reusable components

#### Authentication:
- **No credentials required**: Uses internal n8n workflow execution system
- **Workflow permissions**: Controlled by sub-workflow settings ("This workflow can be called by")
- **Access control**: Can restrict which workflows can call the sub-workflow

#### Operations:
**1. Execute Workflow** - Run another workflow and return its results

#### Core Parameters:

**1. Source** - Choose where to get the sub-workflow information:

**Database (Recommended)**
- **From list**: Select workflow from dropdown of available workflows
  - **Auto-discovery**: Shows all accessible workflows in your account
  - **Input detection**: Automatically displays sub-workflow's input requirements
  - **Direct selection**: Most user-friendly option
- **Workflow ID**: Enter workflow ID manually
  - **ID location**: Found in URL after `/workflow/`
  - **Example**: `https://my-n8n-acct.app.n8n.cloud/workflow/abCDE1f6gHiJKL7`
  - **Extracted ID**: `abCDE1f6gHiJKL7`

**Local File**
- **Workflow Path**: Path to locally saved JSON workflow file
- **Use case**: Execute workflows stored as files on the system
- **File format**: Standard n8n workflow JSON export format

**Parameter**
- **Workflow JSON**: Enter workflow JSON code directly in the node
- **Use case**: Dynamic workflow execution with programmatically generated workflows
- **Flexibility**: Allows workflow construction through expressions

**URL**
- **Workflow URL**: Load workflow from remote URL
- **Use case**: Execute workflows hosted on external systems
- **Format**: URL pointing to workflow JSON file

**2. Workflow Inputs** - Configure data passing to sub-workflow:

**Automatic Input Detection** (Database + From list only):
- **Input display**: Sub-workflow's input items automatically appear
- **Field mapping**: Map parent workflow data to sub-workflow inputs
- **Type conversion**: Optional automatic type conversion
- **Null handling**: Can remove inputs (sub-workflow receives `null`)

**Input Requirements**:
- **Triggered by**: Sub-workflow's Execute Sub-workflow Trigger node configuration
- **Input modes**: 
  - "Define using fields below" → Shows specific input fields
  - "Accept all data" → No input fields shown (accepts any data)
  - "Define using JSON example" → Based on JSON structure

**Type Conversion**:
- **Toggle**: "Attempt to convert types"
- **Automatic conversion**: Try to match sub-workflow's expected data types
- **Flexibility**: Handle type mismatches gracefully

**3. Mode** - Control execution behavior:

**Run once with all items**
- **Behavior**: Pass all input items into single sub-workflow execution
- **Use case**: Process entire dataset together in sub-workflow
- **Performance**: Single execution overhead

**Run once for each item**
- **Behavior**: Execute sub-workflow separately for each input item
- **Use case**: Process items individually with isolated contexts
- **Performance**: Multiple execution overhead, but isolated processing

#### Node Options:

**Wait for Sub-Workflow Completion**
- **On (default)**: Main workflow pauses until sub-workflow completes
  - **Synchronous execution**: Sequential workflow processing
  - **Result availability**: Sub-workflow results available for next nodes
  - **Error propagation**: Sub-workflow errors halt main workflow
- **Off**: Main workflow continues without waiting
  - **Asynchronous execution**: Parallel workflow processing
  - **Fire-and-forget**: Sub-workflow runs independently
  - **No result data**: Main workflow doesn't receive sub-workflow output

#### Sub-workflow Setup Process:

**1. Create Sub-workflow:**
- Create new workflow with Execute Sub-workflow Trigger node
- Configure input data mode in trigger:
  - **Define using fields below**: Specify required input structure
  - **Define using JSON example**: Provide example input format
  - **Accept all data**: Accept any input without validation
- Configure workflow settings for calling permissions
- Build sub-workflow logic and save

**2. Call Sub-workflow:**
- Add Execute Sub-workflow node to parent workflow
- Select sub-workflow using Source parameter
- Map input data to sub-workflow requirements
- Configure execution mode and options

#### Data Flow Mechanics:

**Parent to Sub-workflow:**
- **Data source**: Execute Sub-workflow node passes data
- **Data destination**: Execute Sub-workflow Trigger node in sub-workflow
- **Data format**: Structured according to sub-workflow's input definition

**Sub-workflow to Parent:**
- **Data source**: Last node executed in sub-workflow
- **Data destination**: Execute Sub-workflow node output
- **Data format**: Whatever the last sub-workflow node outputs

**Execution Linking:**
- **Sub-execution tracking**: Navigate from parent to sub-workflow execution
- **Parent execution tracking**: Navigate from sub-workflow back to parent
- **Execution history**: Maintain audit trail of workflow calls

#### Advanced Features:

**Sub-workflow Creation Options:**
- **Direct creation**: Create new sub-workflow from Execute Sub-workflow node
- **Extraction conversion**: Convert selected nodes to sub-workflow using context menu
- **Template workflows**: Use existing workflows as sub-workflow templates

**Access Control:**
- **Workflow settings**: "This workflow can be called by" configuration
- **Permission levels**: Control which workflows can execute the sub-workflow
- **Security isolation**: Prevent unauthorized workflow execution

**Development Workflow:**
- **Data loading**: Load real data into sub-workflow for development
- **Execution saving**: Save sub-workflow executions for debugging
- **Previous execution loading**: Use real data for sub-workflow development

#### Use Cases:

**1. Modular Workflow Design:**
- Break complex workflows into reusable components
- Create specialized processing modules
- Implement shared business logic across multiple workflows
- Reduce workflow complexity and improve maintainability

**2. Reusable Components:**
- Data validation and cleaning workflows
- API integration modules
- Report generation components
- Error handling and notification workflows

**3. Conditional Processing:**
- Dynamic workflow selection based on data
- A/B testing with different processing workflows
- Feature flag implementation
- Environment-specific processing (dev/test/prod)

**4. Parallel Processing:**
- Execute multiple workflows simultaneously (async mode)
- Distribute processing across different workflow instances
- Implement concurrent data processing patterns

**5. Workflow Organization:**
- Separate concerns into focused sub-workflows
- Implement microservices-style workflow architecture
- Create reusable workflow libraries
- Standardize common operations

#### Common Patterns:

**1. Data Processing Pipeline:**
```
Main Workflow → Execute Sub-workflow (Data Validation) → 
Execute Sub-workflow (Data Processing) → 
Execute Sub-workflow (Data Storage)
```

**2. Conditional Workflow Execution:**
```
Main Workflow → Switch → Multiple Execute Sub-workflow nodes
(Different processing based on conditions)
```

**3. Parallel Processing:**
```
Main Workflow → Multiple Execute Sub-workflow nodes (async) → 
Wait/Merge results
```

**4. Error Handling:**
```
Main Workflow → Execute Sub-workflow → 
Error Trigger → Execute Sub-workflow (Error Handler)
```

#### Integration Examples:

**1. E-commerce Order Processing:**
- Main workflow receives order
- Sub-workflow: Inventory validation
- Sub-workflow: Payment processing
- Sub-workflow: Shipping calculation
- Sub-workflow: Order confirmation

**2. Content Management:**
- Main workflow receives content
- Sub-workflow: Content validation
- Sub-workflow: Image processing
- Sub-workflow: SEO optimization
- Sub-workflow: Publishing

**3. Data Pipeline:**
- Main workflow orchestrates data flow
- Sub-workflow: Data extraction
- Sub-workflow: Data transformation
- Sub-workflow: Data validation
- Sub-workflow: Data loading

#### Performance Considerations:

**Execution Overhead:**
- **Sub-workflow startup**: Each execution has initialization overhead
- **Mode selection**: "Run once with all items" more efficient for bulk processing
- **Async execution**: Reduces main workflow blocking time

**Memory Usage:**
- **Data passing**: All data copied between workflows
- **Large datasets**: Consider data size when passing between workflows
- **Execution context**: Each sub-workflow execution consumes memory

**Network Considerations:**
- **Local execution**: Sub-workflows run on same n8n instance
- **No network overhead**: Direct internal execution
- **Database access**: Sub-workflow metadata retrieved from database

#### Error Handling:

**Error Propagation:**
- **Synchronous mode**: Sub-workflow errors halt main workflow
- **Asynchronous mode**: Sub-workflow errors don't affect main workflow
- **Error workflows**: Sub-workflows can have their own error workflows

**Common Issues:**
- **Workflow not found**: Check workflow ID and permissions
- **Input validation errors**: Verify input data matches sub-workflow requirements
- **Permission denied**: Check "This workflow can be called by" settings
- **Circular dependencies**: Avoid workflows calling themselves directly or indirectly

#### Templates Available:
- **Scrape business emails from Google Maps without third party APIs**
- **Back Up Your n8n Workflows To Github**
- **Host Your Own AI Deep Research Agent with n8n, Apify and OpenAI o3**

#### Related Nodes:
- **Execute Sub-workflow Trigger**: Required first node in sub-workflows
- **n8n**: Alternative for workflow management operations
- **Call n8n Workflow Tool**: LangChain-based workflow calling
- **Switch**: Conditional sub-workflow execution
- **Merge**: Combine results from multiple sub-workflows

#### Best Practices:

**1. Design Principles:**
- Keep sub-workflows focused on single responsibilities
- Design for reusability across multiple parent workflows
- Use clear, descriptive names for sub-workflows
- Document sub-workflow inputs and outputs

**2. Input/Output Management:**
- Define clear input contracts using "Define using fields below"
- Validate inputs in sub-workflows
- Provide meaningful output data structures
- Handle null and missing input values gracefully

**3. Performance Optimization:**
- Use "Run once with all items" for bulk processing
- Consider async execution for independent operations
- Minimize data passed between workflows
- Cache expensive operations in sub-workflows

**4. Error Handling:**
- Implement proper error handling in sub-workflows
- Use meaningful error messages
- Consider error workflows for critical sub-workflows
- Test error scenarios thoroughly

**5. Organization:**
- Use folders to organize related sub-workflows
- Implement consistent naming conventions
- Version control sub-workflows
- Document workflow dependencies

#### Security Considerations:
- Configure "This workflow can be called by" settings appropriately
- Validate all input data in sub-workflows
- Use least-privilege access for sub-workflow permissions
- Audit workflow execution chains for security reviews

#### Troubleshooting:

**Sub-workflow Not Executing:**
- Verify workflow ID is correct
- Check workflow permissions settings
- Ensure sub-workflow is saved and contains no errors
- Verify Execute Sub-workflow Trigger is properly configured

**Data Not Passing Correctly:**
- Check input field mapping
- Verify sub-workflow input configuration
- Test with simple data first
- Enable type conversion if needed

**Performance Issues:**
- Consider execution mode selection
- Monitor workflow execution times
- Optimize sub-workflow logic
- Use async execution when appropriate

---

### 20. Execute Sub-workflow Trigger
**Status**: ✅ Active
**Category**: Core - Trigger/Workflow Orchestration
**Purpose**: Start a workflow in response to another workflow calling it. Must be the first node in sub-workflows to receive data from parent workflows.

#### Authentication:
- **No credentials required**: Uses internal n8n workflow execution system
- **Workflow permissions**: Controlled by workflow settings ("This workflow can be called by")
- **Access control**: Configure which workflows can call this sub-workflow

#### Operations:
**1. Receive Workflow Call** - Triggered when parent workflow executes this sub-workflow

#### Core Parameters:

**1. Input Data Mode** - Choose how to define sub-workflow's input data structure:

**Define using fields below** (Recommended)
- **Purpose**: Define individual input names and data types for calling workflow
- **Auto-detection**: Execute Sub-workflow and Call n8n Workflow Tool nodes automatically pull these fields
- **Type safety**: Specify expected data types for each input field
- **Validation**: Ensures calling workflow provides required inputs
- **Structure**: Create explicit input contract

**Field Configuration** (when using "Define using fields below"):
- **Field Name**: Name of input field expected from calling workflow
- **Field Type**: Data type specification (text, number, boolean, object, array)
- **Required**: Whether field is mandatory or optional
- **Description**: Documentation for field purpose and usage
- **Default Value**: Optional default if calling workflow doesn't provide value

**Define using JSON example**
- **Purpose**: Provide example JSON object demonstrating expected input structure
- **Flexibility**: Show complex nested data structures
- **Documentation**: JSON serves as both example and specification
- **Type inference**: n8n infers types from JSON example structure

**JSON Example Configuration**:
- **Example JSON**: Sample JSON object showing expected input format
- **Type detection**: Automatic type inference from example values
- **Nested objects**: Support for complex data structures
- **Array handling**: Examples of expected array formats

**Accept all data**
- **Purpose**: Accept any input data without validation or structure requirements
- **Flexibility**: No predefined input structure
- **Responsibility**: Sub-workflow must handle data inconsistencies
- **Use case**: Dynamic workflows that process varying data structures
- **No input fields**: Calling workflow doesn't see specific input requirements

#### Key Features:

**Workflow Integration**:
- **First node requirement**: Must be first node in sub-workflow
- **Trigger activation**: Automatically triggered by Execute Sub-workflow or Call n8n Workflow Tool nodes
- **Data passing**: Receives data from calling workflow and makes it available to sub-workflow
- **Execution context**: Sets up sub-workflow execution environment

**Input Structure Definition**:
- **Type safety**: Define expected data types and structures
- **Auto-discovery**: Calling nodes automatically detect and display input requirements
- **Validation**: Ensure data quality and consistency
- **Documentation**: Self-documenting input contracts

**Workflow Orchestration**:
- **Parent-child relationship**: Establishes connection between calling and called workflows
- **Data flow**: Manages data transfer between workflow contexts
- **Execution tracking**: Links parent and sub-workflow executions
- **Error propagation**: Handles error scenarios between workflows

#### Sub-workflow Setup Process:

**1. Create Sub-workflow:**
- Create new workflow
- Add Execute Sub-workflow Trigger as first node
- Configure input data mode based on requirements
- Define input structure using chosen mode
- Build sub-workflow logic with additional nodes
- Save workflow

**2. Configure Access Control:**
- Access workflow settings (Options → Settings)
- Configure "This workflow can be called by" setting
- Choose from options:
  - **Any workflow in this n8n instance**
  - **Only specific workflows** (provide workflow IDs)
  - **Only workflows in the same folder**

**3. Set Up Calling Workflow:**
- Add Execute Sub-workflow node to parent workflow
- Select sub-workflow using Database → From list
- Map input data to sub-workflow requirements
- Configure execution options

#### Input Data Mode Details:

**1. Define using fields below - Detailed Configuration:**

**Field Properties:**
- **Name**: Input field identifier
- **Type**: Data type (string, number, boolean, object, array, any)
- **Required**: Mandatory field toggle
- **Description**: Field documentation and usage notes
- **Default Value**: Fallback value if not provided

**Type Options:**
- **String**: Text data
- **Number**: Numeric values (integers, decimals)
- **Boolean**: True/false values
- **Object**: JSON objects and nested structures
- **Array**: Lists and collections
- **Any**: Accept any data type

**Validation Benefits:**
- **Type checking**: Automatic validation of input data types
- **Required fields**: Ensure critical data is provided
- **Clear contracts**: Explicit input/output specifications
- **Error prevention**: Catch data issues early

**2. Define using JSON example - Structure:**

**Example JSON Formats:**
```json
{
  "user": {
    "name": "John Doe",
    "email": "john@example.com",
    "age": 30
  },
  "preferences": ["email", "sms"],
  "metadata": {
    "source": "api",
    "timestamp": "2024-01-01T00:00:00Z"
  }
}
```

**Benefits:**
- **Visual structure**: Clear example of expected data
- **Complex nesting**: Handle sophisticated data structures
- **Array examples**: Show expected list formats
- **Type inference**: Automatic type detection from examples

**3. Accept all data - Characteristics:**

**Flexibility:**
- **No validation**: Accept any input structure
- **Dynamic processing**: Handle varying data formats
- **Runtime adaptation**: Process data based on actual content
- **Error handling**: Must implement robust data validation internally

**Use Cases:**
- **Generic processors**: Workflows that handle multiple data types
- **Data transformation**: Convert between different formats
- **Experimental workflows**: Testing and development scenarios
- **Legacy integration**: Working with inconsistent data sources

#### Development Workflow:

**Data Loading for Development:**
- **Requirement**: n8n Cloud or registered Community plans
- **Process**: Load real data from parent workflow executions
- **Benefits**: Develop with actual data structures

**Steps:**
1. Create sub-workflow with Execute Sub-workflow Trigger
2. Set Input data mode to "Accept all data" initially
3. Configure workflow settings to save executions
4. Set up parent workflow and execute
5. Load execution data into sub-workflow for development
6. Adjust input data mode to match actual data structure
7. Continue development with real data

#### Advanced Features:

**Execution Linking:**
- **Bidirectional navigation**: Navigate between parent and sub-workflow executions
- **Execution history**: Maintain audit trail of workflow calls
- **Performance tracking**: Monitor sub-workflow execution times
- **Debug capabilities**: Trace data flow between workflows

**Error Handling:**
- **Error propagation**: Sub-workflow errors affect parent workflow
- **Error isolation**: Option to handle errors within sub-workflow
- **Error workflows**: Sub-workflows can have their own error handlers
- **Validation errors**: Input validation failures halt execution

**Performance Optimization:**
- **Execution caching**: Reuse sub-workflow results when appropriate
- **Data transfer optimization**: Minimize data copying between workflows
- **Resource management**: Efficient memory and CPU usage
- **Concurrent execution**: Support for parallel sub-workflow calls

#### Use Cases:

**1. Modular Workflow Design:**
- **Data validation modules**: Reusable data quality checks
- **Business logic components**: Shared calculation and processing
- **Integration adapters**: Standardized API interaction patterns
- **Report generators**: Common reporting functionality

**2. Workflow Libraries:**
- **Utility functions**: Common operations across workflows
- **API wrappers**: Standardized external service interactions
- **Data transformers**: Reusable data processing components
- **Notification systems**: Shared alerting and communication

**3. Complex Orchestration:**
- **Multi-step processes**: Break complex workflows into manageable pieces
- **Conditional execution**: Route to different sub-workflows based on data
- **Parallel processing**: Execute multiple sub-workflows simultaneously
- **Data pipelines**: Chain processing stages through sub-workflows

**4. Environment Management:**
- **Environment-specific logic**: Different processing for dev/test/prod
- **Feature toggles**: Enable/disable functionality through sub-workflows
- **A/B testing**: Route traffic to different processing versions
- **Compliance workflows**: Standardized regulatory processing

#### Common Patterns:

**1. Data Processing Pipeline:**
```
Parent → Sub-workflow (Validate) → Sub-workflow (Transform) → Sub-workflow (Store)
```

**2. Service-Oriented Architecture:**
```
Main Workflow → Authentication Sub-workflow → Business Logic Sub-workflow → Response Sub-workflow
```

**3. Error Handling Chain:**
```
Primary Workflow → Data Processing Sub-workflow → Error Handler Sub-workflow
```

**4. Conditional Processing:**
```
Router Workflow → Switch → Multiple Processing Sub-workflows
```

#### Integration Examples:

**1. E-commerce Order Processing:**
- **Order validation sub-workflow**: Validate order data structure
- **Inventory check sub-workflow**: Check product availability
- **Payment processing sub-workflow**: Handle payment transactions
- **Fulfillment sub-workflow**: Manage shipping and delivery

**2. Content Management System:**
- **Content validation sub-workflow**: Validate content structure and quality
- **Image processing sub-workflow**: Resize and optimize images
- **SEO optimization sub-workflow**: Apply SEO best practices
- **Publishing sub-workflow**: Deploy content to production

**3. Data Integration Pipeline:**
- **Data extraction sub-workflow**: Pull data from various sources
- **Data transformation sub-workflow**: Clean and normalize data
- **Data validation sub-workflow**: Ensure data quality
- **Data loading sub-workflow**: Store processed data

#### Technical Implementation:

**Trigger Mechanics:**
- **Event-driven**: Responds to execution calls from parent workflows
- **Synchronous execution**: Parent workflow waits for sub-workflow completion (default)
- **Asynchronous support**: Option for fire-and-forget execution
- **Data serialization**: Efficient data transfer between workflow contexts

**Memory Management:**
- **Data copying**: Input data copied to sub-workflow context
- **Memory isolation**: Sub-workflow execution in separate memory space
- **Garbage collection**: Automatic cleanup after execution
- **Resource limits**: Respect system resource constraints

**Security Considerations:**
- **Access control**: Workflow-level permissions for sub-workflow calls
- **Data isolation**: Secure data handling between workflows
- **Audit logging**: Track all sub-workflow executions
- **Permission inheritance**: Sub-workflows inherit parent permissions

#### Performance Considerations:

**Execution Overhead:**
- **Startup time**: Sub-workflow initialization overhead
- **Data transfer**: Network and serialization costs
- **Context switching**: CPU overhead for workflow transitions
- **Memory usage**: Additional memory for sub-workflow execution

**Optimization Strategies:**
- **Minimize data transfer**: Only pass required data to sub-workflows
- **Cache sub-workflow results**: Avoid redundant executions
- **Batch processing**: Group multiple calls when possible
- **Async execution**: Use fire-and-forget for independent operations

#### Error Handling:

**Common Error Scenarios:**
- **Input validation failures**: Data doesn't match expected structure
- **Type conversion errors**: Incompatible data types
- **Missing required fields**: Required inputs not provided
- **Permission denied**: Workflow not authorized to call sub-workflow

**Error Resolution:**
- **Validate input data**: Ensure data matches sub-workflow requirements
- **Check permissions**: Verify workflow call authorization
- **Handle type mismatches**: Use appropriate data conversion
- **Implement fallbacks**: Graceful degradation for missing data

#### Templates Available:
- **Browse Execute Sub-workflow Trigger integration templates**
- **Search all templates for sub-workflow patterns**

#### Related Nodes:
- **Execute Sub-workflow**: Calls sub-workflows from parent workflows
- **Call n8n Workflow Tool**: LangChain-based workflow calling
- **n8n**: Alternative workflow management operations
- **Workflow Trigger**: Different trigger for workflow automation
- **Manual Trigger**: Manual workflow execution

#### Best Practices:

**1. Input Design:**
- **Clear contracts**: Define explicit input requirements
- **Type safety**: Use specific data types when possible
- **Documentation**: Provide clear field descriptions
- **Validation**: Implement robust input validation

**2. Workflow Organization:**
- **Single responsibility**: Keep sub-workflows focused
- **Reusability**: Design for use across multiple parent workflows
- **Naming conventions**: Use descriptive workflow names
- **Folder organization**: Group related sub-workflows

**3. Performance Optimization:**
- **Minimize data**: Only pass necessary data to sub-workflows
- **Efficient processing**: Optimize sub-workflow logic
- **Caching**: Cache expensive operations when appropriate
- **Monitoring**: Track sub-workflow performance

**4. Error Handling:**
- **Graceful degradation**: Handle missing or invalid inputs
- **Clear error messages**: Provide actionable error information
- **Error workflows**: Implement error handling sub-workflows
- **Testing**: Test error scenarios thoroughly

#### Security Best Practices:
- **Least privilege**: Configure minimal required permissions
- **Input validation**: Validate all input data thoroughly
- **Audit logging**: Log all sub-workflow executions
- **Access control**: Restrict sub-workflow call permissions appropriately

#### Troubleshooting:

**Sub-workflow Not Triggering:**
- **Check permissions**: Verify "This workflow can be called by" settings
- **Validate workflow ID**: Ensure correct workflow ID in calling node
- **Check workflow state**: Ensure sub-workflow is saved and error-free
- **Verify trigger placement**: Confirm Execute Sub-workflow Trigger is first node

**Input Data Issues:**
- **Type mismatches**: Check data types match expected structure
- **Missing fields**: Verify all required fields are provided
- **Validation errors**: Review input validation rules
- **Data format**: Ensure data format matches JSON example

**Performance Issues:**
- **Large data transfers**: Minimize data passed between workflows
- **Execution time**: Optimize sub-workflow processing logic
- **Memory usage**: Monitor memory consumption in sub-workflows
- **Concurrent execution**: Manage parallel sub-workflow calls

---

### 21. Execution Data
**Status**: ✅ Active (Pro/Enterprise plans only)
**Category**: Core - Workflow Management/Metadata
**Purpose**: Save metadata for workflow executions to enable searchable execution data in the Executions list

#### Authentication:
- **No credentials required**: Uses internal n8n execution system
- **Plan requirement**: Available on Pro and Enterprise plans only
- **Storage**: Metadata stored with workflow execution records

#### Operations:
**1. Save Execution Data for Search** - Store custom key/value metadata pairs for workflow executions

#### Core Parameters:

**1. Data to Save** - Configure metadata to associate with workflow execution:

**Saved Field Configuration**:
- **Key**: Metadata field identifier (up to 50 characters)
- **Value**: Metadata field value (up to 512 characters)
- **Multiple fields**: Add multiple key/value pairs for comprehensive metadata
- **Expression support**: Use n8n expressions for dynamic key/value generation

#### Key Features:

**Execution Metadata Storage**:
- **Custom metadata**: Store any key/value pairs relevant to workflow execution
- **Searchable data**: Metadata becomes searchable in n8n Executions list
- **Execution context**: Metadata tied to specific workflow execution
- **Persistent storage**: Metadata preserved with execution records

**Search Integration**:
- **Executions list filtering**: Filter executions by custom metadata
- **Advanced search**: Find specific executions using saved metadata criteria
- **Execution organization**: Categorize and organize workflow runs
- **Audit trail**: Maintain searchable execution history

**Data Flexibility**:
- **Dynamic values**: Use expressions to generate metadata from workflow data
- **Multiple metadata fields**: Store multiple key/value pairs per execution
- **Contextual information**: Capture relevant business or technical context
- **Cross-workflow tracking**: Use consistent metadata across workflows

#### Data Storage Limitations:

**Character Limits**:
- **Key length**: Maximum 50 characters per key
- **Value length**: Maximum 512 characters per value
- **Truncation behavior**: n8n automatically truncates exceeding content
- **Logging**: Truncation events logged for monitoring

**Automatic Handling**:
- **Overflow protection**: Prevents data loss from oversized content
- **Log entries**: Truncation events recorded in system logs
- **Graceful degradation**: Workflow continues despite metadata truncation
- **Data integrity**: Core workflow execution unaffected by metadata limits

#### Use Cases:

**1. Execution Tracking and Organization:**
- **Customer ID tracking**: Tag executions with customer identifiers
- **Order processing**: Track order numbers and status through workflows
- **Batch processing**: Label executions with batch IDs and processing dates
- **A/B testing**: Tag executions with test variant information

**2. Business Context Metadata:**
- **User identification**: Store user IDs or email addresses
- **Department/team tracking**: Tag executions by organizational unit
- **Campaign tracking**: Associate executions with marketing campaigns
- **Project classification**: Label executions by project or initiative

**3. Technical Metadata:**
- **Environment tagging**: Mark executions as dev/staging/production
- **Version tracking**: Store application or workflow version information
- **Performance metrics**: Save execution time or resource usage data
- **Error classification**: Tag failed executions with error categories

**4. Compliance and Auditing:**
- **Regulatory compliance**: Store compliance-related metadata
- **Data source tracking**: Record data origin and processing steps
- **Quality assurance**: Tag executions with QA status and reviewer
- **Change tracking**: Store change request or approval numbers

#### Common Metadata Patterns:

**1. Customer-Centric Tracking:**
```
Key: "customer_id"        Value: "{{ $json.customerId }}"
Key: "customer_email"     Value: "{{ $json.customer.email }}"
Key: "subscription_tier"  Value: "{{ $json.customer.tier }}"
```

**2. Order Processing:**
```
Key: "order_number"       Value: "{{ $json.orderNumber }}"
Key: "order_value"        Value: "{{ $json.totalAmount }}"
Key: "payment_method"     Value: "{{ $json.paymentMethod }}"
```

**3. Technical Context:**
```
Key: "environment"        Value: "production"
Key: "version"            Value: "{{ $workflow.version }}"
Key: "execution_mode"     Value: "{{ $execution.mode }}"
```

**4. Business Intelligence:**
```
Key: "campaign_id"        Value: "{{ $json.campaignId }}"
Key: "channel"            Value: "{{ $json.channel }}"
Key: "conversion_value"   Value: "{{ $json.conversionValue }}"
```

#### Integration Examples:

**1. E-commerce Order Tracking:**
- Store order numbers, customer IDs, and order values
- Enable searching executions by customer or order criteria
- Track order processing status and completion times
- Generate business reports from execution metadata

**2. Customer Support Workflows:**
- Tag executions with ticket numbers and customer information
- Track support case processing through multiple workflows
- Search executions by customer or issue type
- Monitor support workflow performance and resolution times

**3. Data Pipeline Management:**
- Label data processing executions with batch IDs and timestamps
- Track data source and destination information
- Monitor data quality metrics through execution metadata
- Enable data lineage tracking and compliance reporting

**4. Marketing Campaign Automation:**
- Tag executions with campaign IDs and target audience segments
- Track email delivery, click-through, and conversion metrics
- Search executions by campaign performance criteria
- Generate campaign effectiveness reports

#### Retrieval and Usage:

**Code Node Retrieval**:
- **JavaScript access**: Use `$execution.customData` to access saved metadata
- **Runtime retrieval**: Access metadata during workflow execution
- **Conditional logic**: Use metadata for execution branching
- **Dynamic processing**: Adapt workflow behavior based on saved metadata

**Example Code Access**:
```javascript
// Access saved execution metadata
const customerId = $execution.customData.customer_id;
const orderNumber = $execution.customData.order_number;

// Use metadata for conditional processing
if ($execution.customData.priority === 'high') {
  // Handle high-priority execution
}
```

#### Advanced Features:

**1. Expression-Based Metadata:**
- **Dynamic key generation**: Use expressions to create conditional keys
- **Calculated values**: Store computed values from workflow data
- **Timestamp metadata**: Add execution timing information
- **User context**: Include current user or session information

**2. Cross-Workflow Tracking:**
- **Consistent metadata**: Use same keys across related workflows
- **Workflow chains**: Track execution relationships across workflows
- **Parent-child tracking**: Link sub-workflow executions to parent executions
- **Process correlation**: Connect related business processes

**3. Performance Monitoring:**
- **Execution timing**: Store workflow performance metrics
- **Resource usage**: Track memory or CPU consumption
- **Error rates**: Store success/failure indicators
- **SLA tracking**: Monitor workflow performance against targets

#### Search and Filtering:

**Executions List Integration**:
- **Metadata columns**: Display saved metadata in executions list
- **Search filters**: Filter executions by metadata values
- **Sort capabilities**: Sort executions by metadata criteria
- **Export functionality**: Include metadata in execution exports

**Advanced Search Patterns**:
- **Wildcard searches**: Use pattern matching for metadata values
- **Date range filtering**: Combine metadata with execution timing
- **Multi-criteria searches**: Filter by multiple metadata fields
- **Regular expressions**: Advanced pattern matching for complex searches

#### Templates Available:
- **Host Your Own AI Deep Research Agent with n8n, Apify and OpenAI o3**
- **API Schema Extractor**
- **Realtime Notion Todoist 2-way Sync with Redis**
- **Browse Execution Data integration templates**

#### Related Nodes:
- **Code**: Access execution metadata programmatically
- **n8n**: Workflow management and execution operations
- **Edit Fields (Set)**: Prepare metadata values for storage
- **Switch**: Conditional execution based on metadata
- **HTTP Request**: Send execution metadata to external systems

#### Best Practices:

**1. Metadata Design:**
- **Consistent naming**: Use standardized key naming conventions
- **Meaningful values**: Store actionable and searchable information
- **Character limits**: Design keys and values within length constraints
- **Expression efficiency**: Use efficient expressions for dynamic metadata

**2. Performance Optimization:**
- **Selective metadata**: Only store necessary metadata to reduce overhead
- **Efficient expressions**: Optimize metadata generation expressions
- **Batch processing**: Group related metadata operations
- **Storage considerations**: Monitor metadata storage impact

**3. Search Strategy:**
- **Searchable keys**: Design metadata keys for effective searching
- **Filter-friendly values**: Use values that support filtering operations
- **Hierarchical organization**: Structure metadata for drill-down searches
- **Export compatibility**: Design metadata for reporting and analytics

**4. Compliance and Security:**
- **Data privacy**: Avoid storing sensitive information in metadata
- **Retention policies**: Consider metadata retention requirements
- **Access control**: Ensure metadata aligns with security policies
- **Audit requirements**: Include necessary audit trail information

#### Technical Implementation:

**Storage Mechanism**:
- **Database integration**: Metadata stored in n8n execution database
- **Indexing**: Metadata indexed for efficient searching
- **Backup inclusion**: Metadata included in execution backups
- **Migration support**: Metadata preserved during system migrations

**Performance Considerations**:
- **Storage overhead**: Minimal impact on execution performance
- **Search performance**: Indexed metadata enables fast searches
- **Memory usage**: Metadata cached for active executions
- **Database growth**: Monitor metadata impact on database size

#### Troubleshooting:

**Common Issues**:
- **Character limits**: Keys or values exceed maximum length
- **Plan restrictions**: Feature not available on current plan
- **Expression errors**: Invalid expressions in metadata generation
- **Search problems**: Metadata not appearing in search results

**Resolution Steps**:
- **Validate lengths**: Check key/value character counts
- **Verify plan**: Ensure Pro or Enterprise plan access
- **Test expressions**: Validate metadata generation expressions
- **Refresh searches**: Clear search filters and retry

#### Error Handling:

**Metadata Validation**:
- **Length checking**: Validate key/value lengths before storage
- **Expression validation**: Test metadata expressions thoroughly
- **Fallback values**: Provide default values for failed expressions
- **Error logging**: Monitor metadata storage errors

**Graceful Degradation**:
- **Truncation handling**: Accept automatic truncation when necessary
- **Workflow continuation**: Ensure metadata errors don't halt execution
- **Error notification**: Log metadata issues for review
- **Recovery procedures**: Implement metadata correction workflows

#### Security Considerations:
- **Data privacy**: Avoid storing personally identifiable information
- **Access control**: Ensure metadata respects workflow permissions
- **Encryption**: Metadata stored with same security as execution data
- **Audit logging**: Metadata access logged for security monitoring

#### Monitoring and Analytics:
- **Usage tracking**: Monitor metadata utilization across workflows
- **Search analytics**: Track most-used metadata for optimization
- **Performance impact**: Monitor metadata impact on system performance
- **Compliance reporting**: Generate compliance reports from metadata

---

### 22. Extract From File
**Status**: ✅ Active
**Category**: Core - File Processing/Data Extraction
**Purpose**: Extract data from binary format files and convert to JSON for easy manipulation within workflows

#### Authentication:
- **No credentials required**: Direct file processing operations
- **Binary data input**: Requires files passed as binary data from previous nodes
- **Format support**: Multiple file formats with automatic content extraction

#### Operations (11 total):

**1. Extract From CSV** - Extract tabulated data from Comma Separated Values files
- **Use case**: Process spreadsheet data and tabular information
- **Output**: Each row becomes a JSON object with column data
- **Common source**: Data exports, reports, customer lists

**2. Extract From HTML** - Extract fields from standard web page HTML format files
- **Use case**: Parse downloaded web pages and HTML documents
- **Output**: Structured data extracted from HTML elements
- **Applications**: Web scraping, content analysis, data mining

**3. Extract From JSON** - Extract JSON data from a binary file
- **Use case**: Process JSON files received as binary attachments
- **Output**: Parsed JSON objects ready for workflow processing
- **Applications**: API response processing, configuration file parsing

**4. Extract From ICS** - Extract fields from iCalendar format files
- **Use case**: Process calendar data and event information
- **Output**: Event details, dates, times, attendees, locations
- **Applications**: Calendar integration, event management, scheduling

**5. Extract From ODS** - Extract fields from OpenDocument Spreadsheet files
- **Use case**: Process LibreOffice/OpenOffice spreadsheet data
- **Output**: Spreadsheet rows and columns as structured data
- **Applications**: Cross-platform spreadsheet processing

**6. Extract From PDF** - Extract fields from Portable Document Format files
- **Use case**: Extract text content from PDF documents
- **Output**: Text content and document structure
- **Applications**: Document processing, invoice parsing, report analysis

**7. Extract From RTF** - Extract fields from Rich Text Format files
- **Use case**: Process formatted text documents
- **Output**: Text content with basic formatting information
- **Applications**: Document conversion, content extraction

**8. Extract From Text File** - Extract fields from standard text files
- **Use case**: Process plain text files and logs
- **Output**: Text content as structured data
- **Applications**: Log analysis, configuration processing, text mining

**9. Extract From XLS** - Extract fields from Microsoft Excel files (older format)
- **Use case**: Process legacy Excel spreadsheets (.xls format)
- **Output**: Spreadsheet data as JSON objects
- **Applications**: Legacy data migration, Excel file processing

**10. Extract From XLSX** - Extract fields from Microsoft Excel files (modern format)
- **Use case**: Process modern Excel spreadsheets (.xlsx format)
- **Output**: Spreadsheet data with support for multiple sheets
- **Applications**: Business data processing, report analysis

**11. Move File to Base64 String** - Convert binary data to base64 text format
- **Use case**: Convert binary files to text-friendly base64 encoding
- **Output**: Base64 encoded string representation
- **Applications**: File transmission, API integration, data storage

#### Core Parameters:

**1. Input Binary Field**
- **Purpose**: Specify field name containing the binary file data
- **Default**: "data" (standard field name for binary content)
- **Source**: Field from previous node containing file as binary data
- **Examples**: "attachment", "file", "document", "binary_data"

**2. Destination Output Field** (Available for specific operations)
- **Purpose**: Name of field in output that will contain extracted data
- **Available for**: Extract From JSON, Extract From ICS, Extract From Text File, Move File to Base64 String
- **Customization**: Choose meaningful field names for extracted content
- **Organization**: Structure output data with clear field naming

#### File Source Integration:

**Common File Sources:**
- **HTTP Request node**: Download files from web sources
- **Webhook node**: Receive files sent to workflow endpoints
- **Email Trigger (IMAP)**: Process email attachments
- **Read/Write Files from Disk**: Local file system access
- **FTP node**: Files retrieved from FTP servers

**Webhook File Reception:**
- **Raw body setting**: Enable "Raw body" in Webhook node options
- **Binary output**: Webhook outputs file as binary data for Extract From File
- **File upload**: Handle file uploads through webhook endpoints
- **Content type**: Automatic detection of file formats

#### Key Features:

**Format Detection:**
- **Automatic processing**: Recognizes file formats based on operation selection
- **Content parsing**: Extracts meaningful data structures from binary content
- **Error handling**: Graceful handling of malformed or unsupported files
- **Encoding support**: Handles various character encodings and formats

**Data Structure Output:**
- **JSON conversion**: All extracted data converted to JSON format
- **Row/column mapping**: Spreadsheet data mapped to object structures
- **Field naming**: Automatic field name generation from source data
- **Nested objects**: Support for complex data structures

**Workflow Integration:**
- **Seamless processing**: Output ready for immediate use by subsequent nodes
- **Data manipulation**: Extracted JSON can be processed by Edit Fields, Filter, etc.
- **Conditional logic**: Use extracted data for workflow branching and decisions
- **Storage options**: Save processed data to databases, files, or APIs

#### Use Cases:

**1. Document Processing Workflows:**
- **Invoice processing**: Extract data from PDF invoices for accounting systems
- **Report analysis**: Process Excel reports and extract key metrics
- **Form processing**: Extract data from filled PDF forms
- **Document archival**: Convert documents to structured data for storage

**2. Data Integration Pipelines:**
- **CSV import**: Process uploaded CSV files for database import
- **Excel processing**: Handle Excel file uploads for business applications
- **Calendar integration**: Process ICS files for calendar synchronization
- **Configuration management**: Parse configuration files and apply settings

**3. Content Management:**
- **File upload processing**: Handle user file uploads in web applications
- **Email attachment processing**: Extract data from email attachments automatically
- **Batch file processing**: Process multiple files in automated workflows
- **Data migration**: Convert legacy file formats to modern data structures

**4. Business Automation:**
- **Order processing**: Extract order data from uploaded spreadsheets
- **Customer data import**: Process customer lists from various file formats
- **Inventory management**: Update inventory from uploaded Excel files
- **Financial reporting**: Process financial data from accounting file exports

#### Integration Examples:

**1. Invoice Processing System:**
```
Email Trigger → Extract From File (PDF) → Edit Fields (Set) → Database Insert
```
- Monitor email for invoice attachments
- Extract invoice data from PDF files
- Structure data for database storage
- Automatically process invoices

**2. Customer Data Import:**
```
Webhook → Extract From File (XLSX) → Filter → HTTP Request (CRM API)
```
- Receive customer data file uploads
- Extract customer information from Excel
- Filter and validate data
- Import to CRM system

**3. Document Archive:**
```
Local File Trigger → Extract From File → Edit Fields (Set) → Cloud Storage
```
- Monitor folder for new documents
- Extract text content from various formats
- Add metadata and timestamps
- Store in cloud document management

**4. Calendar Synchronization:**
```
HTTP Request → Extract From File (ICS) → Date & Time → Calendar API
```
- Download calendar files from external sources
- Extract event information
- Convert time zones and formats
- Sync with internal calendar system

#### Advanced Features:

**1. Multi-Format Support:**
- **Universal processing**: Handle multiple file formats in single workflow
- **Format detection**: Automatic format recognition and appropriate processing
- **Conversion chains**: Convert between formats using multiple Extract/Convert nodes
- **Batch processing**: Process mixed file types in automated workflows

**2. Error Handling:**
- **Format validation**: Detect and handle unsupported file formats gracefully
- **Content validation**: Verify file content integrity before processing
- **Fallback processing**: Alternative processing paths for problematic files
- **Error reporting**: Detailed error messages for troubleshooting

**3. Performance Optimization:**
- **Streaming processing**: Efficient handling of large files
- **Memory management**: Optimized memory usage for file processing
- **Parallel processing**: Handle multiple files simultaneously
- **Caching**: Cache processed results for repeated operations

#### Technical Details:

**Supported File Formats:**
- **Text formats**: CSV, TXT, JSON, HTML, RTF
- **Spreadsheet formats**: XLS, XLSX, ODS
- **Document formats**: PDF (text extraction)
- **Calendar formats**: ICS (iCalendar)
- **Binary formats**: Any file for Base64 conversion

**Processing Capabilities:**
- **Character encoding**: UTF-8, ASCII, and other standard encodings
- **Large files**: Efficient processing of large spreadsheets and documents
- **Complex structures**: Nested data, multiple sheets, rich formatting
- **Metadata extraction**: File properties, creation dates, author information

**Output Structures:**
- **Spreadsheet data**: Rows as objects with column names as keys
- **Document text**: Paragraphs, sections, and formatting information
- **Calendar events**: Event objects with dates, times, attendees, locations
- **HTML content**: Structured data from HTML elements and attributes

#### Templates Available:
- **Building Your First WhatsApp Chatbot**
- **Extract text from a PDF file**
- **Scrape and store data from multiple website pages**
- **Browse Extract From File integration templates**

#### Related Nodes:
- **Convert to File**: Opposite operation (JSON to binary files)
- **HTTP Request**: Download files for processing
- **Webhook**: Receive uploaded files
- **Read/Write Files from Disk**: Local file operations
- **Edit Fields (Set)**: Process extracted data

#### Best Practices:

**1. File Handling:**
- **Validate sources**: Ensure files come from trusted sources
- **Check formats**: Verify file formats match expected operations
- **Error handling**: Implement proper error handling for malformed files
- **Size limits**: Consider file size limitations and processing time

**2. Data Processing:**
- **Field mapping**: Use clear field names for extracted data
- **Data validation**: Validate extracted data before further processing
- **Type conversion**: Handle data type conversions appropriately
- **Null handling**: Manage missing or empty data gracefully

**3. Performance:**
- **File size**: Monitor processing time for large files
- **Memory usage**: Be aware of memory requirements for file processing
- **Batch processing**: Group similar operations for efficiency
- **Async operations**: Use appropriate execution modes for large files

**4. Security:**
- **Input validation**: Validate file content and structure
- **Malware scanning**: Consider security scanning for uploaded files
- **Access control**: Restrict file access and processing permissions
- **Data sanitization**: Clean extracted data before storage or processing

#### Troubleshooting:

**Common Issues:**
- **Binary field not found**: Verify Input Binary Field name matches source
- **Unsupported format**: Check file format matches selected operation
- **Encoding issues**: Verify character encoding compatibility
- **Large file timeouts**: Monitor processing time for large files

**Resolution Steps:**
- **Check field names**: Ensure binary field name matches previous node output
- **Validate file format**: Confirm file format and operation selection
- **Test with small files**: Verify processing with smaller test files first
- **Monitor memory usage**: Check system resources during file processing

#### Error Handling:

**File Processing Errors:**
- **Invalid format**: Handle files that don't match expected format
- **Corrupted files**: Manage corrupted or incomplete file content
- **Encoding errors**: Handle character encoding issues gracefully
- **Size limitations**: Manage files that exceed processing limits

**Workflow Integration:**
- **Error workflows**: Implement error handling workflows for failed processing
- **Retry logic**: Add retry mechanisms for transient failures
- **Fallback processing**: Alternative processing paths for problem files
- **Notification systems**: Alert on processing failures

#### Security Considerations:
- **File validation**: Validate file content and structure before processing
- **Malicious content**: Scan for potentially harmful file content
- **Access permissions**: Ensure appropriate file access permissions
- **Data privacy**: Handle sensitive file content according to privacy requirements

#### Performance Considerations:
- **File size impact**: Large files require more processing time and memory
- **Concurrent processing**: Multiple file operations may impact system performance
- **Memory management**: Monitor memory usage with large or multiple files
- **Network transfer**: Consider network overhead for file downloads

---

### 23. Filter
**Status**: ✅ Active
**Category**: Core - Data Processing/Conditional Logic
**Purpose**: Filter items based on conditions. Only items that meet the specified conditions pass through to the next node, while items that don't meet conditions are omitted from output.

#### Authentication:
- **No credentials required**: Direct data processing operation
- **Condition-based filtering**: Uses comparison logic to evaluate data
- **Expression support**: Full n8n expression capabilities in conditions

#### Operations:
**1. Filter Items** - Evaluate items against conditions and pass through matching items only

#### Core Parameters:

**1. Conditions** - Define filter criteria for data evaluation:

**Condition Structure:**
- **Data Type Selection**: Choose data type and comparison operation
- **Field References**: Use expressions or direct field references
- **Value Comparison**: Set comparison values or expressions
- **Multiple Conditions**: Add multiple conditions with logical operators

**Data Type Options:**
- **String**: Text-based comparisons and pattern matching
- **Number**: Numeric comparisons and mathematical operations
- **Date & Time**: Date/time comparisons and temporal logic
- **Boolean**: True/false evaluations
- **Array**: List operations and length comparisons
- **Object**: Object existence and structure validation

#### Combining Conditions:

**Logical Operators:**
- **AND**: Items must meet ALL conditions to pass through
  - **Use case**: Strict filtering requiring multiple criteria
  - **Example**: age > 18 AND status = "active" AND region = "US"
- **OR**: Items meeting ANY condition pass through
  - **Use case**: Flexible filtering with alternative criteria
  - **Example**: priority = "high" OR urgent = true OR escalated = true

**Limitation:**
- **No mixed logic**: Cannot combine AND and OR operators in single Filter node
- **Complex logic**: Use multiple Filter nodes or Switch node for complex conditions

#### Node Options:

**1. Ignore Case**
- **Purpose**: Control case sensitivity for string comparisons
- **On**: Case-insensitive matching ("Apple" = "apple" = "APPLE")
- **Off**: Case-sensitive matching ("Apple" ≠ "apple")
- **Use case**: Flexible text matching regardless of capitalization

**2. Less Strict Type Validation**
- **Purpose**: Enable automatic type conversion during comparisons
- **On**: Attempt to convert types based on comparison operator
  - **Example**: String "123" converts to number 123 for numeric comparisons
  - **Benefit**: Reduces "wrong type" errors
- **Off**: Strict type matching without conversion
- **Use case**: Handle mixed data types gracefully

#### Available Data Type Comparisons:

**String Comparisons (14 operations):**
- **exists** / **does not exist**: Field presence validation
- **is empty** / **is not empty**: Empty string detection
- **is equal to** / **is not equal to**: Exact text matching
- **contains** / **does not contain**: Substring detection
- **starts with** / **does not start with**: Prefix matching
- **ends with** / **does not end with**: Suffix matching
- **matches regex** / **does not match regex**: Pattern matching with regular expressions

**Number Comparisons (10 operations):**
- **exists** / **does not exist**: Field presence validation
- **is empty** / **is not empty**: Null/undefined detection
- **is equal to** / **is not equal to**: Exact numeric matching
- **is greater than** / **is less than**: Numeric range comparisons
- **is greater than or equal to** / **is less than or equal to**: Inclusive range comparisons

**Date & Time Comparisons (10 operations):**
- **exists** / **does not exist**: Field presence validation
- **is empty** / **is not empty**: Date value validation
- **is equal to** / **is not equal to**: Exact date/time matching
- **is after** / **is before**: Temporal sequence comparisons
- **is after or equal to** / **is before or equal to**: Inclusive temporal comparisons

**Boolean Comparisons (8 operations):**
- **exists** / **does not exist**: Field presence validation
- **is empty** / **is not empty**: Boolean value validation
- **is true** / **is false**: Boolean state evaluation
- **is equal to** / **is not equal to**: Boolean value matching

**Array Comparisons (12 operations):**
- **exists** / **does not exist**: Array presence validation
- **is empty** / **is not empty**: Array content validation
- **contains** / **does not contain**: Element existence checking
- **length equal to** / **length not equal to**: Array size validation
- **length greater than** / **length less than**: Array size comparisons
- **length greater than or equal to** / **length less than or equal to**: Inclusive size comparisons

**Object Comparisons (4 operations):**
- **exists** / **does not exist**: Object presence validation
- **is empty** / **is not empty**: Object content validation

#### Key Features:

**Conditional Logic:**
- **Data-driven filtering**: Filter based on actual data values and patterns
- **Multiple condition support**: Combine multiple criteria for complex filtering
- **Type-aware comparisons**: Appropriate operations for each data type
- **Expression integration**: Use n8n expressions for dynamic condition values

**Performance Optimization:**
- **Efficient processing**: Only matching items proceed to next nodes
- **Early termination**: Stop evaluation once condition fails (AND logic)
- **Memory efficiency**: Filtered items don't consume downstream memory
- **Workflow optimization**: Reduce processing load on subsequent nodes

**Flexibility Features:**
- **Case handling**: Configure case sensitivity for string operations
- **Type conversion**: Automatic type conversion for mixed data
- **Regex support**: Advanced pattern matching capabilities
- **Expression support**: Dynamic condition values based on workflow data

#### Use Cases:

**1. Data Quality Control:**
- **Validation filtering**: Remove invalid or incomplete records
- **Format verification**: Filter items with correct data formats
- **Range validation**: Ensure numeric values within acceptable ranges
- **Required field checking**: Filter items with all required fields

**2. Business Logic Implementation:**
- **Status-based filtering**: Process items based on status conditions
- **Date range filtering**: Handle items within specific time periods
- **Geographic filtering**: Process items for specific regions or locations
- **Category filtering**: Separate items by type, category, or classification

**3. Workflow Optimization:**
- **Early filtering**: Remove unnecessary items early in workflow
- **Conditional processing**: Process only relevant items through expensive operations
- **Error prevention**: Filter out problematic data before processing
- **Resource optimization**: Reduce API calls and database operations

**4. Data Segmentation:**
- **Customer segmentation**: Filter customers by demographics, behavior, or value
- **Product filtering**: Separate products by category, price, or availability
- **Event filtering**: Process events based on priority, type, or source
- **Content filtering**: Handle content based on type, quality, or relevance

#### Common Filtering Patterns:

**1. Data Validation:**
```
Conditions:
- email (String) matches regex: ^[^\s@]+@[^\s@]+\.[^\s@]+$
- age (Number) is greater than or equal to: 18
- status (String) is equal to: "active"
Operator: AND
```

**2. Date Range Filtering:**
```
Conditions:
- created_date (Date & Time) is after: {{ $now.minus({days: 30}) }}
- created_date (Date & Time) is before: {{ $now }}
Operator: AND
```

**3. Category-Based Filtering:**
```
Conditions:
- category (String) is equal to: "electronics"
- priority (String) is equal to: "high"
- region (String) is equal to: "north-america"
Operator: OR
```

**4. Complex Data Filtering:**
```
Conditions:
- items (Array) is not empty
- total_amount (Number) is greater than: 100
- customer_type (String) contains: "premium"
Operator: AND
```

#### Integration Examples:

**1. E-commerce Order Processing:**
```
HTTP Request (Orders) → Filter (valid orders) → Process Payment → Send Confirmation
```
- Filter orders with complete information
- Remove test orders or invalid payments
- Process only orders meeting business criteria

**2. Customer Data Pipeline:**
```
Database Query → Filter (active customers) → Segmentation → Marketing Automation
```
- Filter active customer records
- Remove incomplete or invalid profiles
- Segment for targeted marketing campaigns

**3. Content Management:**
```
RSS Feed → Filter (relevant content) → AI Analysis → Social Media Publishing
```
- Filter content by keywords or categories
- Remove inappropriate or irrelevant content
- Process only high-quality content

**4. Data Synchronization:**
```
API Data → Filter (changed records) → Compare Datasets → Database Update
```
- Filter records modified since last sync
- Process only items requiring updates
- Optimize synchronization performance

#### Advanced Filtering Techniques:

**1. Multi-Stage Filtering:**
- **Sequential filters**: Chain multiple Filter nodes for complex logic
- **Staged validation**: Implement progressive data quality checks
- **Performance optimization**: Filter cheap conditions first

**2. Expression-Based Filtering:**
- **Dynamic conditions**: Use expressions for calculated filter values
- **Context-aware filtering**: Filter based on workflow state or environment
- **Date calculations**: Filter using relative dates and time periods

**3. Pattern Matching:**
- **Regex filtering**: Use regular expressions for complex text patterns
- **Format validation**: Ensure data meets specific format requirements
- **Content classification**: Categorize content using pattern matching

**4. Conditional Logic Workarounds:**
- **Multiple Filter nodes**: Implement complex AND/OR combinations
- **Switch node integration**: Use Switch for complex conditional logic
- **Code node fallback**: Implement complex filtering in JavaScript

#### Performance Considerations:

**Optimization Strategies:**
- **Order conditions**: Place most selective conditions first
- **Early filtering**: Filter as early as possible in workflow
- **Index-friendly filtering**: Use database-optimized filter conditions
- **Batch processing**: Group similar filtering operations

**Memory Management:**
- **Large datasets**: Consider pagination for very large data sets
- **Memory usage**: Filtered data releases memory for failed items
- **Processing efficiency**: Reduce downstream processing load
- **Resource optimization**: Minimize resource consumption

#### Error Handling:

**Common Issues:**
- **Type mismatches**: "wrong type" errors in comparisons
- **Missing fields**: Undefined field references in conditions
- **Invalid regex**: Malformed regular expression patterns
- **Date format issues**: Invalid date formats in temporal comparisons

**Resolution Strategies:**
- **Enable type conversion**: Use "Less Strict Type Validation" option
- **Field validation**: Check field existence before comparison
- **Regex testing**: Validate regular expressions before deployment
- **Date standardization**: Standardize date formats in preprocessing

#### Security Considerations:

**Data Privacy:**
- **Sensitive data filtering**: Filter out personally identifiable information
- **Access control**: Ensure filtering respects data access permissions
- **Audit logging**: Log filtering operations for compliance
- **Data retention**: Consider data retention policies in filtering

**Input Validation:**
- **Expression injection**: Validate user-provided expressions
- **Regex safety**: Ensure regex patterns don't cause performance issues
- **Data sanitization**: Clean data before filtering operations
- **Error handling**: Prevent information leakage through error messages

#### Templates Available:
- **Scrape business emails from Google Maps without the use of any third party APIs**
- **Build Your First AI Data Analyst Chatbot**
- **Autonomous AI crawler**
- **Browse Filter integration templates**

#### Related Nodes:
- **If**: Simple conditional branching (deprecated, use Switch instead)
- **Switch**: Advanced conditional routing with multiple outputs
- **Compare Datasets**: Compare data between different sources
- **Remove Duplicates**: Remove duplicate items from datasets
- **Code**: Custom filtering logic with JavaScript/Python

#### Best Practices:

**1. Condition Design:**
- **Clear criteria**: Use specific, unambiguous filter conditions
- **Performance optimization**: Order conditions by selectivity
- **Type consistency**: Ensure data types match comparison operations
- **Expression efficiency**: Use efficient expressions for dynamic values

**2. Error Prevention:**
- **Field validation**: Check field existence before filtering
- **Type checking**: Validate data types before comparison
- **Default values**: Provide fallbacks for missing data
- **Testing**: Test filters with representative data samples

**3. Workflow Integration:**
- **Early filtering**: Filter data as early as possible in workflow
- **Logical grouping**: Group related filtering operations together
- **Documentation**: Document filter logic and business rules
- **Monitoring**: Track filter performance and effectiveness

**4. Maintenance:**
- **Regular review**: Review and update filter conditions regularly
- **Performance monitoring**: Monitor filter performance and effectiveness
- **Business rule updates**: Keep filters aligned with changing business rules
- **Testing**: Test filter changes with real data before deployment

#### Troubleshooting:

**Filter Not Working:**
- **Check field names**: Verify field references match actual data structure
- **Validate conditions**: Ensure conditions are logically correct
- **Test data types**: Confirm data types match comparison operations
- **Review expressions**: Validate n8n expressions in conditions

**Performance Issues:**
- **Optimize condition order**: Place most selective conditions first
- **Reduce complexity**: Simplify complex filtering logic
- **Consider alternatives**: Use database-level filtering when possible
- **Monitor resource usage**: Track memory and CPU consumption

**Data Type Errors:**
- **Enable type conversion**: Use "Less Strict Type Validation" option
- **Standardize data**: Preprocess data to ensure consistent types
- **Handle null values**: Account for missing or null values
- **Type validation**: Validate data types before filtering

#### Common Integration Patterns:

**1. Data Pipeline Pattern:**
```
Data Source → Filter (validation) → Transform → Filter (business rules) → Output
```

**2. Multi-Path Processing:**
```
Input → Filter (condition A) → Process A
      → Filter (condition B) → Process B
      → Filter (condition C) → Process C
```

**3. Quality Control Pattern:**
```
Raw Data → Filter (format validation) → Filter (business validation) → Clean Data
```

**4. Conditional Workflow:**
```
Trigger → Filter (criteria) → Conditional Processing → Results
                           → Alternative Processing → Results
```

#### Advanced Use Cases:

**1. Real-time Data Processing:**
- Filter streaming data for real-time analytics
- Process only relevant events in event-driven architectures
- Implement real-time alerts and notifications

**2. Machine Learning Preprocessing:**
- Filter training data for machine learning models
- Remove outliers and invalid data points
- Prepare clean datasets for AI processing

**3. API Response Filtering:**
- Filter API responses for relevant data only
- Remove unnecessary fields to reduce payload size
- Process only changed or updated records

**4. Compliance and Governance:**
- Filter data based on regulatory requirements
- Implement data retention policies
- Ensure data quality standards compliance

---

### 24. FTP
**Status**: ✅ Active
**Category**: Core - File Transfer/Network Operations
**Purpose**: Access and upload files to FTP or SFTP servers, enabling file transfer operations in workflows

#### Authentication:
- **FTP Credential**: Required for standard FTP connections
- **SFTP Credential**: Required for secure FTP connections
- **Credential Setup**: [FTP Credentials Documentation](../../credentials/ftp/)
- **Protocol Support**: Both FTP and SFTP (Secure File Transfer Protocol)

#### Operations (5 total):

**1. Delete** - Delete a file or folder from the FTP/SFTP server
- **Path**: Remote path to file or folder to delete
- **Delete Options**:
  - **Folder**: Enable to delete folders (not just files)
  - **Recursive**: Delete all files and directories within target directory (requires Folder option)

**2. Download** - Download a file from the FTP/SFTP server
- **Path**: Remote path to file to download
- **Put Output File in Field**: Name of binary field to store downloaded file
- **Output**: File content as binary data for use in subsequent nodes

**3. List** - List folder contents on the FTP/SFTP server
- **Path**: Remote path to directory to list
- **Recursive**: Return all directories/objects recursively (on) or just immediate contents (off)
- **Output**: Array of file/directory information with metadata

**4. Rename** - Rename or move a file or folder on the FTP/SFTP server
- **Old Path**: Existing path of file/folder to rename
- **New Path**: New path for renamed/moved file/folder
- **Rename Options**:
  - **Create Directories**: Recursively create destination directory if it doesn't exist

**5. Upload** - Upload a file to the FTP/SFTP server
- **Path**: Remote path where file will be uploaded
- **Binary File**: Toggle for binary file upload vs text content
  - **Input Binary Field**: Name of binary field containing file (if Binary File enabled)
  - **File Content**: Text content to upload (if Binary File disabled)

#### Core Parameters:

**Path Configuration:**
- **Remote Path**: Full path on FTP/SFTP server for file operations
- **Directory Structure**: Support for nested directories and path navigation
- **Path Validation**: Ensure valid paths for target FTP/SFTP system
- **Expression Support**: Use n8n expressions for dynamic path generation

**File Handling:**
- **Binary Data**: Handle files as binary data for preservation of file integrity
- **Text Content**: Support for text-based file uploads and downloads
- **Field Mapping**: Specify input/output fields for file data
- **Metadata**: Access file properties, sizes, and timestamps

#### Key Features:

**Protocol Support:**
- **FTP**: Standard File Transfer Protocol
- **SFTP**: SSH File Transfer Protocol (secure)
- **Authentication**: Username/password and key-based authentication
- **Connection Management**: Persistent connections for multiple operations

**File Operations:**
- **Upload/Download**: Bidirectional file transfer capabilities
- **Directory Management**: Create, list, and delete directories
- **File Management**: Rename, move, and delete files
- **Recursive Operations**: Handle directory trees and nested structures

**Integration Features:**
- **Binary Data Flow**: Seamless integration with other file-processing nodes
- **Expression Support**: Dynamic path and filename generation
- **Error Handling**: Robust error handling for network and permission issues
- **Batch Operations**: Process multiple files in workflow sequences

#### Use Cases:

**1. File Backup and Archival:**
- **Automated backups**: Upload files to FTP servers for backup storage
- **Log archival**: Transfer log files to archive servers
- **Data retention**: Implement file retention policies with automated deletion
- **Disaster recovery**: Maintain offsite file backups

**2. Data Integration:**
- **File synchronization**: Keep files synchronized between systems
- **Data pipeline**: Transfer files between processing stages
- **Import/export**: Handle file-based data imports and exports
- **Batch processing**: Process files from FTP drop zones

**3. Content Management:**
- **Website deployment**: Upload web files to hosting servers
- **Media distribution**: Distribute media files to content servers
- **Document sharing**: Automated document distribution
- **Asset management**: Manage digital assets across systems

**4. Business Workflows:**
- **Invoice processing**: Download invoices from supplier FTP sites
- **Report distribution**: Upload reports to partner FTP servers
- **Order processing**: Handle file-based order systems
- **Compliance**: Automated regulatory file submissions

#### Common Integration Patterns:

**1. File Processing Pipeline:**
```
Local File Trigger → Read/Write Files → FTP (Upload) → Email Notification
```
- Monitor local directory for new files
- Process files locally
- Upload to FTP server
- Notify stakeholders

**2. Data Synchronization:**
```
Schedule Trigger → FTP (List) → FTP (Download) → Database Insert → FTP (Delete)
```
- Periodically check FTP server for new files
- Download new files
- Process and store data
- Clean up processed files

**3. Backup Workflow:**
```
Schedule Trigger → Read/Write Files → Compression → FTP (Upload)
```
- Scheduled backup operations
- Compress files for efficient transfer
- Upload to backup FTP server

**4. File Distribution:**
```
HTTP Request → Edit Fields (Set) → Convert to File → FTP (Upload)
```
- Generate reports or documents
- Format for distribution
- Upload to partner FTP sites

#### File Upload Requirements:

**Binary Data Sources:**
- **Read/Write Files from Disk**: Load files from local filesystem
- **HTTP Request**: Download files from web sources
- **Email Trigger (IMAP)**: Process email attachments
- **Webhook**: Receive file uploads
- **Extract From File**: Process and re-upload files

**File Preparation:**
- **File format**: Ensure files are in binary data format
- **Field naming**: Use consistent binary field names
- **File validation**: Verify file integrity before upload
- **Size limits**: Consider FTP server and network limitations

#### Advanced Features:

**Directory Operations:**
- **Recursive listing**: Get complete directory tree structures
- **Directory creation**: Automatically create directory paths
- **Batch operations**: Process multiple files and directories
- **Path manipulation**: Dynamic path construction and validation

**Error Handling:**
- **Connection errors**: Handle network connectivity issues
- **Permission errors**: Manage authentication and authorization failures
- **File not found**: Graceful handling of missing files
- **Disk space**: Handle server storage limitations

**Performance Optimization:**
- **Connection reuse**: Efficient connection management
- **Batch transfers**: Group multiple file operations
- **Compression**: Reduce transfer time for large files
- **Concurrent operations**: Parallel file processing when possible

#### Security Considerations:

**Authentication:**
- **Secure credentials**: Store FTP credentials securely
- **Key-based authentication**: Use SSH keys for SFTP when possible
- **Connection encryption**: Prefer SFTP over FTP for secure transfers
- **Access control**: Implement proper file and directory permissions

**Data Protection:**
- **File validation**: Validate file types and content before transfer
- **Virus scanning**: Consider malware scanning for downloaded files
- **Data encryption**: Use SFTP for sensitive file transfers
- **Audit logging**: Log all file transfer operations

#### Technical Details:

**Connection Management:**
- **Protocol support**: FTP (port 21) and SFTP (port 22) protocols
- **Passive/Active modes**: Support for different FTP connection modes
- **Timeout handling**: Configurable connection and transfer timeouts
- **SSL/TLS support**: Secure FTP connections when available

**File Handling:**
- **Binary mode**: Preserve file integrity for all file types
- **Text mode**: Handle text files with appropriate line ending conversion
- **Large files**: Support for large file transfers with progress tracking
- **Resume capability**: Resume interrupted transfers when supported

**Network Considerations:**
- **Firewall compatibility**: Handle NAT and firewall configurations
- **Bandwidth management**: Efficient use of network resources
- **Connection pooling**: Reuse connections for multiple operations
- **IPv6 support**: Support for IPv6 FTP servers

#### Templates Available:
- **Working with Excel spreadsheet files (xls & xlsx)**
- **Download a file and upload it to an FTP Server**
- **Explore n8n Nodes in a Visual Reference Library**
- **Browse FTP integration templates**

#### Related Nodes:
- **Read/Write Files from Disk**: Local file operations
- **HTTP Request**: Download files from web
- **SSH**: Remote server access and file operations
- **Extract From File**: Process downloaded files
- **Convert to File**: Prepare files for upload

#### Integration Examples:

**1. Automated Report Distribution:**
- Generate business reports
- Convert to PDF or Excel format
- Upload to partner FTP sites
- Send notification emails

**2. Data Import Pipeline:**
- Monitor FTP server for incoming data files
- Download and validate files
- Process data and import to database
- Archive or delete processed files

**3. Website Deployment:**
- Build website assets locally
- Compress for deployment
- Upload to web hosting FTP server
- Verify deployment success

**4. Log Collection:**
- Collect log files from various systems
- Compress and timestamp logs
- Upload to centralized logging FTP server
- Implement log retention policies

#### Best Practices:

**1. Connection Management:**
- Use SFTP instead of FTP for secure transfers
- Implement proper credential management
- Handle connection timeouts and retries
- Monitor connection health and performance

**2. File Operations:**
- Validate file paths and names
- Implement proper error handling
- Use binary mode for file integrity
- Consider file size and transfer time limitations

**3. Security:**
- Use secure authentication methods
- Encrypt sensitive file transfers
- Implement access controls and permissions
- Log all file operations for audit trails

**4. Performance:**
- Optimize file transfer batch sizes
- Use compression for large files
- Implement proper timeout values
- Monitor transfer speeds and reliability

#### Troubleshooting:

**Connection Issues:**
- **Authentication failures**: Verify credentials and permissions
- **Network connectivity**: Check firewall and network settings
- **Protocol mismatches**: Ensure FTP vs SFTP protocol alignment
- **Port access**: Verify FTP/SFTP port accessibility

**File Transfer Problems:**
- **Permission denied**: Check file and directory permissions
- **Disk space**: Verify available space on target server
- **File conflicts**: Handle existing file conflicts appropriately
- **Path issues**: Validate remote path syntax and existence

**Performance Issues:**
- **Slow transfers**: Check network bandwidth and server performance
- **Timeouts**: Adjust timeout settings for large files
- **Memory usage**: Monitor memory consumption for large file transfers
- **Connection limits**: Consider server connection limitations

#### Error Handling:

**Common Errors:**
- **File not found**: Handle missing files gracefully
- **Permission denied**: Implement proper error messages and fallbacks
- **Connection timeout**: Retry logic for network issues
- **Disk full**: Handle server storage limitations

**Recovery Strategies:**
- **Retry mechanisms**: Implement intelligent retry logic
- **Fallback options**: Alternative servers or methods
- **Error notifications**: Alert administrators of critical failures
- **Transaction rollback**: Undo partial operations when necessary

#### Monitoring and Maintenance:

**Performance Monitoring:**
- **Transfer speeds**: Track upload/download performance
- **Success rates**: Monitor operation success/failure ratios
- **Connection health**: Track connection stability and reliability
- **Resource usage**: Monitor CPU, memory, and network utilization

**Maintenance Tasks:**
- **Credential rotation**: Regular credential updates
- **Server maintenance**: Coordinate with FTP server maintenance
- **Log cleanup**: Regular cleanup of transfer logs
- **Performance tuning**: Optimize based on usage patterns

#### Advanced Configuration:

**Connection Settings:**
- **Timeout values**: Configure connection and transfer timeouts
- **Retry logic**: Set retry attempts and intervals
- **Connection pooling**: Configure connection reuse settings
- **SSL/TLS options**: Configure encryption settings

**File Handling:**
- **Transfer modes**: Configure binary vs text transfer modes
- **Compression**: Enable compression for large file transfers
- **Resume capability**: Configure resume settings for interrupted transfers
- **Verification**: Enable file integrity verification

---

### 25. Git
**Status**: ✅ Active
**Category**: Core - Version Control/Development
**Purpose**: Interact with Git repositories for version control operations, enabling automated Git workflows within n8n

#### Authentication:
- **Git Credential**: Required for operations involving remote repositories
- **Credential Setup**: [Git Credentials Documentation](../../credentials/git/)
- **Authentication Types**: Username/password, SSH keys, personal access tokens
- **Protocol Support**: HTTPS, SSH, and local repository access

#### Operations (13 total):

**1. Add** - Add files or folders to Git staging area (git add)
- **Repository Path**: Local path to Git repository
- **Paths to Add**: Comma-separated list of file/folder paths to stage
- **Path Types**: Absolute or relative paths from Repository Path

**2. Add Config** - Add or set Git configuration properties (git config)
- **Repository Path**: Local path to Git repository
- **Key**: Configuration key name to set
- **Value**: Configuration value to assign
- **Add Config Options**:
  - **Mode**: Set (replace) or Append (add to existing)

**3. Clone** - Clone a remote repository to local system (git clone)
- **Repository Path**: Local path for cloned repository
- **Authentication**: Authenticate (with credentials) or None
- **Credential for Git**: Git credentials (if authentication enabled)
- **New Repository Path**: Local destination path for clone
- **Source Repository**: URL or path of repository to clone

**4. Commit** - Commit staged changes to repository (git commit)
- **Repository Path**: Local path to Git repository
- **Message**: Commit message text
- **Commit Options**:
  - **Paths to Add**: Specific files to commit (blank = all staged files)

**5. Fetch** - Fetch updates from remote repository (git fetch)
- **Repository Path**: Local path to Git repository
- **Purpose**: Download objects and refs from remote without merging

**6. List Config** - Return current Git configuration (git config query)
- **Repository Path**: Local path to Git repository
- **Output**: Current Git configuration settings

**7. Log** - Return Git commit history (git log)
- **Repository Path**: Local path to Git repository
- **Return All**: Return all commits (on) or limited results (off)
- **Limit**: Maximum number of commits to return (if Return All off)
- **Log Options**:
  - **File**: Path to specific file/folder for history filtering

**8. Pull** - Pull updates from remote repository (git pull)
- **Repository Path**: Local path to Git repository
- **Purpose**: Fetch and merge changes from remote branch

**9. Push** - Push commits to remote repository (git push)
- **Repository Path**: Local path to Git repository
- **Authentication**: Authenticate (with credentials) or None
- **Credential for Git**: Git credentials (if authentication enabled)
- **Push Options**:
  - **Target Repository**: URL or path of repository to push to

**10. Push Tags** - Push tags to remote repository (git push --tags)
- **Repository Path**: Local path to Git repository
- **Purpose**: Push all local tags to remote repository

**11. Status** - Return status of current repository (git status)
- **Repository Path**: Local path to Git repository
- **Output**: Current working directory status and staged changes

**12. Tag** - Create a new Git tag (git tag)
- **Repository Path**: Local path to Git repository
- **Name**: Name of tag to create
- **Purpose**: Mark specific commits with version labels

**13. User Setup** - Configure Git user settings
- **Repository Path**: Local path to Git repository
- **Purpose**: Set up user identity for commits

#### Key Features:

**Version Control Operations:**
- **Full Git workflow**: Complete set of Git operations for automation
- **Repository management**: Clone, fetch, pull, push operations
- **Commit lifecycle**: Add, commit, tag, and push changes
- **Configuration management**: Set and query Git configuration

**Integration Capabilities:**
- **Workflow automation**: Automate Git operations within n8n workflows
- **Remote repository support**: Work with GitHub, GitLab, Bitbucket, etc.
- **Authentication handling**: Secure credential management for remote operations
- **Path flexibility**: Support for absolute and relative path specifications

**Development Workflow Support:**
- **CI/CD integration**: Build automated deployment and integration workflows
- **Backup automation**: Automated repository backup and synchronization
- **Code management**: Automated code commit and versioning
- **Configuration deployment**: Manage configuration files through Git

#### Use Cases:

**1. Automated Backup Systems:**
- **Workflow backup**: Automatically backup n8n workflows to Git repositories
- **Configuration backup**: Version control system configurations
- **Data backup**: Backup important data files with version history
- **Scheduled commits**: Regular automated commits of changing files

**2. CI/CD Automation:**
- **Code deployment**: Automated code deployment from Git repositories
- **Configuration management**: Deploy configuration changes through Git
- **Version management**: Automated tagging and release management
- **Integration testing**: Trigger builds and tests from Git changes

**3. Content Management:**
- **Documentation updates**: Automated documentation commits
- **Website deployment**: Deploy website changes from Git repositories
- **Asset management**: Version control for digital assets and media
- **Content synchronization**: Sync content between systems via Git

**4. Development Workflows:**
- **Automated commits**: Commit generated files or reports
- **Branch management**: Automated branch creation and merging
- **Code review automation**: Automated code analysis and commits
- **Release automation**: Automated version tagging and release preparation

#### Common Integration Patterns:

**1. Workflow Backup System:**
```
Schedule Trigger → n8n (Get Workflows) → Convert to File → Git (Add) → Git (Commit) → Git (Push)
```
- Regular backup of n8n workflows
- Convert workflows to JSON files
- Commit and push to backup repository

**2. Configuration Deployment:**
```
Webhook → Extract From File → Edit Fields (Set) → Git (Clone) → Git (Add) → Git (Commit) → Git (Push)
```
- Receive configuration updates
- Clone target repository
- Update configuration files
- Commit and deploy changes

**3. Automated Documentation:**
```
HTTP Request → Edit Fields (Set) → Convert to File → Git (Pull) → Git (Add) → Git (Commit) → Git (Push)
```
- Generate documentation from APIs
- Update documentation repository
- Commit documentation changes

**4. Release Management:**
```
Manual Trigger → Git (Log) → Edit Fields (Set) → Git (Tag) → Git (Push Tags) → Send Email
```
- Review recent commits
- Create version tags
- Push tags to repository
- Notify team of release

#### Advanced Features:

**Repository Management:**
- **Multiple repositories**: Work with multiple Git repositories in workflows
- **Branch operations**: Support for different branches and branch switching
- **Remote management**: Handle multiple remote repositories
- **Submodule support**: Work with Git submodules and complex repository structures

**Configuration Options:**
- **Local vs Global**: Configure repository-specific or global Git settings
- **User identity**: Set author information for commits
- **Repository settings**: Configure repository-specific options
- **Credential management**: Secure handling of authentication credentials

**Workflow Integration:**
- **Dynamic paths**: Use expressions for dynamic repository and file paths
- **Conditional operations**: Conditional Git operations based on workflow data
- **Error handling**: Robust error handling for Git operation failures
- **Status checking**: Check repository status before operations

#### Security Considerations:

**Credential Management:**
- **Secure storage**: Store Git credentials securely in n8n credential system
- **Access control**: Limit access to Git credentials and repositories
- **Key management**: Use SSH keys for secure authentication
- **Token rotation**: Regular rotation of personal access tokens

**Repository Security:**
- **Private repositories**: Secure access to private repositories
- **Permission validation**: Ensure proper permissions for Git operations
- **Audit logging**: Log all Git operations for security monitoring
- **Branch protection**: Respect branch protection rules and policies

#### Technical Details:

**Git Command Integration:**
- **Native Git commands**: Direct mapping to standard Git CLI commands
- **Command options**: Support for Git command options and flags
- **Path handling**: Proper handling of file paths and repository locations
- **Output parsing**: Parse Git command output for workflow use

**Error Handling:**
- **Git errors**: Handle Git-specific error conditions
- **Network issues**: Manage network-related failures for remote operations
- **Permission errors**: Handle authentication and permission failures
- **Repository state**: Validate repository state before operations

**Performance Optimization:**
- **Local operations**: Efficient handling of local Git operations
- **Network optimization**: Minimize network calls for remote operations
- **Large repositories**: Handle large repositories and history efficiently
- **Concurrent operations**: Support for concurrent Git operations

#### Templates Available:
- **Back Up Your n8n Workflows To Github**
- **Building RAG Chatbot for Movie Recommendations with Qdrant and Open AI**
- **ChatGPT Automatic Code Review in Gitlab MR**
- **Browse Git integration templates**

#### Related Nodes:
- **Execute Command**: Alternative for custom Git commands
- **HTTP Request**: Git API operations (GitHub, GitLab, etc.)
- **SSH**: Remote repository access
- **Read/Write Files from Disk**: File operations for Git workflows
- **Convert to File**: Prepare files for Git operations

#### Integration Examples:

**1. n8n Workflow Backup:**
- Export workflows using n8n node
- Convert to JSON files
- Commit to backup repository
- Push to remote Git hosting

**2. Configuration Management:**
- Monitor configuration changes
- Update Git repository with changes
- Deploy configurations to target systems
- Track configuration history

**3. Documentation Automation:**
- Generate API documentation
- Update documentation repository
- Commit and push documentation changes
- Trigger documentation site rebuilds

**4. Code Review Automation:**
- Monitor code changes in repositories
- Run automated analysis
- Commit analysis results
- Create pull requests with findings

#### Best Practices:

**1. Repository Management:**
- Use descriptive commit messages
- Implement proper branching strategies
- Regular repository maintenance and cleanup
- Monitor repository size and performance

**2. Security:**
- Use SSH keys or personal access tokens
- Implement proper access controls
- Regular credential rotation
- Audit Git operations and access

**3. Workflow Design:**
- Check repository status before operations
- Implement error handling for Git failures
- Use meaningful commit messages with context
- Test Git operations in development environment

**4. Performance:**
- Minimize clone operations for large repositories
- Use fetch instead of clone when possible
- Implement proper cleanup of temporary repositories
- Monitor Git operation performance

#### Troubleshooting:

**Common Issues:**
- **Authentication failures**: Check credentials and permissions
- **Repository not found**: Verify repository paths and URLs
- **Merge conflicts**: Handle merge conflicts in pull operations
- **Permission denied**: Check file and directory permissions

**Resolution Strategies:**
- **Credential validation**: Test Git credentials independently
- **Path verification**: Ensure repository paths are correct
- **Network connectivity**: Verify network access to remote repositories
- **Git status checking**: Check repository status before operations

#### Advanced Use Cases:

**1. Multi-Repository Management:**
- Synchronize changes across multiple repositories
- Manage configuration deployment to multiple environments
- Coordinate releases across multiple projects
- Cross-repository dependency management

**2. Automated Testing Integration:**
- Trigger tests on Git commits
- Commit test results and reports
- Manage test branch lifecycle
- Automated rollback on test failures

**3. Content Publishing:**
- Automated blog post publishing from Git
- Documentation site generation from repositories
- Asset pipeline management through Git
- Content versioning and rollback capabilities

**4. Infrastructure as Code:**
- Deploy infrastructure changes from Git
- Version control infrastructure configurations
- Automated infrastructure testing and validation
- Rollback capabilities for infrastructure changes

#### Monitoring and Maintenance:

**Repository Health:**
- Monitor repository size and growth
- Track commit frequency and patterns
- Monitor merge conflicts and resolution
- Analyze repository access patterns

**Performance Monitoring:**
- Git operation execution times
- Network latency for remote operations
- Repository clone and fetch performance
- Error rates and failure patterns

**Maintenance Tasks:**
- Regular credential rotation and validation
- Repository cleanup and optimization
- Branch management and pruning
- Archive old or unused repositories

---

### 26. GraphQL
**Status**: ✅ Active
**Category**: Core - API/Data Query
**Purpose**: Query GraphQL endpoints and retrieve data using GraphQL query language

#### AI Tool Capability:
- **AI Enhancement**: Can be used to enhance AI agent capabilities
- **Automated Configuration**: Many parameters can be set automatically or with AI direction
- **AI Integration**: Part of AI tool parameters documentation for agent workflows

#### Authentication:
- **Multiple Auth Types**: Support for various authentication methods
- **None**: No authentication required
- **Credential Selection**: Choose existing or create new authentication credentials
- **Flexible Auth**: Supports different authentication types based on GraphQL endpoint requirements

#### Operations:
**1. Query GraphQL Endpoint** - Execute GraphQL queries against specified endpoints

#### Core Parameters:

**1. Authentication**
- **Type Selection**: Choose authentication method for GraphQL endpoint
- **Credential Management**: Select existing or create new credentials
- **Auth Types**: None, API Key, OAuth, Basic Auth, and other supported methods
- **Flexible Configuration**: Adapt to different GraphQL service requirements

**2. HTTP Request Method**
- **GET**: Simple query method for basic GraphQL requests
- **POST**: Advanced method with payload configuration options
  - **Request Format Options**:
    - **GraphQL (Raw)**: Send raw GraphQL query in request body
    - **JSON**: Send query as JSON payload structure

**3. Endpoint**
- **GraphQL URL**: Complete URL to GraphQL endpoint
- **Expression Support**: Use n8n expressions for dynamic endpoint construction
- **Service Integration**: Connect to any GraphQL-compliant API or service

**4. Query**
- **GraphQL Language**: Write queries using standard GraphQL syntax
- **Query Structure**: Support for queries, mutations, and subscriptions
- **Variable Support**: Use GraphQL variables within queries
- **Fragment Support**: Utilize GraphQL fragments for reusable query parts

**5. Response Format**
- **JSON** (default): Standard JSON response format for easy data manipulation
- **String**: Raw string response format
  - **Response Data Property Name**: Define property name for string data storage
  - **Custom Field**: Specify field name for string response storage

**6. Ignore SSL Issues**
- **SSL Validation**: Toggle SSL certificate validation
- **Development Mode**: Useful for development environments with self-signed certificates
- **Security Consideration**: Use cautiously in production environments

#### Headers Configuration:

**Custom Headers:**
- **Name/Value Pairs**: Add custom HTTP headers to requests
- **Authentication Headers**: Manual authentication header configuration
- **API Requirements**: Include required headers for specific GraphQL services
- **Content-Type**: Automatic or manual content-type specification

#### Key Features:

**GraphQL Protocol Support:**
- **Query Language**: Full GraphQL query language support
- **Schema Introspection**: Support for GraphQL schema discovery
- **Type System**: Work with strongly-typed GraphQL schemas
- **Real-time Data**: Support for GraphQL subscriptions (where supported)

**Flexible Request Handling:**
- **Multiple HTTP Methods**: GET and POST support for different use cases
- **Payload Formats**: Raw GraphQL or JSON request formats
- **Header Customization**: Complete control over request headers
- **Error Handling**: Proper GraphQL error response handling

**AI and Automation Integration:**
- **AI Tool Enhancement**: Enhance AI agents with GraphQL data capabilities
- **Dynamic Queries**: Build queries using workflow data and expressions
- **Response Processing**: Structured responses for downstream processing
- **Workflow Integration**: Seamless integration with other n8n nodes

#### Use Cases:

**1. API Data Retrieval:**
- **Efficient Queries**: Fetch only required data fields using GraphQL specificity
- **Nested Data**: Retrieve complex nested data structures in single requests
- **Real-time Updates**: Subscribe to data changes through GraphQL subscriptions
- **API Gateway Integration**: Connect to GraphQL gateway services

**2. Content Management:**
- **Headless CMS**: Query headless CMS systems with GraphQL APIs
- **Content Delivery**: Fetch structured content for websites and applications
- **Multi-source Data**: Aggregate content from multiple GraphQL sources
- **Dynamic Content**: Build dynamic content queries based on user requirements

**3. E-commerce Integration:**
- **Product Catalogs**: Query product information with flexible field selection
- **Order Management**: Retrieve and update order information efficiently
- **Customer Data**: Access customer profiles and preferences
- **Inventory Systems**: Real-time inventory queries and updates

**4. Business Intelligence:**
- **Analytics Data**: Query analytics platforms with GraphQL APIs
- **Reporting**: Generate reports from GraphQL data sources
- **Dashboard Data**: Fetch dashboard metrics and KPIs
- **Performance Monitoring**: Query application performance data

#### Common Integration Patterns:

**1. Data Aggregation Workflow:**
```
Schedule Trigger → GraphQL (Query) → Edit Fields (Set) → Database Insert
```
- Regular data synchronization
- Transform GraphQL response for storage
- Maintain data consistency

**2. Real-time Data Processing:**
```
Webhook → GraphQL (Query) → Filter → Switch → Multiple Actions
```
- Process incoming requests with GraphQL data
- Filter relevant information
- Route to appropriate processing paths

**3. Content Publishing:**
```
Manual Trigger → GraphQL (CMS Query) → Edit Fields (Set) → HTTP Request (Publish)
```
- Retrieve content from headless CMS
- Format for target publishing platform
- Publish to multiple channels

**4. Business Analytics:**
```
Schedule Trigger → GraphQL (Analytics) → Aggregate → Convert to File → Send Email
```
- Regular business report generation
- Aggregate analytics data
- Distribute formatted reports

#### Advanced Features:

**Query Optimization:**
- **Field Selection**: Query only required fields to minimize response size
- **Nested Queries**: Retrieve related data in single requests
- **Fragment Usage**: Reuse query fragments for consistency
- **Variable Substitution**: Dynamic query construction with variables

**Error Handling:**
- **GraphQL Errors**: Handle GraphQL-specific error responses
- **HTTP Errors**: Manage network and HTTP-level errors
- **Schema Validation**: Handle schema validation errors
- **Retry Logic**: Implement retry mechanisms for failed requests

**Performance Optimization:**
- **Query Caching**: Cache frequently used queries and responses
- **Request Optimization**: Minimize request overhead and payload size
- **Connection Pooling**: Efficient connection management for multiple requests
- **Response Processing**: Optimize response parsing and data extraction

#### GraphQL Query Examples:

**1. Basic Data Query:**
```graphql
query GetUsers {
  users {
    id
    name
    email
    profile {
      avatar
      bio
    }
  }
}
```

**2. Query with Variables:**
```graphql
query GetUser($userId: ID!) {
  user(id: $userId) {
    id
    name
    email
    posts {
      title
      content
      createdAt
    }
  }
}
```

**3. Mutation Example:**
```graphql
mutation CreateUser($input: CreateUserInput!) {
  createUser(input: $input) {
    id
    name
    email
    success
  }
}
```

**4. Subscription Example:**
```graphql
subscription OnCommentAdded($postId: ID!) {
  commentAdded(postId: $postId) {
    id
    text
    author {
      name
    }
  }
}
```

#### Integration Examples:

**1. Headless CMS Integration:**
- Query content from Strapi, Contentful, or GraphCMS
- Retrieve page content, blog posts, and media
- Dynamic content generation for websites
- Multi-language content management

**2. E-commerce Platform:**
- Shopify GraphQL API integration
- Product catalog management
- Order processing and fulfillment
- Customer data synchronization

**3. Social Media Analytics:**
- Facebook Graph API queries
- Instagram content retrieval
- Social media metrics collection
- Audience insights and analytics

**4. Development Tools:**
- GitHub GraphQL API integration
- Repository management and analytics
- Issue tracking and project management
- Code review and collaboration workflows

#### Security Considerations:

**Authentication Security:**
- **Secure Credentials**: Store API keys and tokens securely
- **Token Rotation**: Regular authentication token updates
- **Access Scopes**: Use minimal required permissions
- **Rate Limiting**: Respect API rate limits and quotas

**Query Security:**
- **Query Validation**: Validate queries before execution
- **Injection Prevention**: Prevent GraphQL injection attacks
- **Depth Limiting**: Limit query depth to prevent abuse
- **Field Whitelisting**: Restrict accessible fields when possible

**Data Privacy:**
- **Sensitive Data**: Handle personally identifiable information appropriately
- **Data Encryption**: Use HTTPS for all GraphQL communications
- **Access Logging**: Log API access for security monitoring
- **Compliance**: Ensure GDPR and other regulatory compliance

#### Performance Considerations:

**Query Efficiency:**
- **Field Selection**: Request only necessary fields
- **Query Complexity**: Monitor and limit query complexity
- **Caching Strategy**: Implement appropriate caching mechanisms
- **Pagination**: Use pagination for large data sets

**Network Optimization:**
- **Request Batching**: Combine multiple queries when possible
- **Connection Reuse**: Efficient HTTP connection management
- **Compression**: Enable response compression
- **CDN Usage**: Utilize CDNs for GraphQL endpoints when available

#### Templates Available:
- **Get top 5 products on Product Hunt every hour**
- **API queries data from GraphQL**
- **Sentiment Analysis Tracking on Support Issues with Linear and Slack**
- **Browse GraphQL integration templates**

#### Related Resources:
- **GraphQL Introduction**: [GraphQL Learning Guide](https://graphql.org/learn/)
- **Query Language**: Understanding GraphQL syntax and capabilities
- **Schema Design**: Best practices for GraphQL schema development
- **Performance**: GraphQL query optimization techniques

#### Related Nodes:
- **HTTP Request**: Alternative for REST API calls
- **Webhook**: Receive GraphQL webhook events
- **Code**: Custom GraphQL query processing
- **Edit Fields (Set)**: Process GraphQL response data
- **Filter**: Filter GraphQL query results

#### Best Practices:

**1. Query Design:**
- **Specific Fields**: Query only required data fields
- **Proper Fragments**: Use fragments for reusable query parts
- **Variable Usage**: Utilize variables for dynamic queries
- **Error Handling**: Implement comprehensive error handling

**2. Authentication:**
- **Secure Storage**: Store credentials securely
- **Token Management**: Handle token refresh and expiration
- **Permission Scoping**: Use minimal required permissions
- **Access Monitoring**: Monitor API access and usage

**3. Performance:**
- **Query Optimization**: Optimize queries for performance
- **Caching Strategy**: Implement appropriate caching
- **Rate Limiting**: Respect API rate limits
- **Monitoring**: Monitor query performance and errors

**4. Security:**
- **Input Validation**: Validate all query inputs
- **Query Complexity**: Limit query complexity and depth
- **Secure Transport**: Always use HTTPS for production
- **Access Control**: Implement proper access controls

#### Troubleshooting:

**Common Issues:**
- **Authentication Failures**: Check credentials and permissions
- **Query Syntax Errors**: Validate GraphQL query syntax
- **Schema Mismatches**: Verify query fields match schema
- **Network Errors**: Check connectivity and endpoint availability

**Resolution Strategies:**
- **Query Testing**: Test queries independently first
- **Schema Introspection**: Use schema introspection for field discovery
- **Error Analysis**: Analyze GraphQL error responses
- **Logging**: Enable detailed logging for debugging

#### Advanced Use Cases:

**1. Multi-Source Data Federation:**
- **Schema Stitching**: Combine multiple GraphQL APIs
- **Data Federation**: Create unified data layer
- **Cross-Service Queries**: Query across multiple services
- **Unified Interface**: Present single API interface

**2. Real-time Applications:**
- **Subscription Handling**: Implement GraphQL subscriptions
- **Live Data Updates**: Real-time data synchronization
- **Event-Driven Architecture**: Build reactive systems
- **WebSocket Integration**: Handle real-time connections

**3. Microservices Integration:**
- **Service Communication**: Inter-service GraphQL communication
- **API Gateway**: GraphQL as API gateway layer
- **Service Discovery**: Dynamic service endpoint discovery
- **Load Balancing**: Distribute queries across service instances

**4. Analytics and Reporting:**
- **Business Intelligence**: Query BI platforms via GraphQL
- **Custom Analytics**: Build custom analytics solutions
- **Real-time Dashboards**: Power live dashboard data
- **Data Visualization**: Feed visualization tools with GraphQL data

---

### 27. HTML
**Status**: ✅ Active
**Category**: Core - HTML Processing/Web Development
**Purpose**: Work with HTML content - generate HTML templates, extract data from HTML sources, and convert data to HTML tables

#### Node History:
- **Replacement**: Replaces HTML Extract node from version 0.213.0
- **Legacy Support**: Older HTML Extract node documentation available for pre-0.213.0 versions
- **Enhanced Features**: Expanded functionality beyond basic HTML extraction

#### Security Warning:
- **XSS Risk**: Cross-site scripting vulnerability when generating HTML templates with untrusted inputs
- **Security Consideration**: Be careful with un-trusted inputs to prevent XSS attacks
- **Input Validation**: Always validate and sanitize user inputs before HTML generation

#### Operations (3 total):

**1. Generate HTML Template** - Create HTML templates with dynamic content
- **Purpose**: Take workflow data and output it as formatted HTML
- **Template Support**: Standard HTML with CSS, JavaScript, and n8n expressions
- **Use Case**: Generate reports, emails, web pages, and formatted documents

**2. Extract HTML Content** - Extract data from HTML sources using CSS selectors
- **Purpose**: Parse HTML and extract specific content using CSS selectors
- **Source Types**: JSON properties or binary HTML files
- **Use Case**: Web scraping, data extraction, content parsing

**3. Convert to HTML Table** - Transform workflow data into HTML table format
- **Purpose**: Convert structured data into formatted HTML tables
- **Styling Options**: Custom styling, attributes, and formatting control
- **Use Case**: Data presentation, reports, email content

#### Operation Details:

**1. Generate HTML Template**

**Template Capabilities:**
- **Standard HTML**: Full HTML5 support with all standard elements
- **CSS Styling**: Include CSS in `<style>` tags for formatting
- **JavaScript**: Add JavaScript in `<script>` tags (n8n doesn't execute JavaScript)
- **N8N Expressions**: Use expressions wrapped in `{{}}` for dynamic content

**Expression Integration:**
- **Built-in Methods**: Access n8n's built-in methods and variables
- **Workflow Data**: Reference data from previous nodes
- **Dynamic Content**: Generate content based on workflow data
- **Template Logic**: Implement conditional content and loops

**Template Examples:**
```html
<!DOCTYPE html>
<html>
<head>
    <title>{{ $json.reportTitle }}</title>
    <style>
        body { font-family: Arial, sans-serif; }
        .header { background-color: #f0f0f0; padding: 20px; }
        .data-table { border-collapse: collapse; width: 100%; }
    </style>
</head>
<body>
    <div class="header">
        <h1>{{ $json.reportTitle }}</h1>
        <p>Generated on: {{ $now.toFormat('yyyy-MM-dd HH:mm') }}</p>
    </div>
    <table class="data-table">
        {{ $json.items.map(item => `
            <tr>
                <td>${item.name}</td>
                <td>${item.value}</td>
            </tr>
        `).join('') }}
    </table>
</body>
</html>
```

**2. Extract HTML Content**

**Source Data Configuration:**
- **JSON Source**: Extract from JSON property containing HTML string or array
  - **JSON Property**: Name of input field containing HTML content
  - **Array Support**: Handle arrays of HTML strings
- **Binary Source**: Extract from binary HTML file
  - **Input Binary Field**: Name of binary field containing HTML file
  - **File Support**: Process .html files and HTML content

**Extraction Values Setup:**
- **Key**: Field name to save extracted value under
- **CSS Selector**: CSS selector to target specific HTML elements
- **Return Value Options**:
  - **Attribute**: Extract specific attribute values (class, id, href, etc.)
    - **Attribute Name**: Specify attribute to extract
  - **HTML**: Return inner HTML content of selected elements
  - **Text**: Extract text content only
    - **Skip Selectors**: Comma-separated list of selectors to exclude
  - **Value**: Extract value from input, select, or textarea elements
- **Return Array**: Return multiple matches as array or single string

**Extraction Options:**
- **Trim Values**: Remove leading/trailing spaces and newlines
- **Clean Up Text**: Remove extra whitespace and normalize line breaks

**CSS Selector Examples:**
```css
/* Basic selectors */
h1                    /* All h1 elements */
.class-name          /* Elements with specific class */
#element-id          /* Element with specific ID */
[attribute]          /* Elements with specific attribute */

/* Advanced selectors */
div.content p        /* Paragraphs inside div with class content */
table tr:nth-child(2) /* Second row of table */
a[href^="https"]     /* Links starting with https */
.product-name:first-child /* First element with product-name class */
```

**3. Convert to HTML Table**

**Table Generation:**
- **Data Input**: Automatically processes data from previous nodes
- **No Parameters**: No required parameters - works with incoming data structure
- **Field Mapping**: Uses object keys as column headers, values as cell content

**Styling Options:**
- **Capitalize Headers**: Automatic header capitalization toggle
- **Custom Styling**: Enable/disable custom styling options
- **Caption**: Add table caption for accessibility and description
- **Table Attributes**: Custom attributes for `<table>` element
- **Header Attributes**: Custom attributes for `<th>` elements  
- **Row Attributes**: Custom attributes for `<tr>` elements
- **Cell Attributes**: Custom attributes for `<td>` elements

**HTML Table Example Output:**
```html
<table class="data-table" style="border-collapse: collapse;">
    <caption>Monthly Sales Report</caption>
    <thead>
        <tr>
            <th style="background-color: #f0f0f0;">Product Name</th>
            <th style="background-color: #f0f0f0;">Sales Amount</th>
            <th style="background-color: #f0f0f0;">Date</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Product A</td>
            <td>$1,250</td>
            <td>2024-01-15</td>
        </tr>
        <tr>
            <td>Product B</td>
            <td>$850</td>
            <td>2024-01-16</td>
        </tr>
    </tbody>
</table>
```

#### Key Features:

**Template Generation:**
- **Dynamic Content**: Generate HTML with workflow data
- **Expression Support**: Full n8n expression capabilities
- **Styling Integration**: CSS and JavaScript support
- **Responsive Design**: Create mobile-friendly HTML content

**Content Extraction:**
- **CSS Selector Power**: Advanced element targeting capabilities
- **Multiple Return Types**: Flexible data extraction options
- **Batch Processing**: Handle multiple HTML sources
- **Data Cleaning**: Built-in text cleaning and formatting

**Table Conversion:**
- **Automatic Formatting**: Intelligent table generation from data
- **Custom Styling**: Complete control over table appearance
- **Accessibility**: Proper table structure with headers and captions
- **Responsive Tables**: Mobile-friendly table generation

#### Use Cases:

**1. Report Generation:**
- **Business Reports**: Generate formatted HTML reports from data
- **Dashboard Creation**: Create HTML dashboards with charts and tables
- **Email Reports**: Format data for HTML email content
- **PDF Generation**: Create HTML for PDF conversion workflows

**2. Web Scraping and Data Extraction:**
- **Content Parsing**: Extract data from scraped web pages
- **Product Information**: Extract product details from e-commerce sites
- **News Aggregation**: Parse news articles and extract content
- **Social Media**: Extract posts, comments, and metadata

**3. Content Management:**
- **Template Generation**: Create dynamic web page templates
- **Email Templates**: Generate HTML email templates with data
- **Documentation**: Create formatted documentation from data
- **Marketing Materials**: Generate HTML marketing content

**4. Data Presentation:**
- **Table Generation**: Convert datasets to formatted HTML tables
- **Data Visualization**: Create HTML-based data presentations
- **Interactive Content**: Generate interactive HTML elements
- **Responsive Design**: Create mobile-friendly content layouts

#### Integration Examples:

**1. Web Scraping Workflow:**
```
HTTP Request → HTML (Extract Content) → Edit Fields (Set) → Database Insert
```
- Scrape web pages for product information
- Extract specific data using CSS selectors
- Structure extracted data
- Store in database

**2. Report Generation:**
```
Database Query → HTML (Generate Template) → Convert to File → Send Email
```
- Query business data
- Generate formatted HTML report
- Convert to PDF or keep as HTML
- Email to stakeholders

**3. Data Table Creation:**
```
Google Sheets → HTML (Convert to Table) → Edit Fields (Set) → HTTP Request
```
- Read data from spreadsheet
- Convert to formatted HTML table
- Add custom styling and branding
- Publish to web service

**4. Email Template Generation:**
```
Customer Data → HTML (Generate Template) → Send Email
```
- Load customer information
- Generate personalized HTML email
- Send formatted email with dynamic content

#### Advanced Features:

**Expression Integration:**
- **Complex Logic**: Implement conditional content generation
- **Data Transformation**: Transform data during template generation
- **Loop Structures**: Generate repeated content sections
- **Function Calls**: Use n8n built-in functions in templates

**CSS and Styling:**
- **Responsive Design**: Create mobile-friendly layouts
- **Custom Themes**: Implement branded styling
- **CSS Frameworks**: Integrate with Bootstrap, Tailwind, etc.
- **Print Styles**: Optimize for print and PDF generation

**Advanced Extraction:**
- **Complex Selectors**: Use advanced CSS selectors for precise targeting
- **Nested Extraction**: Extract related data from multiple elements
- **Conditional Extraction**: Extract different data based on element attributes
- **Data Validation**: Validate extracted data format and content

#### Security Considerations:

**XSS Prevention:**
- **Input Sanitization**: Always sanitize user inputs before template generation
- **HTML Encoding**: Encode special characters in dynamic content
- **Content Security Policy**: Implement CSP headers for generated content
- **Trusted Sources**: Only process HTML from trusted sources

**Data Privacy:**
- **Sensitive Data**: Avoid including sensitive information in generated HTML
- **Access Control**: Implement proper access controls for generated content
- **Data Encryption**: Encrypt sensitive data in templates when necessary
- **Audit Logging**: Log HTML generation for security monitoring

#### Performance Considerations:

**Template Generation:**
- **Complex Templates**: Monitor performance with large datasets
- **Expression Optimization**: Optimize n8n expressions for performance
- **Memory Usage**: Consider memory impact of large HTML generation
- **Caching**: Cache generated templates when appropriate

**Content Extraction:**
- **Large Documents**: Handle large HTML documents efficiently
- **Selector Optimization**: Use efficient CSS selectors
- **Batch Processing**: Process multiple documents efficiently
- **Memory Management**: Monitor memory usage with large extractions

#### Templates Available:
- **Scrape and summarize webpages with AI**
- **Pulling data from services that n8n doesn't have a pre-built integration for**
- **Automated Web Scraping: email a CSV, save to Google Sheets & Microsoft Excel**
- **Browse HTML integration templates**

#### Related Nodes:
- **HTTP Request**: Download HTML content for processing
- **Extract From File**: Alternative for file-based HTML extraction
- **Convert to File**: Save generated HTML as files
- **Send Email**: Send HTML email content
- **Webhook**: Serve generated HTML content

#### Best Practices:

**1. Template Design:**
- **Responsive Design**: Create mobile-friendly templates
- **Semantic HTML**: Use proper HTML5 semantic elements
- **Accessibility**: Include proper ARIA labels and structure
- **Performance**: Optimize CSS and minimize inline styles

**2. Content Extraction:**
- **Robust Selectors**: Use CSS selectors that handle variations
- **Error Handling**: Handle missing elements gracefully
- **Data Validation**: Validate extracted data format
- **Fallback Options**: Provide fallback extraction methods

**3. Security:**
- **Input Validation**: Always validate and sanitize inputs
- **XSS Prevention**: Implement proper XSS protection
- **Content Security**: Use secure HTML generation practices
- **Access Control**: Implement proper access controls

**4. Performance:**
- **Efficient Selectors**: Use performant CSS selectors
- **Memory Management**: Monitor memory usage with large operations
- **Caching**: Cache frequently used templates and extractions
- **Optimization**: Optimize expressions and template logic

#### Troubleshooting:

**Template Generation Issues:**
- **Expression Errors**: Validate n8n expressions in templates
- **Styling Problems**: Check CSS syntax and conflicts
- **Dynamic Content**: Verify data availability for expressions
- **Output Format**: Ensure proper HTML structure

**Extraction Issues:**
- **Selector Problems**: Verify CSS selector syntax and targeting
- **Missing Data**: Check if target elements exist in HTML
- **Format Issues**: Validate HTML structure and encoding
- **Performance**: Optimize selectors for large documents

**Table Conversion Issues:**
- **Data Structure**: Ensure proper data format for table conversion
- **Styling Conflicts**: Check custom styling compatibility
- **Large Datasets**: Monitor performance with large tables
- **Attribute Errors**: Validate custom attributes syntax

#### Advanced Use Cases:

**1. Dynamic Web Pages:**
- **CMS Integration**: Generate dynamic content management pages
- **User Dashboards**: Create personalized user dashboard content
- **Real-time Updates**: Generate HTML with live data feeds
- **Interactive Elements**: Create HTML with interactive components

**2. Document Generation:**
- **Invoice Generation**: Create HTML invoices from business data
- **Contract Templates**: Generate legal documents with dynamic content
- **Certificates**: Create HTML certificates and awards
- **Reports**: Generate comprehensive business reports

**3. Email Marketing:**
- **Newsletter Templates**: Create dynamic newsletter content
- **Personalized Emails**: Generate personalized marketing emails
- **Transactional Emails**: Create transaction confirmation emails
- **Drip Campaigns**: Generate sequential email content

**4. Web Application Integration:**
- **API Responses**: Generate HTML responses for web APIs
- **Widget Generation**: Create embeddable HTML widgets
- **Landing Pages**: Generate dynamic landing page content
- **Form Generation**: Create dynamic HTML forms

---

### 28. HTTP Request
**Status**: ✅ Active
**Category**: Core - HTTP/API/Networking
**Purpose**: One of the most versatile nodes in n8n - make HTTP requests to query data from any app or service with a REST API. Can be used as a regular node or as an AI agent tool.

#### Authentication:
- **Multiple Authentication Types**: Comprehensive support for various authentication methods
- **Predefined Credentials**: For integrations supported by n8n (easier setup and management)
- **Generic Credentials**: For integrations not natively supported by n8n
- **Custom Operations**: Use with existing node credentials for custom API calls

#### Operations:
**1. HTTP Request** - Make REST API calls to any HTTP endpoint with full configuration control

#### Key Features:

**Versatility:**
- **Universal API Integration**: Connect to any REST API or web service
- **Multiple HTTP Methods**: Support for all standard HTTP methods
- **Flexible Authentication**: Comprehensive authentication method support
- **AI Agent Tool**: Can be attached to AI agents as a tool for dynamic API calls
- **cURL Import**: Import existing cURL commands to auto-configure the node

**Request Configuration:**
- **Dynamic URL Construction**: Use expressions for dynamic endpoint generation
- **Query Parameters**: Flexible parameter configuration with multiple formats
- **Custom Headers**: Complete control over request headers
- **Body Content Types**: Support for multiple body formats and file uploads
- **SSL Configuration**: Handle SSL certificates and security settings

**Response Handling:**
- **Multiple Response Formats**: JSON, Text, File, or Auto-detection
- **Pagination Support**: Built-in pagination handling for large datasets
- **Error Management**: Configurable error handling and retry logic
- **Response Processing**: Headers, status codes, and body content access

#### Core Parameters:

**1. Method** - HTTP request method selection:
- **DELETE**: Remove resources
- **GET**: Retrieve data (most common for data fetching)
- **HEAD**: Get headers without body content
- **OPTIONS**: Get allowed methods and CORS information
- **PATCH**: Partial resource updates
- **POST**: Create new resources or submit data
- **PUT**: Create or fully update resources

**2. URL** - Endpoint Configuration:
- **Target Endpoint**: Complete URL to API endpoint
- **Expression Support**: Use n8n expressions for dynamic URL construction
- **Variable Substitution**: Include workflow data in URLs
- **Protocol Support**: HTTP and HTTPS protocols

**3. Authentication** - Security Configuration:

**Predefined Credentials (Recommended):**
- **Supported Integrations**: Use credentials from existing n8n nodes
- **Custom Operations**: Leverage existing node credentials for custom API calls
- **Easier Management**: Simplified credential setup and maintenance
- **No Extra Setup**: Immediate use with supported services

**Generic Credentials:**
- **Custom Services**: For APIs not natively supported by n8n
- **Manual Configuration**: Full control over authentication process
- **Authentication Methods**:
  - **Basic Auth**: Username/password authentication
  - **Custom Auth**: Custom header/parameter authentication
  - **Digest Auth**: Digest-based authentication
  - **Header Auth**: Custom header-based authentication
  - **OAuth1 API**: OAuth 1.0 authentication flow
  - **OAuth2 API**: OAuth 2.0 authentication flow
  - **Query Auth**: Authentication via query parameters

**4. Send Query Parameters** - URL Parameter Configuration:

**Query Parameter Methods:**
- **Using Fields Below**: Name/Value pairs with Add Parameter option
- **Using JSON**: Define parameters as JSON object
- **Array Format Support**: Control array parameter formatting
- **Expression Integration**: Use workflow data in parameter values

**Array Format Options:**
- **No Brackets**: `foo=bar&foo=qux`
- **Brackets Only**: `foo[]=bar&foo[]=qux`
- **Brackets with Indices**: `foo[0]=bar&foo[1]=qux`

**5. Send Headers** - HTTP Header Configuration:

**Header Configuration Methods:**
- **Using Fields Below**: Name/Value pairs for headers
- **Using JSON**: Define headers as JSON object
- **Custom Headers**: Authentication, content-type, user-agent, etc.
- **Expression Support**: Dynamic header values from workflow data

**6. Send Body** - Request Body Configuration:

**Body Content Types:**

**Form URLencoded** (`application/x-www-form-urlencoded`):
- **Using Fields Below**: Name/Value pairs
- **Using Single Field**: Format as `fieldname1=value1&fieldname2=value2`
- **Use Case**: Traditional HTML form submissions

**Form-Data** (`multipart/form-data`):
- **Form Data**: Name/Value pairs for text fields
- **n8n Binary File**: Upload files from binary data
- **Mixed Content**: Combine text fields and file uploads
- **Use Case**: File uploads and complex form submissions

**JSON** (`application/json`):
- **Using Fields Below**: Build JSON from Name/Value pairs
- **Using JSON**: Write JSON directly
- **Expression Support**: Dynamic JSON construction
- **Use Case**: API calls requiring JSON payloads

**n8n Binary File**:
- **File Upload**: Send file contents as request body
- **Input Data Field Name**: Specify binary field containing file
- **Use Case**: File upload endpoints, document processing

**Raw**:
- **Custom Content Type**: Specify any MIME type
- **Raw Body Content**: Send any format as request body
- **Use Case**: XML, plain text, custom formats

#### Node Options:

**Array Format in Query Parameters:**
- **No Brackets**: Simple parameter repetition
- **Brackets Only**: Array notation with empty brackets
- **Brackets with Indices**: Indexed array notation

**Batching:**
- **Items per Batch**: Number of input items per batch
- **Batch Interval**: Delay between batches in milliseconds
- **Use Case**: Rate limiting, API quotas, performance management

**Ignore SSL Issues:**
- **SSL Validation**: Toggle SSL certificate validation
- **Development Mode**: Useful for self-signed certificates
- **Security Warning**: Use cautiously in production

**Lowercase Headers:**
- **Header Normalization**: Control header name casing
- **Default**: Enabled (lowercase header names)
- **Compatibility**: Some APIs require specific header casing

**Redirects:**
- **Follow Redirects**: Enable/disable redirect following
- **Max Redirects**: Maximum number of redirects to follow
- **Default**: Enabled with reasonable limits

**Response Configuration:**
- **Include Response Headers and Status**: Return full response object
- **Never Error**: Success regardless of HTTP status code
- **Response Format**:
  - **Autodetect**: Automatic format detection based on content
  - **File**: Store response as binary file
  - **JSON**: Parse response as JSON
  - **Text**: Treat response as plain text

**Pagination:**
- **Pagination Mode**:
  - **Off**: No pagination
  - **Update a Parameter in Each Request**: Dynamic parameter updates
  - **Response Contains Next URL**: Use next URL from response
- **Built-in Variables**: `$pageCount`, `$request`, `$response`
- **API Compatibility**: Handle different pagination implementations

**Proxy:**
- **HTTP Proxy**: Configure proxy server for requests
- **Network Requirements**: Corporate networks, geographical restrictions
- **Authentication**: Proxy authentication support

**Timeout:**
- **Response Timeout**: Maximum wait time for server response
- **Header Timeout**: Time to wait for response headers
- **Configuration**: Specified in milliseconds

#### Tool-Only Options (AI Agent Integration):

**Optimize Response** - When attached to AI agents:
- **Purpose**: Reduce data passed to LLM for cost and performance optimization
- **Response Type Configuration**: Customize based on expected response format

**JSON Response Optimization:**
- **Field Containing Data**: Specify relevant data portion
- **Include Fields**: Control which fields to include
  - **All**: Include entire response
  - **Selected**: Include only specified fields
  - **Exclude**: Include all except specified fields
- **Dot Notation**: Support for nested field selection

**HTML Response Optimization:**
- **Selector (CSS)**: Target specific HTML elements
- **Return Only Content**: Strip HTML tags, return text only
- **Elements To Omit**: Exclude specific elements
- **Truncate Response**: Limit response size
- **Max Response Characters**: Character limit (default: 1000)

**Text Response Optimization:**
- **Truncate Response**: Limit response size
- **Max Response Characters**: Character limit (default: 1000)

#### Import cURL Command:

**cURL Integration:**
- **Command Import**: Paste cURL commands directly
- **Auto-Configuration**: Automatically populate node parameters
- **Documentation Integration**: Use API documentation examples
- **Configuration Override**: Replaces existing node configuration

**Import Process:**
1. Select "Import cURL" from Parameters tab
2. Paste cURL command in modal
3. Select "Import" to auto-configure node
4. Review and adjust imported settings

#### Pagination System:

**Built-in Variables:**
- **`$pageCount`**: Current pagination count
- **`$request`**: Request object sent by HTTP node
- **`$response`**: Response object with body, headers, statusCode

**Pagination Modes:**

**Update a Parameter in Each Request:**
- **Dynamic Parameters**: Modify parameters for each page
- **Use Case**: APIs using page numbers, offsets, cursors
- **Configuration**: Update query parameters or headers per request

**Response Contains Next URL:**
- **Next URL Expression**: Extract next page URL from response
- **Use Case**: APIs providing next page URLs in response
- **Expression**: Use `$response.body.nextUrl` or similar

#### Use Cases:

**1. API Integration:**
- **Third-party Services**: Connect to any REST API
- **Data Synchronization**: Sync data between services
- **Webhook Calls**: Make outbound webhook requests
- **Microservices Communication**: Inter-service API calls

**2. Web Scraping:**
- **Data Extraction**: Retrieve data from web services
- **Content Aggregation**: Collect content from multiple sources
- **Monitoring**: Check website status and availability
- **Price Monitoring**: Track pricing from e-commerce APIs

**3. File Operations:**
- **File Downloads**: Download files from web services
- **File Uploads**: Upload files to cloud storage
- **Document Processing**: Send files to processing services
- **Image Manipulation**: Send images to processing APIs

**4. Authentication Testing:**
- **API Testing**: Test different authentication methods
- **Token Management**: Retrieve and refresh authentication tokens
- **Permission Validation**: Test API permissions and access
- **Security Testing**: Validate API security implementations

**5. AI Agent Tools:**
- **Dynamic API Calls**: Enable AI agents to make API calls
- **Data Retrieval**: Allow agents to fetch external data
- **Service Integration**: Connect agents to external services
- **Real-time Information**: Provide agents with current data

#### Common Integration Patterns:

**1. Data Synchronization:**
```
Schedule Trigger → Database Query → HTTP Request (API Update) → Edit Fields (Set)
```
- Regular data synchronization between systems
- Transform data for target API format
- Handle API responses and errors

**2. Webhook Chain:**
```
Webhook → HTTP Request (External API) → Edit Fields (Set) → Respond to Webhook
```
- Receive webhook, enrich with external data
- Process and transform data
- Send enriched response

**3. File Processing Pipeline:**
```
HTTP Request (Download) → Extract From File → Edit Fields (Set) → HTTP Request (Upload)
```
- Download files from source
- Process file contents
- Upload processed data to destination

**4. Authentication Flow:**
```
HTTP Request (Get Token) → Edit Fields (Set) → HTTP Request (API Call)
```
- Retrieve authentication token
- Store token for subsequent requests
- Make authenticated API calls

#### Advanced Features:

**Response Processing:**
- **Status Code Handling**: Handle different HTTP status codes
- **Header Analysis**: Process response headers for metadata
- **Content Type Detection**: Automatic content type handling
- **Error Response Processing**: Handle API error responses

**Request Optimization:**
- **Connection Reuse**: Efficient HTTP connection management
- **Compression**: Handle request/response compression
- **Caching**: Implement request caching strategies
- **Rate Limiting**: Respect API rate limits and quotas

**Security Features:**
- **SSL/TLS Support**: Full SSL/TLS encryption support
- **Certificate Validation**: Configurable certificate validation
- **Secure Authentication**: Multiple secure authentication methods
- **Token Management**: Secure token storage and handling

#### Performance Considerations:

**Request Optimization:**
- **Batching**: Group multiple requests for efficiency
- **Parallel Processing**: Concurrent request handling when appropriate
- **Timeout Configuration**: Proper timeout settings for reliability
- **Resource Management**: Efficient memory and connection usage

**Response Handling:**
- **Large Responses**: Handle large API responses efficiently
- **Streaming**: Support for streaming responses when available
- **Memory Management**: Optimize memory usage for large datasets
- **Error Handling**: Graceful handling of network and API errors

#### Security Considerations:

**Authentication Security:**
- **Credential Storage**: Secure credential management
- **Token Rotation**: Regular authentication token updates
- **Access Scoping**: Use minimal required permissions
- **Audit Logging**: Log API access for security monitoring

**Data Protection:**
- **HTTPS Usage**: Always use HTTPS for sensitive data
- **Input Validation**: Validate all request parameters
- **Output Sanitization**: Clean response data before processing
- **Error Handling**: Prevent information leakage through errors

#### Templates Available:
- **Building Your First WhatsApp Chatbot**
- **Scrape and summarize webpages with AI**
- **AI agent that can scrape webpages**
- **Browse HTTP Request integration templates**

#### Related Nodes:
- **GraphQL**: Alternative for GraphQL APIs
- **Webhook**: Receive HTTP requests
- **FTP**: File transfer operations
- **SSH**: Remote server access
- **Code**: Custom HTTP request logic

#### Best Practices:

**1. Authentication:**
- **Use Predefined Credentials**: When available for easier management
- **Secure Token Storage**: Store authentication tokens securely
- **Token Refresh**: Implement token refresh mechanisms
- **Access Control**: Use minimal required permissions

**2. Request Design:**
- **Error Handling**: Implement comprehensive error handling
- **Timeout Configuration**: Set appropriate timeout values
- **Rate Limiting**: Respect API rate limits and quotas
- **Input Validation**: Validate all request parameters

**3. Response Processing:**
- **Format Handling**: Handle different response formats appropriately
- **Status Code Checking**: Check HTTP status codes for success/failure
- **Data Validation**: Validate response data before processing
- **Error Recovery**: Implement retry logic for failed requests

**4. Performance:**
- **Batching**: Use batching for multiple similar requests
- **Caching**: Cache responses when appropriate
- **Connection Reuse**: Optimize HTTP connections
- **Resource Monitoring**: Monitor memory and network usage

#### Troubleshooting:

**Authentication Issues:**
- **Credential Validation**: Verify authentication credentials
- **Permission Checking**: Ensure proper API permissions
- **Token Expiration**: Check for expired authentication tokens
- **Header Configuration**: Verify authentication headers

**Request Problems:**
- **URL Validation**: Verify endpoint URLs and parameters
- **Method Selection**: Ensure correct HTTP method usage
- **Content Type**: Verify content-type headers for body requests
- **Parameter Formatting**: Check query parameter and body formatting

**Response Issues:**
- **Status Code Analysis**: Check HTTP status codes for errors
- **Content Format**: Verify expected response format
- **Timeout Problems**: Adjust timeout settings for slow APIs
- **Size Limitations**: Handle large response sizes appropriately

#### Common Issues Documentation:
- **Dedicated troubleshooting page**: [Common Issues](common-issues/) with detailed solutions
- **Authentication problems**: Credential and permission issues
- **Network connectivity**: Timeout and connection problems
- **Response handling**: Format and parsing issues

#### Integration Examples:

**1. E-commerce Integration:**
- Connect to Shopify, WooCommerce, or custom e-commerce APIs
- Retrieve product information, orders, and customer data
- Update inventory levels and pricing
- Process payments and order fulfillment

**2. CRM Integration:**
- Integrate with Salesforce, HubSpot, or custom CRM systems
- Sync customer data and interaction history
- Create leads and opportunities
- Update contact information and preferences

**3. Social Media Integration:**
- Connect to Twitter, Facebook, LinkedIn APIs
- Retrieve social media posts and metrics
- Post content and engage with audiences
- Monitor brand mentions and sentiment

**4. Cloud Storage Integration:**
- Upload/download files to/from cloud storage services
- Process documents and media files
- Implement backup and archival workflows
- Manage file permissions and sharing

#### Advanced Use Cases:

**1. Multi-Step Authentication:**
- Implement complex OAuth flows
- Handle multiple authentication steps
- Manage refresh tokens and session handling
- Support for custom authentication schemes

**2. Data Pipeline Integration:**
- Extract data from multiple API sources
- Transform and normalize data formats
- Load data into data warehouses or databases
- Implement real-time data streaming

**3. Microservices Orchestration:**
- Coordinate calls across multiple microservices
- Implement saga patterns for distributed transactions
- Handle service discovery and load balancing
- Manage inter-service communication

**4. AI and Machine Learning:**
- Send data to AI/ML APIs for processing
- Retrieve model predictions and results
- Implement A/B testing for model comparisons
- Handle large dataset processing workflows

#### Monitoring and Maintenance:

**Performance Monitoring:**
- **Response Times**: Track API response times
- **Success Rates**: Monitor request success/failure rates
- **Error Analysis**: Analyze error patterns and frequencies
- **Resource Usage**: Monitor memory and network consumption

**Maintenance Tasks:**
- **Credential Updates**: Regular credential rotation and updates
- **API Documentation**: Keep up with API changes and updates
- **Performance Tuning**: Optimize request/response handling
- **Security Reviews**: Regular security audits and updates

---

### 29. If
**Status**: ✅ Active (Note: Switch node recommended for complex conditions)
**Category**: Core - Conditional Logic/Flow Control
**Purpose**: Split a workflow conditionally based on comparison operations. Use to create true/false branching logic in workflows.

#### Authentication:
- **No credentials required**: Direct data comparison operations
- **Expression support**: Full n8n expression capabilities in conditions
- **Data-driven logic**: Conditional routing based on actual data values

#### Operations:
**1. Conditional Branching** - Evaluate data against conditions and route to true/false outputs

#### Core Parameters:

**1. Conditions** - Define comparison criteria for data evaluation:

**Condition Structure:**
- **Data Type Selection**: Choose data type and comparison operation
- **Field References**: Use expressions or direct field references  
- **Value Comparison**: Set comparison values or expressions
- **Multiple Conditions**: Add multiple conditions with logical operators

**Data Type Options:**
- **String**: Text-based comparisons and pattern matching
- **Number**: Numeric comparisons and mathematical operations
- **Date & Time**: Date/time comparisons and temporal logic
- **Boolean**: True/false evaluations
- **Array**: List operations and length comparisons
- **Object**: Object existence and structure validation

#### Combining Conditions:

**Logical Operators:**
- **AND**: Items must meet ALL conditions to go to true output
  - **Use case**: Strict validation requiring multiple criteria
  - **Example**: age > 18 AND status = "active" AND region = "US"
- **OR**: Items meeting ANY condition go to true output
  - **Use case**: Flexible validation with alternative criteria
  - **Example**: priority = "high" OR urgent = true OR escalated = true

**Limitation:**
- **No mixed logic**: Cannot combine AND and OR operators in single If node
- **Complex logic**: Use multiple If nodes or Switch node for complex conditions

#### Available Data Type Comparisons:

**String Comparisons (14 operations):**
- **exists** / **does not exist**: Field presence validation
- **is empty** / **is not empty**: Empty string detection
- **is equal to** / **is not equal to**: Exact text matching
- **contains** / **does not contain**: Substring detection
- **starts with** / **does not start with**: Prefix matching
- **ends with** / **does not end with**: Suffix matching
- **matches regex** / **does not match regex**: Pattern matching with regular expressions

**Number Comparisons (10 operations):**
- **exists** / **does not exist**: Field presence validation
- **is empty** / **is not empty**: Null/undefined detection
- **is equal to** / **is not equal to**: Exact numeric matching
- **is greater than** / **is less than**: Numeric range comparisons
- **is greater than or equal to** / **is less than or equal to**: Inclusive range comparisons

**Date & Time Comparisons (10 operations):**
- **exists** / **does not exist**: Field presence validation
- **is empty** / **is not empty**: Date value validation
- **is equal to** / **is not equal to**: Exact date/time matching
- **is after** / **is before**: Temporal sequence comparisons
- **is after or equal to** / **is before or equal to**: Inclusive temporal comparisons

**Boolean Comparisons (8 operations):**
- **exists** / **does not exist**: Field presence validation
- **is empty** / **is not empty**: Boolean value validation
- **is true** / **is false**: Boolean state evaluation
- **is equal to** / **is not equal to**: Boolean value matching

**Array Comparisons (12 operations):**
- **exists** / **does not exist**: Array presence validation
- **is empty** / **is not empty**: Array content validation
- **contains** / **does not contain**: Element existence checking
- **length equal to** / **length not equal to**: Array size validation
- **length greater than** / **length less than**: Array size comparisons
- **length greater than or equal to** / **length less than or equal to**: Inclusive size comparisons

**Object Comparisons (4 operations):**
- **exists** / **does not exist**: Object presence validation
- **is empty** / **is not empty**: Object content validation

#### Key Features:

**Conditional Logic:**
- **Binary Output**: True/false routing for workflow branching
- **Data-driven decisions**: Route based on actual data values and patterns
- **Multiple condition support**: Combine multiple criteria for complex validation
- **Type-aware comparisons**: Appropriate operations for each data type
- **Expression integration**: Use n8n expressions for dynamic condition values

**Workflow Control:**
- **True Output**: Items meeting conditions proceed through true path
- **False Output**: Items not meeting conditions proceed through false path
- **Early routing**: Make decisions early in workflow to optimize processing
- **Conditional processing**: Process different item types through different paths

**Simplicity and Performance:**
- **Straightforward logic**: Simple true/false branching
- **Efficient evaluation**: Fast condition evaluation and routing
- **Clear workflow**: Visual representation of conditional logic
- **Easy debugging**: Clear true/false paths for troubleshooting

#### Use Cases:

**1. Data Validation and Quality Control:**
- **Input validation**: Ensure data meets required criteria before processing
- **Format verification**: Check data formats and structures
- **Range validation**: Verify numeric values within acceptable ranges
- **Required field checking**: Ensure all necessary fields are present

**2. Business Logic Implementation:**
- **Status-based routing**: Route items based on status conditions
- **User type handling**: Different processing for different user types
- **Priority routing**: Handle high-priority items differently
- **Category-based processing**: Route items by category or classification

**3. Error Handling and Workflow Control:**
- **Error detection**: Route problematic data to error handling paths
- **Success/failure routing**: Different paths for successful vs failed operations
- **Retry logic**: Conditional retry based on error types
- **Fallback processing**: Alternative processing paths for edge cases

**4. Data Processing Optimization:**
- **Early filtering**: Route only relevant items to expensive operations
- **Conditional API calls**: Make API calls only when necessary
- **Resource optimization**: Avoid unnecessary processing for certain items
- **Performance tuning**: Route different data types to optimized processing paths

#### Common Conditional Patterns:

**1. Data Validation:**
```
Conditions:
- email (String) matches regex: ^[^\s@]+@[^\s@]+\.[^\s@]+$
- age (Number) is greater than or equal to: 18
- status (String) is equal to: "active"
Operator: AND
```

**2. Priority Routing:**
```
Conditions:
- priority (String) is equal to: "high"
- urgent (Boolean) is true
- escalated (Boolean) is true
Operator: OR
```

**3. Date-based Processing:**
```
Conditions:
- created_date (Date & Time) is after: {{ $now.minus({days: 30}) }}
- status (String) is not equal to: "archived"
Operator: AND
```

**4. Content Filtering:**
```
Conditions:
- category (String) is equal to: "approved"
- content_length (Number) is greater than: 100
- has_images (Boolean) is true
Operator: AND
```

#### Integration Examples:

**1. Customer Data Processing:**
```
Customer Data → If (validate customer) → True: Process Order / False: Request Information
```
- Validate customer information completeness
- Route complete profiles to order processing
- Route incomplete profiles to data collection

**2. Content Management:**
```
Content Input → If (content approved) → True: Publish / False: Review Queue
```
- Check content approval status
- Route approved content to publishing
- Route unapproved content to review process

**3. E-commerce Order Processing:**
```
Order Data → If (payment valid) → True: Fulfillment / False: Payment Retry
```
- Validate payment information
- Route valid payments to fulfillment
- Route invalid payments to retry process

**4. User Onboarding:**
```
User Registration → If (email verified) → True: Welcome Flow / False: Verification Reminder
```
- Check email verification status
- Route verified users to welcome workflow
- Route unverified users to verification reminder

#### Advanced Features:

**Expression-Based Conditions:**
- **Dynamic values**: Use expressions for calculated condition values
- **Context-aware logic**: Conditions based on workflow state or environment
- **Date calculations**: Use relative dates and time periods in conditions
- **Complex field references**: Access nested object properties and array elements

**Multiple Condition Strategies:**
- **Sequential If nodes**: Chain multiple If nodes for complex logic trees
- **Nested conditions**: Use If nodes within true/false branches
- **Parallel evaluation**: Multiple If nodes evaluating different aspects
- **Switch alternative**: Use Switch node for more than two conditional outputs

#### Performance Considerations:

**Optimization Strategies:**
- **Condition order**: Place most selective conditions first in AND logic
- **Early evaluation**: Use If nodes early in workflow to avoid unnecessary processing
- **Simple conditions**: Keep conditions simple for faster evaluation
- **Expression efficiency**: Use efficient expressions for dynamic values

**Resource Management:**
- **True/false routing**: Only one path executes, saving resources
- **Conditional API calls**: Avoid unnecessary external requests
- **Processing optimization**: Route expensive operations only when needed
- **Memory efficiency**: Failed conditions don't consume downstream resources

#### Legacy Workflow Considerations:

**Version 0.236.0 and Below (v0 legacy execution):**
- **If + Merge interaction**: Both true and false outputs may execute with Merge nodes
- **Execution behavior**: Merge node can trigger execution of unused branch
- **Workflow settings**: Change execution order to v1 to avoid this behavior
- **Modern alternative**: Use Switch node for complex conditional routing

#### Templates Available:
- **AI agent that can scrape webpages**
- **Pulling data from services that n8n doesn't have a pre-built integration for**
- **✨🤖Automate Multi-Platform Social Media Content Creation with AI**
- **Browse If integration templates**

#### Related Resources:
- **Switch Node**: Use for more than two conditional outputs or complex routing
- **Splitting with conditionals**: [Flow Logic Documentation](../../../../flow-logic/splitting/)
- **Filter Node**: Alternative for filtering items rather than routing
- **Merge Node**: Combine results from true/false branches

#### Related Nodes:
- **Switch**: Advanced conditional routing with multiple outputs
- **Filter**: Remove items that don't meet conditions (single output)
- **Merge**: Combine data from multiple branches
- **Code**: Custom conditional logic with JavaScript/Python
- **Compare Datasets**: Compare data between different sources

#### Best Practices:

**1. Condition Design:**
- **Clear criteria**: Use specific, unambiguous condition criteria
- **Type consistency**: Ensure data types match comparison operations
- **Expression efficiency**: Use efficient expressions for dynamic values
- **Logical organization**: Group related conditions logically

**2. Workflow Organization:**
- **Early decisions**: Place If nodes early to optimize workflow performance
- **Clear paths**: Make true/false paths visually clear in workflow design
- **Documentation**: Comment complex conditional logic
- **Testing**: Test both true and false paths thoroughly

**3. Alternative Considerations:**
- **Switch vs If**: Use Switch for more than two outputs
- **Filter vs If**: Use Filter when you only need items that meet conditions
- **Multiple If vs Switch**: Consider Switch for complex multi-condition routing
- **Performance**: Evaluate performance impact of conditional complexity

**4. Error Prevention:**
- **Field validation**: Check field existence before comparison
- **Type checking**: Validate data types before conditional evaluation
- **Default handling**: Plan for unexpected data values
- **Expression testing**: Test expressions with representative data

#### Troubleshooting:

**Conditions Not Working:**
- **Check field names**: Verify field references match actual data structure
- **Validate data types**: Ensure data types match comparison operations
- **Test expressions**: Validate n8n expressions in conditions
- **Review logic**: Confirm AND/OR logic meets requirements

**Unexpected Routing:**
- **Data inspection**: Examine actual data values being compared
- **Condition testing**: Test individual conditions separately
- **Expression debugging**: Verify expression evaluation results
- **Type conversion**: Check for automatic type conversion issues

**Performance Issues:**
- **Condition complexity**: Simplify complex conditional logic
- **Expression optimization**: Optimize condition expressions
- **Early placement**: Move If nodes earlier in workflow
- **Alternative approaches**: Consider Filter or Switch alternatives

#### Common Issues and Solutions:

**1. Mixed AND/OR Logic:**
- **Problem**: Need both AND and OR logic in single condition
- **Solution**: Use multiple If nodes or Switch node for complex logic

**2. More Than Two Outputs:**
- **Problem**: Need to route to more than true/false paths
- **Solution**: Use Switch node for multiple conditional outputs

**3. Legacy Execution Issues:**
- **Problem**: Both branches executing with Merge nodes
- **Solution**: Update workflow execution order to v1 in settings

**4. Performance with Complex Conditions:**
- **Problem**: Slow evaluation with many conditions
- **Solution**: Optimize expressions and consider alternative approaches

#### Advanced Use Cases:

**1. Multi-Stage Validation:**
- **Primary validation**: Basic data format checks
- **Business rules**: Complex business logic validation
- **Final verification**: Last-minute validation before processing
- **Error categorization**: Different error handling for different validation failures

**2. Dynamic Workflow Routing:**
- **Environment-based**: Different processing for dev/test/prod
- **User role-based**: Different workflows for different user types
- **Time-based**: Different processing based on time of day/week
- **Load-based**: Route to different processing based on system load

**3. Quality Control Workflows:**
- **Content quality**: Route content based on quality scores
- **Data completeness**: Different processing for complete vs incomplete data
- **Compliance checking**: Route based on regulatory compliance status
- **Performance thresholds**: Different handling based on performance metrics

**4. A/B Testing Implementation:**
- **User segmentation**: Route users to different experience paths
- **Feature flags**: Enable/disable features based on conditions
- **Experiment routing**: Direct traffic to different experiment variants
- **Results tracking**: Route results to different analytics paths

#### Monitoring and Analytics:

**Conditional Performance:**
- **True/false ratios**: Monitor which path is taken more frequently
- **Condition effectiveness**: Track how often conditions are met
- **Performance impact**: Monitor execution time for conditional logic
- **Error rates**: Track condition evaluation errors

**Business Intelligence:**
- **Decision patterns**: Analyze patterns in conditional routing
- **Data quality trends**: Monitor validation failure rates
- **Process optimization**: Identify opportunities for workflow optimization
- **User behavior**: Understand user patterns through conditional routing

#### Migration and Modernization:

**From If to Switch:**
- **Multiple outputs**: When you need more than true/false routing
- **Complex conditions**: When AND/OR combinations become unwieldy
- **Better organization**: Switch provides clearer multi-path visualization
- **Enhanced features**: Switch offers more advanced conditional capabilities

**Workflow Modernization:**
- **Legacy execution**: Update from v0 to v1 execution order
- **Performance optimization**: Replace complex If chains with Switch
- **Maintainability**: Simplify conditional logic for easier maintenance
- **Feature utilization**: Leverage newer conditional node features

---

### 30. JWT
**Status**: ✅ Active
**Category**: Core - Security/Authentication/Token Management
**Purpose**: Work with JSON Web Tokens (JWT) in n8n workflows - decode, sign, and verify tokens for secure authentication and data exchange

#### AI Tool Capability:
- **AI Enhancement**: Can be used to enhance AI agent capabilities
- **Automated Configuration**: Many parameters can be set automatically or with AI direction
- **AI Integration**: Part of AI tool parameters documentation for agent workflows

#### Authentication:
- **JWT Credential**: Required for signing and verifying tokens
- **Credential Setup**: [JWT Credential Documentation](../../credentials/jwt/)
- **Algorithm Support**: Multiple JWT signing algorithms supported
- **Key Management**: Secure storage and handling of signing keys

#### Operations (3 total):

**1. Decode** - Decode JWT tokens without verification
- **Purpose**: Extract payload and header information from JWT tokens
- **Use Case**: Inspect token contents, extract claims, debug token issues
- **Security Note**: Does not verify token signature or validity

**2. Sign** - Create and sign new JWT tokens
- **Purpose**: Generate JWT tokens with custom claims and signatures
- **Use Case**: User authentication, API authorization, secure data exchange
- **Customization**: Full control over token claims and expiration

**3. Verify** - Verify JWT token signature and validity
- **Purpose**: Validate JWT tokens against signing keys and claims
- **Use Case**: Authentication validation, API security, token verification
- **Security Features**: Signature validation, expiration checking, claim verification

#### Core Parameters:

**1. Credential to connect with**
- **Required**: JWT credential containing signing keys and algorithm configuration
- **Purpose**: Provides cryptographic keys and algorithm settings for token operations
- **Setup**: Configure through n8n credential system with appropriate keys

**2. Token** (Decode and Verify operations)
- **Required**: JWT token string to decode or verify
- **Format**: Standard JWT format (header.payload.signature)
- **Expression Support**: Use n8n expressions for dynamic token input
- **Sources**: HTTP headers, API responses, user input, stored values

**3. Use JSON to Build Payload** (Sign operation only)
- **Purpose**: Choose method for constructing JWT payload claims
- **Options**:
  - **On**: Use JSON editor for flexible payload construction
  - **Off**: Use individual claim fields for structured input
- **Flexibility**: JSON mode allows complex nested claims and custom structures

#### Payload Claims (Sign Operation):

The available payload claims depend on the "Use JSON to Build Payload" setting:

**JSON Mode (Use JSON to Build Payload = On):**
- **JSON Editor**: Construct complete payload using JSON syntax
- **Custom Claims**: Add any custom claims beyond standard JWT claims
- **Nested Objects**: Support for complex data structures in claims
- **Expression Integration**: Use n8n expressions within JSON payload

**Individual Claims Mode (Use JSON to Build Payload = Off):**
- **Add Claim**: Add individual standard and custom claims
- **Structured Input**: Guided interface for common JWT claims
- **Validation**: Built-in validation for standard claim formats

#### Standard JWT Claims:

**1. Audience (aud)**
- **Purpose**: Identifies intended recipients of the JWT
- **Type**: String or array of strings
- **Use Case**: API endpoint identification, service targeting
- **RFC Reference**: [RFC 7519 Section 4.1.3](https://datatracker.ietf.org/doc/html/rfc7519#section-4.1.3)

**2. Expires In (exp)**
- **Purpose**: Identifies expiration time after which JWT expires
- **Type**: NumericDate (seconds since Unix epoch)
- **Use Case**: Token lifetime management, automatic invalidation
- **RFC Reference**: [RFC 7519 Section 4.1.4](https://datatracker.ietf.org/doc/html/rfc7519#section-4.1.4)

**3. Issuer (iss)**
- **Purpose**: Identifies principal that issued the JWT
- **Type**: String containing URI or arbitrary string
- **Use Case**: Token source identification, trust establishment
- **RFC Reference**: [RFC 7519 Section 4.1.1](https://datatracker.ietf.org/doc/html/rfc7519#section-4.1.1)

**4. JWT ID (jti)**
- **Purpose**: Provides unique identifier for the JWT
- **Type**: Case-sensitive string
- **Use Case**: Prevent replay attacks, token tracking, revocation lists
- **RFC Reference**: [RFC 7519 Section 4.1.7](https://datatracker.ietf.org/doc/html/rfc7519#section-4.1.7)

**5. Not Before (nbf)**
- **Purpose**: Identifies time before which JWT must not be accepted
- **Type**: NumericDate (seconds since Unix epoch)
- **Use Case**: Delayed token activation, scheduled authentication
- **RFC Reference**: [RFC 7519 Section 4.1.5](https://datatracker.ietf.org/doc/html/rfc7519#section-4.1.5)

**6. Subject (sub)**
- **Purpose**: Identifies principal that is the subject of the JWT
- **Type**: String containing URI or arbitrary string
- **Use Case**: User identification, resource ownership, access control
- **RFC Reference**: [RFC 7519 Section 4.1.2](https://datatracker.ietf.org/doc/html/rfc7519#section-4.1.2)

#### Node Options:

**Decode Operation Options:**

**Return Additional Info**
- **Purpose**: Control amount of information returned in decode operation
- **On**: Returns complete decoded token with header, payload, and signature information
- **Off**: Returns only the payload claims
- **Use Case**: Debugging vs production use, information security

**Sign Operation Options:**

**Override Algorithm**
- **Purpose**: Override algorithm specified in JWT credentials
- **Algorithms**: HS256, HS384, HS512, RS256, RS384, RS512, ES256, ES384, ES512
- **Use Case**: Dynamic algorithm selection, multi-algorithm support
- **Security**: Ensure algorithm matches intended security requirements

**Verify Operation Options:**

**Return Additional Info**
- **Purpose**: Control amount of information returned in verify operation
- **On**: Returns complete decoded token with header, payload, and signature information
- **Off**: Returns only the payload claims
- **Use Case**: Security vs functionality balance

**Ignore Expiration**
- **Purpose**: Skip expiration time claim (exp) validation
- **Use Case**: Testing, legacy token handling, grace period implementations
- **Security Warning**: Use cautiously as it bypasses important security check

**Ignore Not Before Claim**
- **Purpose**: Skip not before claim (nbf) validation
- **Use Case**: Clock synchronization issues, testing scenarios
- **Security Warning**: May allow premature token usage

**Clock Tolerance**
- **Purpose**: Number of seconds to tolerate for nbf and exp claims
- **Default**: 0 seconds (strict validation)
- **Use Case**: Handle small clock differences between servers
- **Range**: Typically 0-300 seconds for production use

**Override Algorithm**
- **Purpose**: Override algorithm specified in JWT credentials for verification
- **Use Case**: Multi-algorithm support, algorithm migration
- **Security**: Must match algorithm used for signing

#### Key Features:

**Comprehensive JWT Support:**
- **Full RFC 7519 compliance**: Complete implementation of JWT standard
- **Multiple algorithms**: Support for HMAC, RSA, and ECDSA algorithms
- **Standard claims**: Built-in support for all standard JWT claims
- **Custom claims**: Ability to add custom claims for specific use cases

**Security Features:**
- **Signature verification**: Cryptographic signature validation
- **Expiration handling**: Automatic expiration time validation
- **Clock tolerance**: Handle server time synchronization issues
- **Algorithm verification**: Ensure tokens use expected signing algorithms

**Workflow Integration:**
- **Expression support**: Dynamic token generation and validation
- **Error handling**: Proper error handling for invalid tokens
- **Performance optimization**: Efficient token processing
- **Debugging support**: Detailed token information for troubleshooting

#### Use Cases:

**1. User Authentication Systems:**
- **Login workflows**: Generate JWT tokens after successful authentication
- **Session management**: Validate user sessions with JWT verification
- **Single sign-on (SSO)**: Implement SSO across multiple services
- **Mobile app authentication**: Secure mobile application access

**2. API Security:**
- **API authorization**: Validate API requests using JWT tokens
- **Microservices security**: Secure inter-service communication
- **Rate limiting**: Use JWT claims for user-based rate limiting
- **Access control**: Implement role-based access control (RBAC)

**3. Secure Data Exchange:**
- **Data integrity**: Ensure data hasn't been tampered with
- **Trusted communication**: Establish trust between services
- **Claim-based authorization**: Use JWT claims for fine-grained permissions
- **Temporary access**: Generate time-limited access tokens

**4. Integration Workflows:**
- **Third-party APIs**: Generate tokens for external API authentication
- **Webhook security**: Validate incoming webhook requests
- **Data pipeline security**: Secure data processing workflows
- **Legacy system integration**: Bridge authentication between systems

#### Common Integration Patterns:

**1. Authentication Flow:**
```
User Login → Database Validation → JWT (Sign) → Return Token
```
- Validate user credentials
- Generate JWT with user claims
- Return signed token for client use

**2. API Request Validation:**
```
HTTP Request → Extract Token → JWT (Verify) → Process Request
```
- Extract JWT from request headers
- Verify token signature and claims
- Proceed with authorized request processing

**3. Token Refresh:**
```
Refresh Token → JWT (Verify) → JWT (Sign) → Return New Token
```
- Validate existing refresh token
- Generate new access token
- Implement token rotation for security

**4. Microservice Communication:**
```
Service A → JWT (Sign) → HTTP Request → Service B → JWT (Verify)
```
- Service A generates token for request
- Service B validates token before processing
- Secure inter-service communication

#### Advanced Features:

**Custom Claims:**
- **User roles**: Include user role information in tokens
- **Permissions**: Embed specific permissions in token claims
- **Session data**: Store session-related information
- **Application metadata**: Include application-specific data

**Algorithm Support:**
- **HMAC algorithms**: HS256, HS384, HS512 for shared secret scenarios
- **RSA algorithms**: RS256, RS384, RS512 for public/private key scenarios
- **ECDSA algorithms**: ES256, ES384, ES512 for elliptic curve cryptography
- **Algorithm flexibility**: Support for multiple algorithms in single workflow

**Security Best Practices:**
- **Key rotation**: Support for key rotation workflows
- **Token revocation**: Implement token blacklisting and revocation
- **Claim validation**: Validate all claims against expected values
- **Secure storage**: Proper handling of signing keys and sensitive data

#### JWT Token Structure:

**Header Example:**
```json
{
  "alg": "HS256",
  "typ": "JWT"
}
```

**Payload Example:**
```json
{
  "sub": "1234567890",
  "name": "John Doe",
  "iat": 1516239022,
  "exp": 1516325422,
  "aud": "api.example.com",
  "iss": "auth.example.com"
}
```

**Complete Token Format:**
```
eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c
```

#### Security Considerations:

**Key Management:**
- **Secure storage**: Store signing keys securely in credentials
- **Key rotation**: Implement regular key rotation procedures
- **Access control**: Limit access to signing keys
- **Algorithm selection**: Choose appropriate algorithms for security requirements

**Token Validation:**
- **Signature verification**: Always verify token signatures
- **Expiration checking**: Enforce token expiration times
- **Audience validation**: Validate intended token recipients
- **Issuer verification**: Verify token source authenticity

**Attack Prevention:**
- **Algorithm confusion**: Ensure algorithm consistency
- **Token replay**: Use unique JWT IDs (jti) when necessary
- **Clock attacks**: Implement appropriate clock tolerance
- **Information disclosure**: Avoid sensitive data in payload claims

#### Performance Considerations:

**Token Processing:**
- **Algorithm efficiency**: Choose appropriate algorithms for performance requirements
- **Token size**: Balance claims vs token size for network efficiency
- **Caching**: Cache validation results when appropriate
- **Batch processing**: Process multiple tokens efficiently

**Network Optimization:**
- **Token compression**: Consider token size impact on network traffic
- **Header optimization**: Minimize token header size
- **Claim optimization**: Include only necessary claims
- **Transport security**: Use HTTPS for token transmission

#### Integration Examples:

**1. Web Application Authentication:**
- Generate JWT tokens for user sessions
- Validate tokens on protected route access
- Implement token refresh mechanisms
- Handle token expiration gracefully

**2. API Gateway Integration:**
- Validate incoming API requests
- Extract user information from token claims
- Implement rate limiting based on token claims
- Route requests based on user roles

**3. Microservices Security:**
- Generate service-to-service authentication tokens
- Validate inter-service communication
- Implement distributed authorization
- Handle service identity verification

**4. Mobile Application Backend:**
- Authenticate mobile app users
- Generate session tokens for app usage
- Validate API requests from mobile clients
- Implement offline token validation

#### Templates Available:
- **Browse JWT integration templates**
- **Search all templates** for JWT authentication patterns
- **API security workflows** using JWT validation
- **Authentication system implementations**

#### Related Nodes:
- **Crypto**: Cryptographic operations and key management
- **HTTP Request**: API calls with JWT authentication
- **Webhook**: Validate incoming webhook requests with JWT
- **Code**: Custom JWT processing and validation logic
- **TOTP**: Time-based authentication tokens

#### Best Practices:

**1. Security:**
- **Always verify signatures**: Never trust unverified tokens
- **Use strong algorithms**: Choose appropriate cryptographic algorithms
- **Implement expiration**: Always set reasonable token expiration times
- **Validate claims**: Verify all critical claims (aud, iss, exp, nbf)

**2. Performance:**
- **Optimize token size**: Include only necessary claims
- **Cache validation**: Cache validation results when appropriate
- **Choose efficient algorithms**: Balance security and performance
- **Monitor token usage**: Track token generation and validation patterns

**3. Workflow Design:**
- **Error handling**: Implement proper error handling for invalid tokens
- **Token refresh**: Design token refresh mechanisms
- **Claim extraction**: Efficiently extract and use token claims
- **Integration patterns**: Use consistent JWT patterns across workflows

**4. Debugging:**
- **Use decode operation**: Debug token contents without verification
- **Enable additional info**: Get complete token information for troubleshooting
- **Log validation errors**: Implement proper error logging
- **Test token scenarios**: Test various token validity scenarios

#### Troubleshooting:

**Common Issues:**
- **Invalid signature**: Check signing key and algorithm consistency
- **Token expired**: Verify token expiration and generation time
- **Algorithm mismatch**: Ensure signing and verification algorithms match
- **Clock skew**: Adjust clock tolerance for time-sensitive claims

**Resolution Strategies:**
- **Validate credentials**: Ensure JWT credentials are properly configured
- **Check token format**: Verify JWT token structure and encoding
- **Test algorithms**: Confirm algorithm compatibility between sign/verify
- **Debug claims**: Use decode operation to inspect token contents

#### Error Handling:

**Token Validation Errors:**
- **Invalid format**: Handle malformed JWT tokens gracefully
- **Signature verification failures**: Implement appropriate error responses
- **Expiration errors**: Handle expired tokens with refresh mechanisms
- **Claim validation failures**: Provide specific error messages

**Workflow Integration:**
- **Error workflows**: Implement error handling workflows for token failures
- **Fallback mechanisms**: Provide alternative authentication methods
- **User communication**: Clear error messages for authentication failures
- **Logging**: Comprehensive logging for security monitoring

#### Advanced Use Cases:

**1. Multi-Tenant Applications:**
- **Tenant isolation**: Use JWT claims for tenant identification
- **Resource access**: Control access to tenant-specific resources
- **Cross-tenant operations**: Secure operations across tenant boundaries
- **Tenant-specific configurations**: Embed tenant settings in tokens

**2. Federated Authentication:**
- **Identity providers**: Integrate with external identity providers
- **Trust relationships**: Establish trust between authentication systems
- **Claim mapping**: Map external claims to internal authorization
- **Protocol bridging**: Bridge different authentication protocols

**3. IoT Device Authentication:**
- **Device identity**: Authenticate IoT devices using JWT
- **Certificate-based**: Use certificate-based JWT signing
- **Lightweight tokens**: Optimize tokens for resource-constrained devices
- **Device lifecycle**: Handle device registration and decommissioning

**4. Distributed Systems:**
- **Service mesh security**: Secure service-to-service communication
- **Load balancer integration**: JWT validation at load balancer level
- **Edge computing**: Distribute JWT validation to edge nodes
- **Hybrid cloud**: Secure communication across cloud boundaries

#### Monitoring and Analytics:

**Token Usage Metrics:**
- **Generation rates**: Monitor token creation patterns
- **Validation success/failure**: Track authentication success rates
- **Expiration patterns**: Analyze token lifetime usage
- **Algorithm usage**: Monitor cryptographic algorithm usage

**Security Monitoring:**
- **Failed validations**: Track and alert on validation failures
- **Suspicious patterns**: Detect unusual token usage patterns
- **Key rotation**: Monitor key rotation schedules and compliance
- **Attack detection**: Identify potential JWT-based attacks

#### Compliance and Standards:

**RFC Compliance:**
- **RFC 7519**: Full JSON Web Token standard compliance
- **RFC 7515**: JSON Web Signature (JWS) compliance
- **RFC 7517**: JSON Web Key (JWK) support where applicable
- **RFC 7518**: JSON Web Algorithms (JWA) implementation

**Security Standards:**
- **OWASP guidelines**: Follow OWASP JWT security recommendations
- **Industry best practices**: Implement security industry standards
- **Compliance frameworks**: Support for regulatory compliance requirements
- **Audit trails**: Maintain audit logs for compliance reporting

### 31. LDAP
**Status**: ✅ Active
**Category**: Core - Directory Services/Authentication/Enterprise Integration
**Purpose**: Interact with LDAP (Lightweight Directory Access Protocol) servers to create, find, and update directory objects for enterprise authentication and data management

#### AI Tool Capability:
- **AI Enhancement**: Can be used to enhance AI agent capabilities
- **Automated Configuration**: Many parameters can be set automatically or with AI direction
- **AI Integration**: Part of AI tool parameters documentation for agent workflows

#### Authentication:
- **LDAP Credential**: Required for connecting to LDAP servers
- **Credential Setup**: [LDAP Credential Documentation](../../credentials/ldap/)
- **Server Support**: Active Directory, OpenLDAP, and other LDAP-compliant directory services
- **Security**: Support for secure LDAP connections (LDAPS) and authentication methods

#### Operations (6 total):

**1. Compare** - Compare an attribute value in LDAP entry
- **Purpose**: Verify if an attribute has a specific value without retrieving the entire entry
- **Use Case**: Password verification, attribute validation, security checks
- **Efficiency**: More efficient than search for simple existence checks

**2. Create** - Create a new LDAP entry
- **Purpose**: Add new directory objects like users, groups, or organizational units
- **Use Case**: User provisioning, group creation, organizational structure management
- **Flexibility**: Support for all LDAP object classes and attributes

**3. Delete** - Delete an LDAP entry
- **Purpose**: Remove directory objects from LDAP server
- **Use Case**: User deprovisioning, cleanup operations, organizational restructuring
- **Security**: Requires appropriate permissions and proper DN specification

**4. Rename** - Rename the Distinguished Name (DN) of an existing entry
- **Purpose**: Move or rename LDAP entries within directory structure
- **Use Case**: Organizational restructuring, user account management, hierarchy changes
- **Flexibility**: Support for moving entries between organizational units

**5. Search** - Search LDAP directory for entries
- **Purpose**: Query and retrieve directory information based on search criteria
- **Use Case**: User lookup, group membership, directory browsing, reporting
- **Advanced Features**: Flexible search scopes, attribute filtering, pagination

**6. Update** - Update attributes of existing LDAP entries
- **Purpose**: Modify attribute values in existing directory objects
- **Use Case**: Profile updates, group membership changes, attribute management
- **Operations**: Add, remove, or replace attribute values

#### Core Parameters:

**1. Credential to connect with**
- **Required**: LDAP credential for server authentication
- **Purpose**: Authenticate and connect to LDAP directory server
- **Setup**: Configure server details, authentication method, and security settings

#### Operation-Specific Parameters:

**Compare Operation:**
- **DN**: Distinguished Name of the entry to compare against
- **Attribute ID**: ID of the attribute to compare (e.g., userPassword, mail, cn)
- **Value**: Value to compare against the attribute
- **Result**: Boolean result indicating match or no match

**Create Operation:**
- **DN**: Distinguished Name for the new entry to create
- **Attributes**: Collection of Attribute ID/Value pairs
  - **Attribute ID**: LDAP attribute name (e.g., cn, sn, mail, userPassword)
  - **Value**: Attribute value to set
  - **Multiple Attributes**: Add multiple attributes for complete object creation

**Delete Operation:**
- **DN**: Distinguished Name of the entry to delete
- **Validation**: Ensure proper permissions and DN accuracy
- **Cascading**: Consider impact on child entries (if any)

**Rename Operation:**
- **DN**: Current Distinguished Name of the entry to rename
- **New DN**: New Distinguished Name for the entry
- **Move Support**: Can move entries between organizational units
- **Validation**: Ensure target location exists and permissions are appropriate

**Search Operation:**
- **Base DN**: Distinguished Name of the subtree to search within
- **Search For**: Directory object class to search for (user, group, organizationalUnit, etc.)
- **Attribute**: Attribute to search against
- **Search Text**: Text to search for (supports wildcard "*" character)
- **Return All**: Toggle to return all results or limit results
- **Limit**: Maximum number of results to return (when Return All is off)

**Update Operation:**
- **DN**: Distinguished Name of the entry to update
- **Update Attributes**: Choose operation type:
  - **Add**: Add new attribute values
  - **Remove**: Remove existing attribute values
  - **Replace**: Replace existing attribute values
- **Attribute ID**: LDAP attribute identifier
- **Value**: New value for the attribute

#### Search Options (Advanced Configuration):

**Attribute Names or IDs:**
- **Purpose**: Specify which attributes to return in search results
- **Format**: Comma-separated list of attribute names
- **Expression Support**: Use expressions for dynamic attribute selection
- **Performance**: Limit returned attributes for better performance

**Page Size:**
- **Purpose**: Control pagination for large result sets
- **Configuration**: Maximum number of results per page
- **Value**: Set to 0 to disable paging
- **Performance**: Optimize memory usage and network traffic

**Scopes - Search Depth Control:**

**Base Tree (Subordinate Subtree):**
- **Scope**: Search subordinates of Base DN but not the Base DN itself
- **Use Case**: Find children without including parent entry
- **Depth**: All levels below Base DN

**Single Level (One Level):**
- **Scope**: Search only immediate children of Base DN
- **Use Case**: Browse directory structure level by level
- **Depth**: One level below Base DN only

**Whole Subtree (Sub):**
- **Scope**: Search Base DN entry and all subordinates to any depth
- **Use Case**: Comprehensive searches across entire directory branch
- **Depth**: Base DN and all levels below

#### Key Features:

**Enterprise Directory Integration:**
- **Active Directory**: Full support for Microsoft Active Directory
- **OpenLDAP**: Compatible with OpenLDAP and other open-source directories
- **Schema Awareness**: Work with any LDAP schema and object classes
- **Standards Compliance**: Full LDAP v3 protocol compliance

**Security and Authentication:**
- **Secure Connections**: Support for LDAPS (LDAP over SSL/TLS)
- **Authentication Methods**: Simple bind, SASL, and other authentication mechanisms
- **Access Control**: Respect LDAP server access control lists (ACLs)
- **Credential Management**: Secure storage and handling of LDAP credentials

**Performance Optimization:**
- **Connection Pooling**: Efficient connection management
- **Pagination**: Handle large result sets efficiently
- **Attribute Filtering**: Return only required attributes
- **Search Optimization**: Flexible search scopes for optimal performance

**Workflow Integration:**
- **Expression Support**: Dynamic DN and attribute construction
- **Error Handling**: Comprehensive error handling for LDAP operations
- **Data Transformation**: Integration with other n8n nodes for data processing
- **Batch Operations**: Process multiple LDAP operations in workflows

#### Use Cases:

**1. User Management and Provisioning:**
- **Account Creation**: Automate user account creation in Active Directory
- **Profile Updates**: Sync user profile information from HR systems
- **Password Management**: Implement password change workflows
- **Account Deprovisioning**: Automate user account cleanup and removal

**2. Group and Access Management:**
- **Group Membership**: Manage user group memberships dynamically
- **Role-Based Access**: Implement role-based access control workflows
- **Organizational Units**: Manage organizational structure and hierarchy
- **Access Reviews**: Automated access review and compliance reporting

**3. Directory Synchronization:**
- **Multi-System Sync**: Synchronize user data between multiple systems
- **HR Integration**: Import employee data from HR systems to directory
- **Application Integration**: Sync directory data with business applications
- **Data Validation**: Validate and clean directory information

**4. Authentication and Security:**
- **Single Sign-On**: Support SSO implementations with directory lookups
- **Authentication Workflows**: Implement custom authentication logic
- **Security Auditing**: Monitor and audit directory access and changes
- **Compliance Reporting**: Generate compliance reports from directory data

**5. Enterprise Automation:**
- **Onboarding**: Automate employee onboarding with directory setup
- **Offboarding**: Streamline employee departure processes
- **Organizational Changes**: Handle reorganizations and role changes
- **Self-Service**: Enable user self-service directory operations

#### Common Integration Patterns:

**1. User Onboarding Workflow:**
```
HR System → Edit Fields (Set) → LDAP (Create) → Email Notification → Group Assignment
```
- Import employee data from HR system
- Create user account in Active Directory
- Send welcome email with account details
- Assign to appropriate groups and organizational units

**2. Directory Synchronization:**
```
Schedule Trigger → LDAP (Search) → Compare Datasets → LDAP (Update) → Audit Log
```
- Periodically search directory for changes
- Compare with authoritative data source
- Update directory with changes
- Log all modifications for audit purposes

**3. Access Review Automation:**
```
Schedule Trigger → LDAP (Search Groups) → Filter → Email Report → Manual Review
```
- Regularly extract group membership information
- Filter for sensitive or high-privilege groups
- Generate access review reports
- Send to managers for review and approval

**4. Self-Service Password Reset:**
```
Webhook → LDAP (Compare) → LDAP (Update) → Send Email → Audit Log
```
- Receive password reset request
- Validate user identity with security questions
- Update password in directory
- Send confirmation and log the change

#### Advanced Features:

**Distinguished Name (DN) Construction:**
- **Dynamic DN Building**: Use expressions to construct DNs dynamically
- **Organizational Structure**: Navigate complex organizational hierarchies
- **Multi-Domain Support**: Work with multiple domains and forests
- **Canonical Name Conversion**: Convert between DN formats as needed

**Attribute Management:**
- **Multi-Value Attributes**: Handle attributes with multiple values
- **Binary Attributes**: Support for binary data like photos and certificates
- **Operational Attributes**: Access operational attributes like timestamps
- **Custom Attributes**: Work with custom schema extensions

**Search Capabilities:**
- **Complex Filters**: Build complex LDAP search filters
- **Wildcard Searches**: Use wildcards for flexible search patterns
- **Range Searches**: Perform range queries on numeric and date attributes
- **Nested Searches**: Combine multiple search criteria

**Error Handling and Resilience:**
- **Connection Retry**: Automatic retry for transient connection issues
- **Graceful Degradation**: Handle server unavailability gracefully
- **Detailed Error Messages**: Comprehensive error information for troubleshooting
- **Transaction Support**: Handle multi-step operations safely

#### Security Considerations:

**Access Control:**
- **Least Privilege**: Use accounts with minimal required permissions
- **Secure Binding**: Use secure authentication methods
- **Connection Security**: Always use encrypted connections for sensitive operations
- **Audit Logging**: Log all directory operations for security monitoring

**Data Protection:**
- **Sensitive Attributes**: Handle sensitive data like passwords securely
- **Personal Information**: Comply with privacy regulations for personal data
- **Data Validation**: Validate all input data before directory operations
- **Encryption**: Use encryption for data in transit and at rest

**Credential Management:**
- **Service Accounts**: Use dedicated service accounts for automation
- **Credential Rotation**: Regular rotation of LDAP service account passwords
- **Secure Storage**: Store credentials securely in n8n credential system
- **Access Monitoring**: Monitor and audit credential usage

#### Performance Considerations:

**Query Optimization:**
- **Efficient Filters**: Use indexed attributes in search filters
- **Pagination**: Use pagination for large result sets
- **Attribute Selection**: Request only required attributes
- **Search Scope**: Use appropriate search scope for performance

**Connection Management:**
- **Connection Pooling**: Reuse connections for multiple operations
- **Timeout Configuration**: Set appropriate timeouts for operations
- **Load Balancing**: Distribute load across multiple LDAP servers
- **Caching**: Cache frequently accessed directory information

**Scalability:**
- **Batch Operations**: Group multiple operations for efficiency
- **Parallel Processing**: Process multiple LDAP operations concurrently
- **Resource Monitoring**: Monitor memory and CPU usage
- **Network Optimization**: Optimize network traffic and bandwidth usage

#### Integration Examples:

**1. Active Directory User Management:**
- Create and manage Windows user accounts
- Sync with Office 365 and Azure AD
- Manage group memberships and permissions
- Implement password policies and security settings

**2. Enterprise Application Integration:**
- Sync user data with CRM and ERP systems
- Provision accounts in business applications
- Implement single sign-on workflows
- Manage application-specific attributes

**3. Compliance and Governance:**
- Generate user access reports
- Implement data retention policies
- Audit directory changes and access
- Ensure regulatory compliance (SOX, GDPR, etc.)

**4. Help Desk Automation:**
- Automate common help desk tasks
- Self-service password resets
- Account unlock procedures
- Directory information lookup

#### Templates Available:
- **Adaptive RAG with Google Gemini & Qdrant: Context-Aware Query Answering**
- **Adaptive RAG Strategy with Query Classification & Retrieval (Gemini & Qdrant)**
- **OpenAI Responses API Adapter for LLM and AI Agent Workflows**
- **Browse LDAP integration templates**

#### Related Resources:
- **LDAP Documentation**: [The LDAP Search Operation](https://ldap.com/the-ldap-search-operation/)
- **Active Directory**: Microsoft Active Directory integration guides
- **OpenLDAP**: Open-source LDAP server documentation
- **Directory Standards**: LDAP protocol and schema specifications

#### Related Nodes:
- **HTTP Request**: REST API integration for directory services
- **Code**: Custom LDAP operations and data processing
- **Database**: Alternative user management systems
- **Edit Fields (Set)**: Data transformation for directory operations
- **Filter**: Filter directory search results

#### Best Practices:

**1. Directory Design:**
- **Schema Planning**: Plan directory schema and object classes carefully
- **Naming Conventions**: Use consistent naming conventions for DNs
- **Organizational Structure**: Design logical organizational unit structure
- **Attribute Standards**: Standardize attribute usage across objects

**2. Security Implementation:**
- **Secure Connections**: Always use LDAPS for production environments
- **Access Controls**: Implement proper access controls and permissions
- **Credential Management**: Use service accounts with minimal privileges
- **Audit Trail**: Maintain comprehensive audit logs

**3. Performance Optimization:**
- **Indexed Searches**: Use indexed attributes in search filters
- **Efficient Queries**: Design efficient search patterns
- **Connection Management**: Optimize connection usage and pooling
- **Result Pagination**: Use pagination for large datasets

**4. Operational Excellence:**
- **Error Handling**: Implement robust error handling and retry logic
- **Monitoring**: Monitor directory operations and performance
- **Documentation**: Document directory structure and workflows
- **Testing**: Test thoroughly in non-production environments

#### Troubleshooting:

**Common Issues:**
- **Authentication Failures**: Check credential configuration and permissions
- **Connection Problems**: Verify server connectivity and security settings
- **Search Failures**: Validate search filters and base DN specifications
- **Attribute Errors**: Confirm attribute names and schema requirements

**Resolution Strategies:**
- **Connection Testing**: Test LDAP connectivity independently
- **Permission Validation**: Verify service account permissions
- **Schema Verification**: Check LDAP schema and object class requirements
- **Network Diagnostics**: Diagnose network connectivity and firewall issues

#### Error Handling:

**LDAP-Specific Errors:**
- **Invalid DN**: Handle malformed Distinguished Name errors
- **Attribute Violations**: Manage schema constraint violations
- **Access Denied**: Handle insufficient permission errors
- **Server Unavailable**: Implement retry logic for server outages

**Workflow Integration:**
- **Error Workflows**: Implement error handling workflows for LDAP failures
- **Fallback Procedures**: Provide alternative processing paths
- **Notification Systems**: Alert administrators of critical LDAP errors
- **Data Validation**: Validate data before LDAP operations

#### Advanced Use Cases:

**1. Multi-Forest Active Directory:**
- **Cross-Forest Queries**: Search across multiple AD forests
- **Trust Relationships**: Leverage forest trust relationships
- **Global Catalog**: Utilize global catalog for efficient searches
- **Site Topology**: Optimize operations based on site topology

**2. Hybrid Cloud Integration:**
- **Azure AD Connect**: Integrate with Azure AD synchronization
- **Cloud Directory**: Sync with cloud-based directory services
- **Hybrid Authentication**: Support hybrid authentication scenarios
- **Migration Workflows**: Assist in cloud migration projects

**3. Compliance Automation:**
- **SOX Compliance**: Implement Sarbanes-Oxley compliance workflows
- **GDPR Right to be Forgotten**: Automate data deletion procedures
- **Access Certification**: Implement regular access certification processes
- **Regulatory Reporting**: Generate compliance reports automatically

**4. DevOps Integration:**
- **Infrastructure as Code**: Manage directory objects as code
- **CI/CD Pipelines**: Integrate directory operations in deployment pipelines
- **Environment Management**: Manage directory objects across environments
- **Automation Testing**: Test directory operations in automated pipelines

#### Monitoring and Analytics:

**Directory Health:**
- **Connection Monitoring**: Monitor LDAP server connectivity and health
- **Performance Metrics**: Track query performance and response times
- **Error Rates**: Monitor error rates and failure patterns
- **Capacity Planning**: Track directory growth and usage patterns

**Business Intelligence:**
- **User Analytics**: Analyze user access patterns and behavior
- **Group Membership**: Track group membership changes over time
- **Access Trends**: Identify access trends and anomalies
- **Compliance Metrics**: Generate compliance dashboards and reports

#### Future Considerations:

**Technology Evolution:**
- **Cloud Migration**: Prepare for cloud directory migrations
- **Zero Trust**: Implement zero trust security models
- **API Integration**: Leverage modern directory APIs alongside LDAP
- **Identity Governance**: Integrate with identity governance platforms

**Scalability Planning:**
- **Performance Optimization**: Plan for increased directory usage
- **High Availability**: Implement high availability and disaster recovery
- **Load Distribution**: Plan for load distribution and scaling
- **Architecture Evolution**: Evolve directory architecture for modern needs

### 32. Limit
**Status**: ✅ Active
**Category**: Core - Data Processing/Flow Control
**Purpose**: Remove items beyond a defined maximum number to control data flow and optimize workflow performance by limiting dataset size

#### Authentication:
- **No credentials required**: Direct data processing operation
- **Simple configuration**: Only requires item count and selection criteria
- **Performance focused**: Designed for workflow optimization and resource management

#### Operations:
**1. Limit Item Count** - Control the number of items that pass through to subsequent nodes

#### Core Parameters:

**1. Max Items**
- **Purpose**: Set the maximum number of items that n8n should keep from input data
- **Type**: Numeric value (positive integer)
- **Behavior**: If input data contains more items than this value, excess items are removed
- **Range**: Any positive integer (1, 10, 100, 1000, etc.)
- **Expression Support**: Use n8n expressions for dynamic limit values

**2. Keep**
- **Purpose**: Choose which items to keep when limiting is necessary
- **Selection Strategy**: Determine whether to keep items from beginning or end of dataset

**Keep Options:**

**First Items** (Default):
- **Behavior**: Keeps the first N items from the beginning of input data
- **Use Case**: Process the most recent entries, chronological data, priority items
- **Example**: Keep first 10 items from a list of 100 items
- **Order**: Maintains original item order

**Last Items**:
- **Behavior**: Keeps the last N items from the end of input data  
- **Use Case**: Get most recent entries, latest updates, tail of dataset
- **Example**: Keep last 10 items from a list of 100 items
- **Order**: Maintains original item order (last items in original sequence)

#### Key Features:

**Data Flow Control:**
- **Volume Management**: Control data volume flowing through workflows
- **Performance Optimization**: Reduce processing load on downstream nodes
- **Resource Conservation**: Prevent memory and processing overloads
- **Workflow Efficiency**: Optimize execution time by limiting dataset size

**Flexible Selection:**
- **Direction Control**: Choose items from beginning or end of dataset
- **Dynamic Limits**: Use expressions for calculated item limits
- **Order Preservation**: Maintain original data ordering
- **Simple Configuration**: Minimal parameters for maximum effectiveness

**Workflow Integration:**
- **Strategic Placement**: Use early in workflows to optimize downstream processing
- **API Optimization**: Limit API calls and responses for efficiency
- **Testing Support**: Use smaller datasets during development and testing
- **Production Scaling**: Control data volume in production environments

#### Use Cases:

**1. Performance Optimization:**
- **Large Dataset Handling**: Limit processing of large datasets to manageable sizes
- **API Rate Limiting**: Reduce API calls by processing fewer items
- **Memory Management**: Prevent memory overflow with large data sets
- **Execution Time Control**: Reduce workflow execution time by limiting data volume

**2. Development and Testing:**
- **Test Data Samples**: Use small data samples during workflow development
- **Debugging**: Limit data volume for easier debugging and testing
- **Proof of Concept**: Test workflows with smaller datasets before scaling
- **Development Efficiency**: Faster iteration cycles with limited data

**3. Business Logic Implementation:**
- **Top N Results**: Get top N items from sorted data (sales, rankings, scores)
- **Recent Items**: Process only the most recent entries (logs, updates, messages)
- **Sampling**: Create representative samples from larger datasets
- **Batch Processing**: Limit batch sizes for controlled processing

**4. Resource Management:**
- **Cost Control**: Limit API calls to control costs
- **System Protection**: Prevent system overload with large datasets
- **Bandwidth Conservation**: Reduce data transfer by limiting item count
- **Storage Optimization**: Control data storage requirements

#### Common Integration Patterns:

**1. API Response Limiting:**
```
HTTP Request → Limit (First 10) → Process Items → Store Results
```
- Limit API response processing to first 10 items
- Optimize processing time and resource usage
- Focus on most relevant items

**2. Database Query Optimization:**
```
Database Query → Sort → Limit (Last 5) → Send Email Report
```
- Get recent database entries
- Limit report to last 5 items
- Optimize email content and processing

**3. Social Media Processing:**
```
Social Media API → Filter → Limit (First 20) → AI Analysis → Response
```
- Limit social media posts processing
- Focus AI analysis on subset of data
- Improve response time and accuracy

**4. File Processing:**
```
Read Directory → Limit (First 5) → Process Files → Archive
```
- Process only first 5 files from directory
- Prevent overwhelming downstream processing
- Maintain manageable batch sizes

#### Advanced Features:

**Expression-Based Limiting:**
- **Dynamic Limits**: Use expressions to calculate limit based on data or conditions
- **Conditional Limiting**: Apply different limits based on data characteristics
- **Time-Based Limits**: Calculate limits based on time windows or schedules
- **Data-Driven Limits**: Set limits based on previous node outputs

**Strategic Placement:**
- **Early Workflow**: Place early to optimize entire workflow performance
- **After Sorting**: Combine with Sort node for top N functionality
- **Before Expensive Operations**: Limit data before costly API calls or processing
- **Testing Branches**: Use in test branches for development workflows

**Data Selection Strategies:**
- **Priority Processing**: Use "First Items" for priority-ordered data
- **Recent Data**: Use "Last Items" for chronologically ordered data
- **Sample Selection**: Use for statistical sampling of larger datasets
- **Quality Control**: Limit processing to high-quality subset of data

#### Performance Considerations:

**Workflow Optimization:**
- **Early Placement**: Place Limit nodes early in workflows for maximum benefit
- **Resource Savings**: Significantly reduce downstream processing requirements
- **Memory Efficiency**: Reduce memory consumption for large datasets
- **Execution Speed**: Improve overall workflow execution time

**Strategic Usage:**
- **Development vs Production**: Use different limits for development and production
- **Scalability**: Design workflows to handle varying data volumes
- **Resource Planning**: Plan resource requirements based on limited data volumes
- **Cost Management**: Control API and processing costs through limiting

#### Integration Examples:

**1. E-commerce Analytics:**
- **Top Products**: Limit to top 20 products for dashboard display
- **Recent Orders**: Process only last 50 orders for daily reports
- **Customer Insights**: Limit customer analysis to top 100 customers
- **Inventory Management**: Focus on top-selling items for inventory updates

**2. Content Management:**
- **Recent Articles**: Display only last 10 articles on homepage
- **Popular Content**: Show top 5 most popular content items
- **Comment Processing**: Process only recent 25 comments for moderation
- **Media Processing**: Limit image processing to first 15 uploaded files

**3. Social Media Monitoring:**
- **Brand Mentions**: Process top 30 brand mentions for analysis
- **Trending Topics**: Focus on top 10 trending topics
- **Influencer Posts**: Analyze posts from top 20 influencers
- **Engagement Analysis**: Limit to most engaging 50 posts

**4. Business Intelligence:**
- **Sales Reports**: Focus on top 25 sales representatives
- **Performance Metrics**: Display top 15 performing metrics
- **Customer Analysis**: Analyze top 40 customers by revenue
- **Market Trends**: Focus on top 12 market trend indicators

#### Expression Examples:

**Dynamic Limiting:**
```javascript
// Limit based on day of week (more on weekdays)
{{ $now.weekday <= 5 ? 20 : 10 }}

// Limit based on data volume
{{ $input.all().length > 100 ? 50 : $input.all().length }}

// Limit based on item property
{{ $json.priority === 'high' ? 10 : 5 }}

// Time-based limiting
{{ $now.hour < 12 ? 15 : 25 }}
```

**Conditional Logic:**
```javascript
// Limit based on environment
{{ $env.NODE_ENV === 'development' ? 5 : 100 }}

// Limit based on user role
{{ $json.user_role === 'admin' ? 50 : 20 }}

// Limit based on data source
{{ $json.source === 'api' ? 30 : 15 }}
```

#### Best Practices:

**1. Strategic Placement:**
- **Early Optimization**: Place Limit nodes early in workflows for maximum benefit
- **Post-Sorting**: Use after Sort nodes to get top/bottom N items
- **Pre-Processing**: Limit data before expensive operations (AI, API calls)
- **Development Testing**: Use smaller limits during development and testing

**2. Limit Selection:**
- **Business Requirements**: Align limits with actual business needs
- **Performance Testing**: Test different limits to find optimal performance
- **Resource Constraints**: Consider system resource limitations
- **User Experience**: Balance data completeness with response time

**3. Keep Strategy:**
- **Data Nature**: Choose First/Last based on data characteristics
- **Business Logic**: First items for priority data, Last items for recent data
- **Sorting Integration**: Combine with Sort for meaningful top/bottom selection
- **Use Case Alignment**: Match strategy with specific use case requirements

**4. Workflow Design:**
- **Fallback Plans**: Consider what happens when limit is reached
- **Monitoring**: Monitor how often limits are applied
- **Documentation**: Document limit rationale and impact
- **Testing**: Test workflows with various data volumes

#### Common Patterns:

**1. Top N Pattern:**
```
Data Source → Sort (by value) → Limit (First N) → Process Top Items
```

**2. Recent Items Pattern:**
```
Data Source → Sort (by date) → Limit (Last N) → Process Recent Items
```

**3. Sample Processing Pattern:**
```
Large Dataset → Limit (First 100) → Test Processing → Scale Up
```

**4. Batch Control Pattern:**
```
API Data → Limit (First 25) → Process Batch → Continue/Stop
```

#### Security Considerations:

**Data Privacy:**
- **Sensitive Data**: Ensure limited data doesn't expose sensitive information
- **Access Control**: Apply appropriate access controls to limited datasets
- **Audit Logging**: Log limiting operations for compliance
- **Data Retention**: Consider data retention policies with limiting

**Performance Security:**
- **DoS Prevention**: Use limiting to prevent denial-of-service attacks
- **Resource Protection**: Protect system resources from data overflow
- **Rate Limiting**: Implement rate limiting through data volume control
- **System Stability**: Maintain system stability with reasonable limits

#### Templates Available:
- **Scrape and summarize webpages with AI**
- **Chat with OpenAI Assistant (by adding a memory)**
- **Hacker News to Video Content**
- **Browse Limit integration templates**

#### Related Resources:
- **Data Structure and Data Flow**: [n8n Data Handling Documentation](../../../../data/)
- **Performance Optimization**: Best practices for workflow optimization
- **Workflow Design**: Guidelines for efficient workflow design

#### Related Nodes:
- **Sort**: Sort data before limiting for meaningful top/bottom selection
- **Filter**: Alternative for removing items based on conditions rather than count
- **Split in Batches**: Process data in batches instead of limiting
- **Remove Duplicates**: Remove duplicate items to reduce dataset size
- **Aggregate**: Group and summarize data to reduce volume

#### Troubleshooting:

**Common Issues:**
- **No Limiting Applied**: Check if input data count exceeds Max Items setting
- **Wrong Items Selected**: Verify Keep setting (First vs Last items)
- **Unexpected Results**: Ensure data is properly sorted before limiting
- **Performance Issues**: Check if limit is too high for available resources

**Resolution Strategies:**
- **Data Inspection**: Examine input data count and structure
- **Parameter Validation**: Verify Max Items and Keep settings
- **Sort Integration**: Add Sort node before Limit for predictable results
- **Testing**: Test with known datasets to verify behavior

#### Error Handling:

**Input Validation:**
- **Positive Numbers**: Max Items must be positive integer
- **Data Availability**: Handle cases with no input data
- **Limit Validation**: Ensure Max Items is reasonable for use case
- **Expression Errors**: Validate expressions used in Max Items

**Workflow Integration:**
- **Empty Results**: Handle cases where limiting results in no data
- **Downstream Impact**: Consider impact on nodes expecting specific data counts
- **Error Propagation**: Ensure limiting errors don't break workflow
- **Monitoring**: Monitor limiting effectiveness and impact

#### Advanced Use Cases:

**1. Dynamic Batch Processing:**
- **Variable Batches**: Use expressions to vary batch sizes based on conditions
- **Adaptive Limiting**: Adjust limits based on system performance
- **Resource-Based Limiting**: Limit based on available system resources
- **Time-Based Batching**: Vary limits based on time of day or load

**2. Quality Control:**
- **Sample Testing**: Limit to sample size for quality testing
- **Error Rate Control**: Limit processing when error rates are high
- **Performance Monitoring**: Use limiting to monitor workflow performance
- **A/B Testing**: Limit different data sets for comparison testing

**3. Multi-Stage Processing:**
- **Pipeline Stages**: Different limits at different pipeline stages
- **Iterative Processing**: Process data in multiple limited iterations
- **Progressive Enhancement**: Start with limited data, expand gradually
- **Staged Deployment**: Use limiting for gradual feature rollouts

**4. Cost Management:**
- **API Cost Control**: Limit API calls to control costs
- **Processing Cost**: Limit expensive processing operations
- **Storage Cost**: Limit data storage through reduced datasets
- **Resource Cost**: Control compute costs through data limiting

#### Monitoring and Analytics:

**Limit Effectiveness:**
- **Application Rate**: Monitor how often limits are applied
- **Data Volume**: Track input vs output data volumes
- **Performance Impact**: Measure performance improvement from limiting
- **Business Impact**: Assess impact on business metrics

**Optimization Opportunities:**
- **Limit Adjustment**: Regularly review and adjust limits
- **Pattern Analysis**: Analyze data patterns to optimize limiting strategy
- **Performance Correlation**: Correlate limits with performance metrics
- **Cost Analysis**: Track cost savings from limiting operations

#### Future Considerations:

**Scalability Planning:**
- **Growth Accommodation**: Plan for data growth and limit adjustments
- **Performance Scaling**: Scale limits with system capabilities
- **Business Scaling**: Adjust limits as business requirements change
- **Technology Evolution**: Adapt limiting strategy to new technologies

**Enhanced Features:**
- **Smart Limiting**: AI-based dynamic limit adjustment
- **Predictive Limiting**: Predict optimal limits based on patterns
- **Adaptive Processing**: Automatically adjust limits based on performance
- **Context-Aware Limiting**: Limits based on broader workflow context

#### Workflow Optimization Tips:

**1. Limit Placement Strategy:**
- **Early Limiting**: Place limits as early as possible in workflows
- **Multiple Limits**: Use multiple limit nodes for different processing stages
- **Conditional Limits**: Apply different limits based on conditions
- **Branch-Specific Limits**: Use different limits for different workflow branches

**2. Data Flow Design:**
- **Sorting Integration**: Always sort before limiting for meaningful results
- **Filter Combination**: Combine filtering and limiting for optimal results
- **Batch Coordination**: Coordinate limits with batch processing requirements
- **Pipeline Optimization**: Design limits to optimize entire pipeline performance

**3. Performance Monitoring:**
- **Baseline Measurement**: Establish performance baselines with different limits
- **Continuous Monitoring**: Monitor limit effectiveness over time
- **Adaptive Adjustment**: Adjust limits based on performance data
- **Impact Assessment**: Regularly assess business impact of limiting decisions

### 33. Local File Trigger
**Status**: ✅ Active (Self-hosted only)
**Category**: Core - Trigger/File System
**Purpose**: Start a workflow when detecting changes on the file system - files or folders getting added, changed, or deleted

#### Availability:
- **Self-hosted n8n only**: Not available on n8n Cloud
- **File system access**: Requires direct access to host file system
- **Local monitoring**: Monitors local file system changes in real-time

#### Authentication:
- **No credentials required**: Direct file system access
- **File permissions**: Requires appropriate file system permissions
- **Path access**: Must have read access to monitored paths

#### Operations:
**1. File System Monitoring** - Trigger workflows based on file system changes

#### Core Parameters:

**1. Trigger On** - Choose what event type to watch for:

**Changes to a Specific File**:
- **Purpose**: Monitor a single file for changes
- **File to Watch**: Enter the complete path to the file to monitor
- **Path Format**: Use absolute or relative paths from n8n working directory
- **Change Detection**: Triggers on file modification, creation, or deletion

**Changes Involving a Specific Folder**:
- **Purpose**: Monitor a directory for file and folder changes
- **Folder to Watch**: Enter the path of the folder to monitor
- **Watch for**: Select specific types of changes to monitor

#### Change Type Options (Folder Monitoring):

**All Changes** (Default):
- **File Addition**: New files created in the folder
- **File Modification**: Existing files modified
- **File Deletion**: Files removed from the folder
- **Folder Changes**: Subfolder creation, modification, deletion

**File Added**:
- **Trigger**: Only when new files are added to the monitored folder
- **Use Case**: Process new file uploads, imports, or deliveries
- **Behavior**: Ignores modifications and deletions

**File Changed**:
- **Trigger**: Only when existing files are modified
- **Use Case**: Process updated documents, logs, or data files
- **Behavior**: Ignores new files and deletions

**File Deleted**:
- **Trigger**: Only when files are removed from the folder
- **Use Case**: Cleanup operations, archival workflows, audit trails
- **Behavior**: Ignores new files and modifications

**Folder Added**:
- **Trigger**: Only when new subfolders are created
- **Use Case**: Organizational structure changes, automatic folder processing
- **Behavior**: Monitors directory structure changes only

**Folder Changed**:
- **Trigger**: When subfolder properties or contents change
- **Use Case**: Monitor directory modifications and reorganizations
- **Behavior**: Detects folder metadata and structure changes

**Folder Deleted**:
- **Trigger**: When subfolders are removed
- **Use Case**: Directory cleanup, structure reorganization
- **Behavior**: Monitors folder removal only

#### Node Options:

**1. Include Linked Files/Folders**
- **Purpose**: Monitor changes to symbolic links and linked files/folders
- **Behavior**: Follow symbolic links and monitor their targets
- **Use Case**: Handle linked content, shared folders, mounted drives
- **Default**: Disabled (ignore symbolic links)

**2. Ignore**
- **Purpose**: Specify files, folders, or patterns to exclude from monitoring
- **Pattern Support**: Uses [Anymatch](https://github.com/micromatch/anymatch) syntax for flexible pattern matching
- **Path Matching**: Tests against the entire file path, not just filename
- **Multiple Patterns**: Add multiple ignore patterns as needed

**3. Max Folder Depth**
- **Purpose**: Control how deep into folder structure to monitor for changes
- **Range**: Specify depth level (0 = only immediate folder, unlimited = all subfolders)
- **Performance**: Limiting depth reduces system resource usage
- **Use Case**: Focus monitoring on specific directory levels

#### Ignore Pattern Examples:

**Single File Ignore:**
```
**/<fileName>.<suffix>
# Example: **/myfile.txt
```
- **Pattern**: `**/myfile.txt`
- **Effect**: Ignores file named "myfile.txt" in any subdirectory
- **Use Case**: Exclude specific configuration or temporary files

**Directory Ignore:**
```
**/<directoryName>/**
# Example: **/myDirectory/**
```
- **Pattern**: `**/myDirectory/**`
- **Effect**: Ignores entire subdirectory and all its contents
- **Use Case**: Exclude cache folders, temp directories, build outputs

**File Type Ignore:**
```
**/*.log
**/*.tmp
**/*.cache
```
- **Pattern**: File extension-based exclusion
- **Effect**: Ignore all files with specified extensions
- **Use Case**: Exclude log files, temporary files, cache files

**Complex Pattern Examples:**
```
# Ignore all hidden files/folders (starting with .)
**/.*

# Ignore node_modules directories
**/node_modules/**

# Ignore backup files
**/*.bak
**/*~

# Ignore specific patterns
**/*temp*
**/*cache*
**/*.lock
```

#### Key Features:

**Real-time Monitoring:**
- **Instant Detection**: Immediate detection of file system changes
- **Event-driven**: Workflow triggers immediately when changes occur
- **Efficient Monitoring**: Uses file system watchers for optimal performance
- **Cross-platform**: Works on Windows, macOS, and Linux

**Flexible Pattern Matching:**
- **Anymatch Syntax**: Powerful pattern matching for include/exclude rules
- **Path-based Filtering**: Filter based on complete file paths
- **Wildcard Support**: Use wildcards for flexible file matching
- **Regular Expressions**: Support for regex patterns in ignore rules

**Workflow Integration:**
- **File Information**: Provides detailed information about changed files
- **Path Data**: Includes file paths, names, and change types
- **Binary Content**: Can access file content for processing
- **Metadata**: Includes file timestamps, sizes, and properties

#### Use Cases:

**1. File Processing Automation:**
- **Document Processing**: Automatically process new documents dropped into folders
- **Data Import**: Process CSV, JSON, or other data files when added
- **Image Processing**: Automatically resize, convert, or optimize uploaded images
- **Log Processing**: Real-time processing of application log files

**2. Backup and Synchronization:**
- **Automatic Backup**: Trigger backup workflows when files change
- **Cloud Sync**: Synchronize local changes to cloud storage
- **Version Control**: Automatically commit changes to version control
- **Archive Management**: Move old files to archive folders

**3. Development Workflows:**
- **Build Automation**: Trigger builds when source code changes
- **Deployment**: Automatically deploy when files are updated
- **Testing**: Run tests when test files or source code changes
- **Documentation**: Generate documentation when code changes

**4. Content Management:**
- **Media Processing**: Process uploaded media files automatically
- **Content Publication**: Publish content when files are added to specific folders
- **Asset Optimization**: Optimize and process digital assets
- **Quality Control**: Validate and process content uploads

**5. System Administration:**
- **Log Monitoring**: Monitor and alert on log file changes
- **Configuration Management**: React to configuration file changes
- **Security Monitoring**: Monitor sensitive files for unauthorized changes
- **Cleanup Automation**: Automatically clean up temporary files

#### Common Integration Patterns:

**1. File Processing Pipeline:**
```
Local File Trigger → Extract From File → Edit Fields (Set) → Database Insert
```
- Monitor folder for new data files
- Extract and process file contents
- Transform data for storage
- Store processed data in database

**2. Document Workflow:**
```
Local File Trigger → Convert to File → Send Email → Archive File
```
- Monitor for new documents
- Convert to different formats
- Email processed documents
- Move files to archive folder

**3. Media Processing:**
```
Local File Trigger → Edit Image → Compression → Cloud Upload
```
- Monitor for new images
- Process and optimize images
- Compress for web use
- Upload to cloud storage

**4. Backup Automation:**
```
Local File Trigger → Read/Write Files → Compression → FTP Upload
```
- Monitor for file changes
- Create backup copies
- Compress for efficient storage
- Upload to backup server

#### Advanced Features:

**File Change Detection:**
- **Change Types**: Distinguish between create, modify, and delete operations
- **Timestamp Tracking**: Access file modification timestamps
- **Size Monitoring**: Detect file size changes
- **Metadata Changes**: Monitor file attribute and permission changes

**Performance Optimization:**
- **Selective Monitoring**: Use ignore patterns to reduce monitoring overhead
- **Depth Limiting**: Limit folder depth to improve performance
- **Efficient Watchers**: Use native file system watchers for optimal performance
- **Resource Management**: Manage system resources effectively

**Error Handling:**
- **Permission Errors**: Handle insufficient file system permissions gracefully
- **Path Validation**: Validate file and folder paths before monitoring
- **Watcher Failures**: Recover from file system watcher failures
- **Resource Limits**: Handle system resource limitations

#### Output Data Structure:

**File Information:**
- **fileName**: Name of the changed file
- **filePath**: Complete path to the file
- **fileDirectory**: Directory containing the file
- **changeType**: Type of change (added, changed, deleted)
- **timestamp**: When the change occurred
- **fileSize**: Size of the file (if applicable)

**Change Details:**
- **eventType**: Specific file system event
- **watchedPath**: The path being monitored
- **relativePath**: Path relative to watched directory
- **isDirectory**: Whether the change involves a directory
- **fileExtension**: File extension for filtering and processing

#### Security Considerations:

**File System Access:**
- **Permission Management**: Ensure appropriate file system permissions
- **Path Validation**: Validate all file paths to prevent unauthorized access
- **Secure Monitoring**: Monitor only authorized directories
- **Access Logging**: Log file access for security auditing

**Data Protection:**
- **Sensitive Files**: Avoid monitoring sensitive system files
- **Content Security**: Handle file content securely
- **Path Traversal**: Prevent path traversal attacks
- **Malware Prevention**: Consider malware scanning for processed files

#### Performance Considerations:

**Monitoring Efficiency:**
- **Selective Monitoring**: Use ignore patterns to reduce monitoring overhead
- **Depth Limiting**: Limit monitoring depth for large directory structures
- **Pattern Optimization**: Use efficient ignore patterns
- **Resource Monitoring**: Monitor system resource usage

**Scalability:**
- **Large Directories**: Handle large directories efficiently
- **High Frequency Changes**: Manage high-frequency file changes
- **Multiple Watchers**: Optimize multiple file watcher instances
- **Memory Management**: Efficient memory usage for long-running monitors

#### Platform Considerations:

**Windows:**
- **Path Separators**: Handle Windows path separators correctly
- **Hidden Files**: Manage Windows hidden file attributes
- **Long Paths**: Handle long Windows file paths
- **File Locking**: Deal with Windows file locking behavior

**macOS:**
- **Resource Forks**: Handle macOS resource forks appropriately
- **Hidden Files**: Manage macOS hidden files (starting with .)
- **Case Sensitivity**: Handle case-sensitive/insensitive file systems
- **Spotlight**: Avoid triggering on Spotlight indexing

**Linux:**
- **Permissions**: Handle Linux file permissions and ownership
- **Symbolic Links**: Manage symbolic links appropriately
- **File System Types**: Support various Linux file systems
- **inotify Limits**: Handle inotify system limits

#### Integration Examples:

**1. Content Management System:**
- Monitor uploads folder for new content
- Automatically process and publish content
- Generate thumbnails and optimize media
- Update content database and search indices

**2. Development Pipeline:**
- Monitor source code directories
- Trigger automated builds and tests
- Deploy successful builds automatically
- Update documentation and notifications

**3. Data Processing:**
- Monitor data drop folders
- Process CSV, JSON, and XML files
- Validate and clean data automatically
- Import processed data to databases

**4. System Administration:**
- Monitor configuration files
- Restart services on configuration changes
- Log changes for audit purposes
- Backup critical files automatically

#### Templates Available:
- **Breakdown Documents into Study Notes using Templating MistralAI and Qdrant**
- **Build a Financial Documents Assistant using Qdrant and Mistral.ai**
- **Organise Your Local File Directories With AI**
- **Browse Local File Trigger integration templates**

#### Related Nodes:
- **Read/Write Files from Disk**: Process files found by trigger
- **Extract From File**: Process file contents
- **Convert to File**: Generate output files
- **HTTP Request**: Upload processed files to web services
- **Send Email**: Notify about file changes

#### Best Practices:

**1. Monitoring Setup:**
- **Specific Paths**: Monitor specific paths rather than broad directories
- **Ignore Patterns**: Use comprehensive ignore patterns to reduce noise
- **Depth Limits**: Set appropriate folder depth limits
- **Performance Testing**: Test monitoring performance with representative data

**2. File Processing:**
- **Error Handling**: Implement robust error handling for file operations
- **Atomic Operations**: Ensure file operations are atomic when possible
- **Backup Strategy**: Implement backup strategies for processed files
- **Validation**: Validate file formats and content before processing

**3. Security:**
- **Path Validation**: Always validate file paths and names
- **Permission Checks**: Verify file permissions before processing
- **Content Scanning**: Consider scanning files for security threats
- **Access Control**: Implement proper access controls

**4. Performance:**
- **Selective Monitoring**: Monitor only necessary files and folders
- **Efficient Patterns**: Use efficient ignore patterns
- **Resource Management**: Monitor system resource usage
- **Optimization**: Optimize workflows for expected file volumes

#### Troubleshooting:

**Common Issues:**
- **Permissions**: Insufficient file system permissions
- **Path Problems**: Invalid or inaccessible file paths
- **Pattern Issues**: Incorrect ignore patterns
- **Performance**: High resource usage with large directories

**Resolution Strategies:**
- **Permission Validation**: Verify file system permissions
- **Path Testing**: Test file paths manually
- **Pattern Testing**: Test ignore patterns with sample data
- **Performance Monitoring**: Monitor system resource usage

#### Error Handling:

**File System Errors:**
- **Permission Denied**: Handle insufficient permissions gracefully
- **File Not Found**: Manage missing files and directories
- **Access Violations**: Handle file access conflicts
- **System Limits**: Manage file system resource limits

**Workflow Integration:**
- **Error Workflows**: Implement error handling workflows
- **Retry Logic**: Add retry mechanisms for transient failures
- **Notification**: Alert administrators of critical errors
- **Fallback Procedures**: Provide alternative processing paths

#### Advanced Use Cases:

**1. Real-time Analytics:**
- **Log File Processing**: Real-time analysis of application logs
- **Performance Monitoring**: Monitor performance data files
- **Alert Generation**: Generate alerts based on file content changes
- **Dashboard Updates**: Update dashboards with file-based data

**2. Content Pipeline:**
- **Media Processing**: Automated media file processing pipelines
- **Document Workflows**: Automated document processing and distribution
- **Asset Management**: Digital asset management and optimization
- **Publishing Workflows**: Automated content publishing systems

**3. DevOps Automation:**
- **Configuration Management**: Automated configuration deployment
- **Build Pipelines**: Trigger builds on source code changes
- **Deployment Automation**: Automated application deployment
- **Infrastructure as Code**: Manage infrastructure configuration files

**4. Business Process Automation:**
- **Invoice Processing**: Automated invoice processing from file drops
- **Report Generation**: Automated report generation from data files
- **Data Integration**: Automated data integration from file sources
- **Compliance Monitoring**: Monitor compliance-related file changes

#### Monitoring and Maintenance:

**System Health:**
- **Resource Usage**: Monitor CPU and memory usage
- **File System Health**: Monitor file system performance
- **Watcher Status**: Ensure file watchers remain active
- **Error Rates**: Track file processing error rates

**Maintenance Tasks:**
- **Log Cleanup**: Regular cleanup of processing logs
- **Performance Tuning**: Optimize ignore patterns and settings
- **Path Validation**: Regular validation of monitored paths
- **Security Audits**: Regular security audits of monitored files

#### Future Considerations:

**Scalability:**
- **Distributed Monitoring**: Scale to multiple servers
- **Cloud Integration**: Integrate with cloud file storage
- **High Volume**: Handle high-volume file processing
- **Real-time Processing**: Optimize for real-time processing needs

**Technology Evolution:**
- **File System Changes**: Adapt to new file system technologies
- **Container Integration**: Optimize for containerized environments
- **Cloud Migration**: Prepare for cloud-based file monitoring
- **Performance Improvements**: Leverage new monitoring technologies

### 34. Loop Over Items (Split in Batches)
**Status**: ✅ Active
**Category**: Core - Flow Control/Batch Processing
**Purpose**: Loop through data when needed by splitting items into batches and processing them iteratively

#### Authentication:
- **No credentials required**: Direct data processing operation
- **Flow control**: Manages data flow through iterative processing
- **Batch processing**: Splits large datasets into manageable chunks

#### Operations:
**1. Batch Processing Loop** - Process data in batches with loop and done outputs

#### Core Functionality:

**Data Flow Mechanism:**
- **Save Original Data**: Node saves the original incoming data internally
- **Loop Output**: Returns predefined amount of data through "loop" output with each iteration
- **Done Output**: When execution completes, combines all processed data and returns through "done" output
- **Iterative Processing**: Enables controlled, batch-by-batch data processing

**Dual Output System:**
- **Loop Output**: Used during iterations - connects to processing nodes
- **Done Output**: Used after completion - contains all processed results
- **State Management**: Node maintains internal state between iterations
- **Data Accumulation**: Automatically accumulates results from all iterations

#### When to Use Loop Over Items Node:

**Important Note**: Most n8n nodes are designed to process lists of input items automatically. You often don't need the Loop Over Items node in workflows.

**Valid Use Cases:**
1. **Loop until all items are processed**: When you need explicit control over batch processing
2. **Node exceptions**: Specific nodes that don't automatically process multiple items
3. **Rate limiting avoidance**: Batch API requests to avoid rate limits from external services
4. **Memory management**: Process large datasets in smaller chunks to manage memory usage
5. **Conditional looping**: When you need dynamic loop control with termination conditions

**When NOT to Use:**
- **Standard processing**: Most n8n nodes already handle multiple items automatically
- **Simple transformations**: Use regular nodes for straightforward data transformations
- **Basic filtering**: Use Filter node instead of looping
- **One-time operations**: When you don't need iterative processing

#### Core Parameters:

**1. Batch Size**
- **Purpose**: Enter the number of items to return with each call
- **Type**: Numeric value (positive integer)
- **Behavior**: Determines how many items are processed in each iteration
- **Default**: Varies based on use case (commonly 1, 10, 50, 100)
- **Expression Support**: Use n8n expressions for dynamic batch sizing

**Batch Size Considerations:**
- **Small Batches (1-10)**: Better for API rate limiting, real-time processing
- **Medium Batches (10-100)**: Balanced performance for most use cases
- **Large Batches (100+)**: Better performance but higher memory usage
- **Dynamic Sizing**: Use expressions to adjust batch size based on conditions

#### Node Options:

**1. Reset**
- **Purpose**: Control how the node handles incoming data with each loop iteration
- **Default**: Off (continue with previous data)
- **Use Case**: Essential for pagination and conditional looping scenarios

**Reset Options:**

**Reset Off (Default)**:
- **Behavior**: Node treats incoming data as continuation of previous items
- **Use Case**: Standard batch processing of known dataset
- **State**: Maintains state between iterations
- **Memory**: Accumulates all data in memory

**Reset On**:
- **Behavior**: Node resets with current input data newly initialized with each loop
- **Use Case**: Pagination, API calls with changing parameters, conditional loops
- **State**: Each iteration starts fresh
- **Memory**: More efficient for large datasets with changing inputs

**Expression-Based Reset:**
- **Dynamic Control**: Switch from Fixed to Expression mode
- **Conditional Reset**: Reset based on expression evaluation results
- **Advanced Logic**: Implement complex reset conditions
- **Runtime Decision**: Determine reset behavior during execution

#### Advanced Reset Configuration:

**Fixed vs Expression Mode:**
- **Fixed Mode**: Simple on/off toggle for reset behavior
- **Expression Mode**: Dynamic reset control using n8n expressions
- **Conditional Logic**: Reset based on data content, iteration count, or external conditions
- **Complex Scenarios**: Handle advanced pagination and conditional looping

**Expression Examples:**
```javascript
// Reset every 5 iterations
{{ $node["Loop Over Items"].context["currentRunIndex"] % 5 === 0 }}

// Reset based on data content
{{ $json.hasMorePages === true }}

// Reset based on external condition
{{ $node["Previous Node"].json.continue === true }}
```

#### Use Cases:

**1. API Rate Limiting Management:**
- **Batch API Calls**: Split large datasets into smaller API request batches
- **Rate Limit Compliance**: Stay within API rate limits by controlling request frequency
- **Throttling**: Add delays between batches using Wait node
- **Error Recovery**: Handle rate limit errors gracefully with retry logic

**2. Large Dataset Processing:**
- **Memory Management**: Process large datasets without overwhelming system memory
- **Performance Optimization**: Improve workflow performance by processing in chunks
- **Progress Tracking**: Monitor processing progress through iterations
- **Partial Results**: Access intermediate results during processing

**3. Pagination Workflows:**
- **API Pagination**: Handle paginated API responses with unknown total pages
- **Database Pagination**: Process large database result sets in pages
- **File Processing**: Process large files in chunks
- **Search Results**: Handle search results that require pagination

**4. Conditional Processing:**
- **Dynamic Loops**: Continue looping based on runtime conditions
- **Data-Driven Iterations**: Number of iterations determined by data content
- **Exit Conditions**: Stop processing when specific conditions are met
- **Adaptive Processing**: Adjust processing based on intermediate results

**5. Specialized Node Integration:**
- **Node Exceptions**: Work with nodes that only process first item (RSS Feed Read, etc.)
- **Single-Item Nodes**: Enable multi-item processing for single-item-only nodes
- **Legacy Compatibility**: Handle older nodes with limited multi-item support
- **Custom Processing**: Implement custom processing logic requiring iteration

#### Common Integration Patterns:

**1. API Pagination Pattern:**
```
Manual Trigger → Loop Over Items → HTTP Request (API) → If (has more pages) → Loop Back
```
- Process paginated API responses
- Continue until all pages processed
- Use Reset option for fresh page data

**2. Batch API Processing:**
```
Data Source → Loop Over Items → HTTP Request → Wait → Process Results → Done Output
```
- Split large datasets into API-friendly batches
- Add delays to respect rate limits
- Accumulate all results at completion

**3. Large File Processing:**
```
Read File → Loop Over Items → Process Chunk → Transform Data → Done Output
```
- Process large files in manageable chunks
- Prevent memory overflow issues
- Maintain processing efficiency

**4. Multi-Source RSS Processing:**
```
RSS URLs → Loop Over Items → RSS Feed Read → Process Items → Aggregate Results
```
- Process multiple RSS feeds sequentially
- Handle single-item limitation of RSS Feed Read
- Combine results from all sources

#### Built-in Context Variables:

**Access Loop State Information:**

**1. Check Items Remaining:**
```javascript
// Check if node still has items to process
{{$node["Loop Over Items"].context["noItemsLeft"]}}
```
- **Returns**: Boolean value
- **false**: Node still has data to process
- **true**: All items have been processed
- **Use Case**: Conditional logic based on remaining items

**2. Get Current Index:**
```javascript
// Get current running index of the node
{{$node["Loop Over Items"].context["currentRunIndex"]}}
```
- **Returns**: Numeric index of current iteration
- **Zero-based**: First iteration returns 0
- **Use Case**: Progress tracking, conditional logic, logging

**3. Additional Context:**
```javascript
// Access other context variables
{{$node["Loop Over Items"].context["totalItems"]}}     // Total number of items
{{$node["Loop Over Items"].context["processedItems"]}} // Items processed so far
{{$node["Loop Over Items"].context["batchSize"]}}      // Current batch size
```

#### Advanced Features:

**Dynamic Batch Sizing:**
- **Expression-Based**: Use expressions to calculate batch size dynamically
- **Adaptive Sizing**: Adjust batch size based on performance or conditions
- **Resource-Based**: Size batches based on available system resources
- **Content-Based**: Adjust batch size based on data characteristics

**Loop Control:**
- **Conditional Termination**: Stop looping based on specific conditions
- **Early Exit**: Exit loop when desired results are achieved
- **Error Handling**: Handle errors during loop processing
- **State Persistence**: Maintain state between loop iterations

**Memory Management:**
- **Efficient Processing**: Optimize memory usage for large datasets
- **Garbage Collection**: Proper cleanup between iterations
- **Resource Monitoring**: Monitor memory usage during processing
- **Performance Optimization**: Balance batch size with memory constraints

#### Error Handling:

**Loop Safety:**
- **Infinite Loop Prevention**: Always include valid termination conditions
- **Timeout Handling**: Set appropriate timeouts for long-running loops
- **Error Recovery**: Handle errors during loop processing
- **State Validation**: Validate loop state before each iteration

**Critical Warning:**
**Include Valid Termination Condition**: For workflows using Reset option with conditional looping, it's critical to include a valid termination condition. If your termination condition never matches, your workflow execution will get stuck in an infinite loop.

**Common Termination Patterns:**
```javascript
// Maximum iteration limit
{{ $node["Loop Over Items"].context["currentRunIndex"] >= 100 }}

// Data-based termination
{{ $json.hasMore === false }}

// API-based termination
{{ $json.nextPageToken === null }}

// Error-based termination
{{ $json.error !== undefined }}
```

#### Performance Considerations:

**Batch Size Optimization:**
- **Small Batches**: Better for rate limiting, real-time processing
- **Large Batches**: Better performance but higher memory usage
- **Dynamic Adjustment**: Adjust based on system performance and data characteristics
- **Testing**: Test different batch sizes to find optimal performance

**Memory Management:**
- **Reset Usage**: Use Reset option for large datasets to prevent memory accumulation
- **Garbage Collection**: Allow time for garbage collection between iterations
- **Resource Monitoring**: Monitor memory usage during loop processing
- **Optimization**: Optimize processing logic within loops

**Network Efficiency:**
- **Rate Limiting**: Respect API rate limits with appropriate batch sizes
- **Connection Reuse**: Optimize network connections for multiple requests
- **Error Handling**: Implement retry logic for network failures
- **Monitoring**: Monitor network performance during batch processing

#### Examples and Templates:

**1. RSS Feed Processing Example:**
```javascript
// Code node to create RSS URLs
return [
  { json: { url: 'https://medium.com/feed/n8n-io' } },
  { json: { url: 'https://dev.to/feed/n8n' } }
];
```
- Loop Over Items with Batch Size: 1
- RSS Feed Read node with URL mapping: `{{ $json.url }}`
- Process each RSS feed sequentially

**2. Pagination Example:**
```javascript
// Initial page setup
{ "page": 1, "hasMore": true }

// Loop condition (If node)
{{ $json.hasMore === true }}

// API call with page parameter
{{ $json.page }}

// Update page number for next iteration
{{ $json.page + 1 }}
```

**3. Batch API Processing:**
- Split 1000 items into batches of 50
- Process each batch with API calls
- Add 1-second wait between batches
- Accumulate results in done output

#### Integration Examples:

**1. E-commerce Bulk Operations:**
- **Product Updates**: Update thousands of products in batches
- **Inventory Sync**: Synchronize inventory data in manageable chunks
- **Price Updates**: Bulk price updates with rate limiting
- **Category Management**: Process category assignments in batches

**2. Data Migration:**
- **Database Migration**: Migrate large datasets between systems
- **File Processing**: Process large files in chunks
- **API Data Transfer**: Transfer data between APIs in batches
- **Backup Operations**: Create backups of large datasets

**3. Social Media Management:**
- **Bulk Posting**: Schedule multiple social media posts
- **Content Processing**: Process large content libraries
- **Engagement Analysis**: Analyze engagement data in batches
- **Follower Management**: Manage large follower lists

**4. Business Intelligence:**
- **Report Generation**: Generate reports from large datasets
- **Data Analysis**: Analyze business data in manageable chunks
- **ETL Processes**: Extract, transform, load data in batches
- **Dashboard Updates**: Update dashboards with processed data

#### Templates Available:
- **Scrape business emails from Google Maps without the use of any third party APIs**
- **Back Up Your n8n Workflows To Github**
- **Scrape and store data from multiple website pages**
- **Browse Loop Over Items (Split in Batches) integration templates**

#### Related Resources:
- **Looping in n8n**: [Flow Logic Documentation](../../../../flow-logic/looping/)
- **Node Exceptions**: Understanding which nodes require loop handling
- **Rate Limits**: [Handle Rate Limits Documentation](../../rate-limits/)
- **Performance Optimization**: Best practices for batch processing

#### Related Nodes:
- **If**: Conditional logic for loop termination
- **Wait**: Add delays between batch processing
- **Merge**: Combine results from different processing paths
- **Filter**: Alternative for simple data filtering
- **RSS Feed Read**: Common use case requiring loop processing

#### Best Practices:

**1. Loop Design:**
- **Termination Conditions**: Always include valid termination conditions for Reset scenarios
- **Batch Size Optimization**: Test different batch sizes for optimal performance
- **Memory Management**: Use Reset option for large datasets
- **Error Handling**: Implement comprehensive error handling in loops

**2. Performance Optimization:**
- **Efficient Batch Sizes**: Balance performance with resource usage
- **Rate Limiting**: Respect external service rate limits
- **Progress Monitoring**: Track loop progress for long-running operations
- **Resource Management**: Monitor system resources during processing

**3. Workflow Integration:**
- **Clear Logic Flow**: Make loop logic clear and understandable
- **Documentation**: Document loop logic and termination conditions
- **Testing**: Test loops with various data volumes and conditions
- **Monitoring**: Monitor loop performance and success rates

**4. Safety Measures:**
- **Infinite Loop Prevention**: Implement maximum iteration limits
- **Timeout Handling**: Set appropriate timeouts for long operations
- **Error Recovery**: Handle and recover from processing errors
- **State Validation**: Validate loop state and data integrity

#### Troubleshooting:

**Common Issues:**
- **Infinite Loops**: Missing or incorrect termination conditions
- **Memory Issues**: Batch sizes too large for available memory
- **Performance Problems**: Inefficient batch sizes or processing logic
- **State Problems**: Incorrect Reset option configuration

**Resolution Strategies:**
- **Termination Validation**: Verify termination conditions are reachable
- **Batch Size Adjustment**: Optimize batch sizes for performance and memory
- **Reset Configuration**: Ensure Reset option matches use case requirements
- **Progress Monitoring**: Use context variables to monitor loop progress

#### Security Considerations:

**Data Protection:**
- **Memory Management**: Prevent memory leaks with proper batch sizing
- **State Security**: Ensure loop state doesn't expose sensitive data
- **Error Handling**: Prevent data exposure through error messages
- **Resource Limits**: Implement limits to prevent resource exhaustion

**Processing Security:**
- **Input Validation**: Validate data before loop processing
- **Rate Limiting**: Implement proper rate limiting for external calls
- **Access Control**: Ensure proper access controls during processing
- **Audit Logging**: Log loop operations for security monitoring

#### Advanced Use Cases:

**1. Dynamic Data Processing:**
- **Adaptive Algorithms**: Adjust processing based on data characteristics
- **Machine Learning**: Process training data in optimized batches
- **Real-time Analytics**: Process streaming data in time-based batches
- **Conditional Workflows**: Implement complex conditional processing logic

**2. Integration Orchestration:**
- **Multi-System Sync**: Coordinate data sync across multiple systems
- **Workflow Chaining**: Chain multiple workflows through batch processing
- **Service Coordination**: Coordinate multiple services with batch operations
- **Event Processing**: Process event streams in manageable batches

**3. Enterprise Operations:**
- **Bulk Operations**: Enterprise-scale bulk data operations
- **Compliance Processing**: Handle compliance requirements with batch processing
- **Audit Trail**: Maintain audit trails for batch operations
- **Resource Optimization**: Optimize enterprise resource usage through batching

#### Monitoring and Analytics:

**Loop Performance:**
- **Iteration Tracking**: Monitor iteration counts and processing times
- **Batch Efficiency**: Analyze batch size effectiveness
- **Error Rates**: Track error rates during loop processing
- **Resource Usage**: Monitor CPU, memory, and network usage

**Business Metrics:**
- **Processing Volume**: Track data processing volumes
- **Success Rates**: Monitor processing success rates
- **Performance Trends**: Analyze performance trends over time
- **Cost Optimization**: Optimize costs through efficient batch processing

#### Future Considerations:

**Scalability Planning:**
- **Horizontal Scaling**: Plan for distributed batch processing
- **Cloud Integration**: Leverage cloud services for large-scale processing
- **Performance Evolution**: Adapt to changing performance requirements
- **Technology Updates**: Stay current with n8n loop processing improvements

**Enhanced Features:**
- **Smart Batching**: AI-driven batch size optimization
- **Predictive Processing**: Predict optimal processing parameters
- **Automated Recovery**: Enhanced error recovery and retry mechanisms
- **Advanced Monitoring**: Enhanced monitoring and analytics capabilities

---

### 35. Manual Trigger
**Status**: ✅ Active
**Category**: Core - Trigger/Workflow Control
**Purpose**: Start a workflow manually by selecting "Test Workflow" when you don't want any automatic triggering options

#### Authentication:
- **No credentials required**: Direct workflow triggering mechanism
- **Manual control**: User-initiated workflow execution only
- **Testing focus**: Designed for manual testing and controlled execution

#### Operations:
**1. Manual Workflow Execution** - Trigger workflow execution on demand via Test Workflow button

#### Core Functionality:

**Manual Triggering:**
- **Test Workflow Button**: Workflows start only when user clicks "Test Workflow"
- **No Automatic Execution**: Prevents any automatic or scheduled execution
- **Development Focus**: Ideal for testing and development workflows
- **User Control**: Complete user control over when workflow executes

**Workflow Entry Point:**
- **Required Trigger**: All workflows need a trigger or start point
- **Manual Start Point**: Serves as workflow trigger for non-automatic workflows
- **Simple Configuration**: No parameters or configuration required
- **Single Instance**: Only one Manual Trigger allowed per workflow

#### Key Features:

**Simplicity:**
- **No Configuration**: No parameters, credentials, or setup required
- **Instant Setup**: Add to workflow and ready to use immediately
- **Clear Purpose**: Explicitly manual-only workflow execution
- **Visual Clarity**: Makes workflow execution intent clear

**Development Support:**
- **Testing Workflows**: Perfect for testing before adding automatic triggers
- **Development Phase**: Use during workflow development and debugging
- **Controlled Execution**: Execute workflows only when needed
- **Safe Testing**: No risk of accidental automatic execution

**Workflow Control:**
- **Intentional Execution**: Workflows run only when deliberately triggered
- **Resource Control**: No background processing or automatic resource usage
- **Debugging Support**: Manual control aids in debugging and troubleshooting
- **Step-by-step Testing**: Test workflow components individually

#### Use Cases:

**1. Workflow Development and Testing:**
- **Development Phase**: Use while building and testing workflows
- **Component Testing**: Test individual workflow components
- **Debug Workflows**: Debug workflow logic with controlled execution
- **Pre-deployment Testing**: Final testing before adding automatic triggers

**2. Manual-Only Workflows:**
- **Administrative Tasks**: Workflows that should only run on demand
- **Data Cleanup**: Manual data cleanup and maintenance operations
- **Report Generation**: Generate reports only when requested
- **System Maintenance**: System maintenance tasks requiring manual approval

**3. Controlled Operations:**
- **Sensitive Operations**: Operations requiring explicit user approval
- **Cost-Sensitive Tasks**: Expensive operations that should be manually triggered
- **One-Time Tasks**: Tasks that run infrequently or only once
- **Emergency Procedures**: Emergency or disaster recovery procedures

**4. Testing and Validation:**
- **Integration Testing**: Test integrations with external services
- **Data Validation**: Validate data processing logic
- **API Testing**: Test API calls and responses
- **Performance Testing**: Test workflow performance under controlled conditions

#### Common Integration Patterns:

**1. Development Workflow:**
```
Manual Trigger → [Workflow Logic] → Test Results
```
- Use during development for testing
- Replace with automatic trigger when ready for production
- Maintains same workflow logic throughout development

**2. Administrative Tasks:**
```
Manual Trigger → Database Cleanup → Send Notification
```
- Manual database maintenance operations
- User-controlled system administration
- Controlled execution of sensitive operations

**3. Report Generation:**
```
Manual Trigger → Data Collection → Generate Report → Send Email
```
- On-demand report generation
- User-initiated business reports
- Controlled resource usage for reporting

**4. Data Processing:**
```
Manual Trigger → Extract Data → Transform → Load → Validate
```
- Manual data processing workflows
- Controlled ETL operations
- Testing data processing logic

#### Workflow Lifecycle:

**Development Phase:**
1. **Start with Manual Trigger**: Begin workflow development with Manual Trigger
2. **Build and Test**: Develop workflow logic with manual testing
3. **Debug and Refine**: Use manual execution for debugging
4. **Validate Functionality**: Ensure workflow works correctly

**Production Transition:**
1. **Replace Trigger**: Replace Manual Trigger with appropriate automatic trigger
2. **Schedule/Event-Based**: Add Schedule Trigger, Webhook, or other automatic triggers
3. **Maintain Logic**: Keep same workflow logic, only change trigger
4. **Production Deployment**: Deploy with automatic triggering capability

#### Limitations and Constraints:

**Single Instance Restriction:**
- **One Per Workflow**: Only one Manual Trigger node allowed per workflow
- **Error Prevention**: System prevents adding multiple Manual Trigger nodes
- **Workflow Structure**: Maintains clear workflow entry point
- **Design Clarity**: Ensures workflow triggering logic remains clear

**Manual-Only Execution:**
- **No Automation**: Cannot be used for automated workflows
- **User Dependent**: Requires user interaction for every execution
- **No Scheduling**: Cannot be scheduled or triggered automatically
- **Testing Focus**: Primarily designed for testing and development

#### Common Issues and Troubleshooting:

**1. Multiple Manual Trigger Error:**
- **Error Message**: "Only one 'Manual Trigger' node is allowed in a workflow"
- **Cause**: Attempting to add second Manual Trigger node to workflow
- **Resolution**: Remove existing Manual Trigger or connect to different node
- **Prevention**: Use single Manual Trigger as workflow entry point

**2. Production Workflow Concerns:**
- **Issue**: Manual Trigger in production workflows
- **Problem**: Workflows won't run automatically in production
- **Solution**: Replace with appropriate automatic trigger (Schedule, Webhook, etc.)
- **Best Practice**: Use Manual Trigger only for testing and development

#### Best Practices:

**1. Development Workflow:**
- **Start Simple**: Begin all workflows with Manual Trigger during development
- **Test Thoroughly**: Use manual execution to test all workflow paths
- **Debug Systematically**: Test individual components with manual triggering
- **Document Testing**: Document test cases and expected results

**2. Transition Planning:**
- **Plan Triggers**: Design automatic triggers before development completion
- **Gradual Transition**: Replace Manual Trigger when workflow is stable
- **Maintain Logic**: Keep workflow logic identical when changing triggers
- **Test Transition**: Test automatic triggers thoroughly before production

**3. Workflow Design:**
- **Clear Intent**: Use Manual Trigger when manual control is intended
- **Single Entry**: Maintain single workflow entry point
- **Documentation**: Document why Manual Trigger is appropriate
- **Alternative Planning**: Plan alternative triggers for different scenarios

**4. Production Readiness:**
- **Trigger Replacement**: Replace before production deployment
- **Automation Ready**: Ensure workflow logic supports automation
- **Error Handling**: Implement proper error handling for automatic execution
- **Monitoring**: Add monitoring for automatic workflow execution

#### Integration Examples:

**1. Data Migration Testing:**
- Use Manual Trigger to test data migration logic
- Validate transformation rules with sample data
- Test error handling and data validation
- Ensure migration process works correctly

**2. API Integration Development:**
- Test API calls with Manual Trigger
- Validate request/response handling
- Test authentication and error scenarios
- Debug API integration issues

**3. Report Development:**
- Develop report generation logic with manual testing
- Test data collection and formatting
- Validate report output and delivery
- Ensure report accuracy before automation

**4. System Administration:**
- Create manual system maintenance workflows
- Test backup and recovery procedures
- Validate system health checks
- Control sensitive system operations

#### Related Nodes:
- **Schedule Trigger**: Automatic time-based workflow triggering
- **Webhook**: HTTP-based automatic workflow triggering
- **Email Trigger (IMAP)**: Email-based automatic workflow triggering
- **Local File Trigger**: File system-based automatic workflow triggering
- **Error Trigger**: Error-based workflow triggering

#### Templates Available:
- Most n8n templates use automatic triggers in production
- Manual Trigger primarily used during development phase
- Browse workflow templates for automatic trigger examples

#### Migration Strategies:

**From Manual to Automatic:**
1. **Identify Requirements**: Determine when workflow should run automatically
2. **Choose Trigger**: Select appropriate automatic trigger type
3. **Replace Node**: Replace Manual Trigger with chosen automatic trigger
4. **Configure Parameters**: Set up trigger parameters and credentials
5. **Test Automation**: Thoroughly test automatic execution
6. **Monitor Production**: Monitor automatic workflow execution

**Common Trigger Replacements:**
- **Time-Based**: Replace with Schedule Trigger for regular execution
- **Event-Based**: Replace with Webhook for external event triggering
- **Data-Based**: Replace with Email/File triggers for data-driven execution
- **Integration-Based**: Replace with service-specific triggers

#### Security Considerations:

**Access Control:**
- **Manual Control**: Manual Trigger provides complete user control
- **No Background Execution**: Prevents unauthorized automatic execution
- **User Authentication**: Relies on n8n user authentication
- **Explicit Execution**: Every execution requires explicit user action

**Development Security:**
- **Safe Testing**: No risk of accidental production execution
- **Controlled Environment**: Test in controlled development environment
- **Resource Protection**: Prevents accidental resource consumption
- **Data Protection**: Protects against accidental data processing

#### Performance Considerations:

**Resource Usage:**
- **Zero Background Load**: No background processing or resource usage
- **On-Demand Execution**: Resources used only during manual execution
- **Controlled Load**: Complete control over when resources are consumed
- **Testing Efficiency**: Efficient resource usage during testing

**Development Efficiency:**
- **Quick Testing**: Fast iteration cycles with manual triggering
- **Controlled Debugging**: Debug specific scenarios on demand
- **Resource Conservation**: No wasted resources on automatic execution
- **Development Speed**: Rapid development and testing cycles

#### Advanced Use Cases:

**1. Emergency Procedures:**
- Create emergency response workflows with Manual Trigger
- Ensure procedures run only when explicitly needed
- Provide controlled execution during crisis situations
- Maintain manual oversight of critical operations

**2. Compliance Operations:**
- Manual compliance checking and reporting
- Controlled execution of audit procedures
- User-supervised compliance workflows
- Explicit approval for compliance actions

**3. System Recovery:**
- Manual system recovery and restoration procedures
- Controlled execution of recovery workflows
- User-supervised disaster recovery operations
- Explicit approval for system changes

**4. Quality Assurance:**
- Manual quality check procedures
- Controlled testing and validation workflows
- User-supervised quality assurance processes
- Explicit approval for quality operations

#### Future Considerations:

**Workflow Evolution:**
- **Development to Production**: Plan evolution from manual to automatic
- **Trigger Flexibility**: Design workflows to support multiple trigger types
- **Automation Readiness**: Ensure workflows are ready for automation
- **Scalability Planning**: Plan for automatic execution scalability

**Enhanced Features:**
- **Conditional Manual**: Conditional manual triggering based on data
- **Approval Workflows**: Integration with approval and review processes
- **Multi-Step Manual**: Multi-step manual workflow execution
- **Advanced Controls**: Enhanced manual control and monitoring features

---

### 36. Markdown
**Status**: ✅ Active
**Category**: Core - Text Processing/Document Conversion
**Purpose**: Convert between Markdown and HTML formats, enabling content transformation for documentation, web publishing, and text processing workflows

#### Authentication:
- **No credentials required**: Direct text processing operations
- **Parser integration**: Uses established open-source parsing libraries
- **Format conversion**: Bidirectional conversion between Markdown and HTML

#### Operations (2 total):

**1. Markdown to HTML** - Convert Markdown text to HTML format
- **Purpose**: Transform Markdown content into HTML for web publishing, email, or display
- **Parser**: Uses Showdown library with GitHub Flavored Markdown extensions
- **Features**: Extensive customization options for HTML output

**2. HTML to Markdown** - Convert HTML content to Markdown format
- **Purpose**: Transform HTML content into Markdown for documentation, editing, or storage
- **Parser**: Uses node-html-markdown library
- **Features**: Comprehensive HTML element handling and formatting options

#### Core Parameters:

**1. Mode Selection**
- **Markdown to HTML**: Convert Markdown syntax to HTML elements
- **HTML to Markdown**: Convert HTML elements to Markdown syntax
- **Bidirectional**: Full support for both conversion directions

**2. Content Input**
- **HTML** (HTML to Markdown mode): Enter HTML content to convert to Markdown
- **Markdown** (Markdown to HTML mode): Enter Markdown content to convert to HTML
- **Field Name**: Changes dynamically based on selected mode
- **Expression Support**: Use n8n expressions for dynamic content input

**3. Destination Key**
- **Output Field**: Specify field name for converted content output
- **Nested Fields**: Support dot notation (e.g., `level1.level2.newKey`)
- **Flexible Naming**: Choose meaningful field names for output organization
- **Expression Support**: Use expressions for dynamic field naming

#### Node Options:

**Option Testing Recommendation**: Some options depend on each other or can interact. Test options to confirm effects meet requirements.

#### Markdown to HTML Options (25+ comprehensive options):

**Link and URL Handling:**
- **Add Blank To Links**: Open links in new window (`target="_blank"`) - Default: Disabled
- **Automatic Linking To URLs**: Convert URL strings to clickable links - Default: Disabled
- **Exclude Trailing Punctuation From URLs**: Clean URL endings for automatic linking - Default: Disabled

**Text Processing:**
- **Backslash Escapes HTML Tags**: Allow `\<div\>` to render as `&lt;div&gt;` - Default: Disabled
- **Encode Emails**: Transform ASCII emails to decimal entities for spam protection - Default: Enabled
- **Simple Line Breaks**: Create line breaks without double space requirement - Default: Disabled

**Document Structure:**
- **Complete HTML Document**: Output full HTML document vs fragment - Default: Disabled
  - **Fragment**: Just the converted content
  - **Complete**: Includes `<!DOCTYPE HTML>`, `<html>`, `<body>`, and `<head>` tags

**Header Configuration:**
- **Header Level Start**: Set starting header level (e.g., 2 makes `#` become `<h2>`) - Default: 1
- **Customized Header ID**: Support custom heading IDs using `{header ID here}` syntax - Default: Disabled
- **GitHub Compatible Header IDs**: Generate GitHub-style header IDs with dashes - Default: Disabled
- **Mandatory Space Before Header**: Require space between `#` and heading text - Default: Disabled
- **No Header ID**: Disable automatic header ID generation - Default: Disabled
- **Prefix Header ID**: Add prefix to all header IDs - Default: None
- **Raw Header ID**: Remove spaces and quotes from header IDs, replace with `-` - Default: Disabled
- **Raw Prefix Header ID**: Prevent modification of header prefixes - Default: Disabled

**GitHub Flavored Markdown Features:**
- **GitHub Code Blocks**: Enable fenced code blocks with syntax highlighting - Default: Enabled
- **GitHub Mentions**: Support `@username` linking to GitHub profiles - Default: Disabled
- **GitHub Mention Link**: Customize the link format for GitHub mentions - Default: Disabled
- **GitHub Task Lists**: Support `- [ ]` and `- [x]` task list syntax - Default: Disabled

**Text Formatting:**
- **Middle Word Asterisks**: Handle asterisks within words literally vs as Markdown - Default: Disabled
- **Middle Word Underscores**: Handle underscores within words literally vs as Markdown - Default: Disabled
- **Strikethrough**: Support `~~strikethrough~~` syntax - Default: Disabled
- **Emoji Support**: Enable emoji parsing and rendering - Default: Disabled

**Advanced Features:**
- **Parse Image Dimensions**: Support maximum image dimensions in Markdown syntax - Default: Disabled
- **Smart Indentation Fix**: Fix ES6 template string indentation in code blocks - Default: Disabled
- **Spaces Indented Sublists**: Remove four-space sublist requirement - Default: Disabled
- **Split Adjacent Blockquotes**: Split blockquotes separated by empty lines - Default: Disabled

**Table Support:**
- **Tables Support**: Enable table parsing and rendering - Default: Disabled
- **Tables Header ID**: Add IDs to table header tags - Default: Disabled

#### HTML to Markdown Options (13 comprehensive options):

**List and Text Formatting:**
- **Bullet Marker**: Character for unordered lists - Default: `*`
- **Emphasis Delimiter**: Character for `<em>` tags - Default: `_`
- **Strong Delimiter**: Characters for `<strong>` tags - Default: `**`

**Code Block Handling:**
- **Code Block Fence**: Characters for code blocks - Default: ` ``` `
- **Style For Code Block**: Choose between Fence and Indented styles - Default: Fence

**URL and Link Management:**
- **Place URLs At The Bottom**: Use link reference definitions at page bottom - Default: Disabled
- **Keep Images With Data**: Preserve data URLs for images (up to 1MB) - Default: Disabled

**Content Control:**
- **Max Consecutive New Lines**: Limit consecutive line breaks - Default: 3
- **Ignored Elements**: Specify HTML elements to ignore completely - Default: None
- **Treat As Blocks**: Elements to surround with blank lines - Default: None

**Advanced Text Processing:**
- **Text Replacement Pattern**: Define regex-based text replacement - Default: None
- **Global Escape Pattern**: Override default character escaping - Default: None
- **Line Start Escape Pattern**: Override line start character escaping - Default: None

#### Key Features:

**Comprehensive Format Support:**
- **Standard Markdown**: Full CommonMark specification compliance
- **GitHub Flavored Markdown**: Extended syntax support including tables, task lists, strikethrough
- **HTML Elements**: Complete HTML element conversion and handling
- **Custom Extensions**: Configurable extensions and syntax enhancements

**Bidirectional Conversion:**
- **Markdown → HTML**: Rich HTML output with customizable rendering
- **HTML → Markdown**: Clean Markdown generation from HTML sources
- **Format Preservation**: Maintain formatting intent during conversion
- **Loss Prevention**: Minimize information loss during conversion

**Extensive Customization:**
- **Output Control**: Fine-grained control over output formatting
- **Style Options**: Multiple styling approaches for different use cases
- **Compatibility Modes**: GitHub, standard, and custom compatibility options
- **Processing Options**: Advanced text processing and cleanup features

#### Use Cases:

**1. Documentation Workflows:**
- **README Generation**: Convert Markdown documentation to HTML for web display
- **API Documentation**: Transform API docs between formats for different platforms
- **Technical Writing**: Convert between formats for different publishing platforms
- **Knowledge Base**: Maintain documentation in Markdown, publish as HTML

**2. Content Management:**
- **Blog Publishing**: Convert Markdown posts to HTML for website publishing
- **Email Newsletters**: Transform Markdown content to HTML for email campaigns
- **CMS Integration**: Convert content between formats for different CMS platforms
- **Multi-platform Publishing**: Publish same content across different format requirements

**3. Web Development:**
- **Static Site Generation**: Convert Markdown content to HTML for static sites
- **Template Processing**: Process template content between formats
- **Content Migration**: Migrate content between different platforms and formats
- **Component Generation**: Generate HTML components from Markdown specifications

**4. Data Processing:**
- **Web Scraping**: Convert scraped HTML content to clean Markdown format
- **Content Extraction**: Extract and clean content from HTML sources
- **Document Processing**: Process documents between different markup formats
- **Archive Conversion**: Convert archived content between formats

**5. AI and Automation:**
- **AI Content Processing**: Prepare content in optimal format for AI processing
- **LLM Integration**: Convert between formats for language model consumption
- **Content Analysis**: Process content in preferred format for analysis tools
- **Automated Publishing**: Convert AI-generated content to publication formats

#### Integration Examples:

**1. Blog Publishing Pipeline:**
```
Markdown Files → Markdown (Markdown to HTML) → Edit Fields (Set) → HTTP Request (Publish)
```
- Convert Markdown blog posts to HTML
- Add metadata and formatting
- Publish to content management system

**2. Documentation Generation:**
```
GitHub API → Extract From File → Markdown (HTML to Markdown) → Convert to File
```
- Extract documentation from repositories
- Convert HTML docs to Markdown
- Generate clean documentation files

**3. Content Migration:**
```
HTTP Request (HTML) → Markdown (HTML to Markdown) → Edit Fields (Set) → Database Insert
```
- Scrape HTML content from old system
- Convert to Markdown for new system
- Store in database with proper formatting

**4. Email Newsletter:**
```
Markdown Content → Markdown (Markdown to HTML) → Edit Fields (Set) → Send Email
```
- Write newsletter content in Markdown
- Convert to HTML for email formatting
- Send formatted HTML emails

#### Advanced Features:

**GitHub Integration:**
- **GitHub Flavored Markdown**: Full GFM support including tables, task lists, mentions
- **Syntax Highlighting**: Code block syntax highlighting support
- **GitHub Mentions**: Automatic linking to GitHub user profiles
- **Issue References**: Support for GitHub issue and PR references

**Content Processing:**
- **Image Handling**: Support for image dimensions, data URLs, and processing
- **Table Processing**: Full table conversion with header support
- **Code Block Processing**: Multiple code block styles and syntax support
- **Link Processing**: Comprehensive link handling and URL processing

**Text Cleanup:**
- **Character Escaping**: Intelligent character escaping and processing
- **Line Break Handling**: Configurable line break processing
- **Whitespace Management**: Control over whitespace and formatting
- **Content Filtering**: Element filtering and content exclusion options

#### Parser Libraries:

**Markdown to HTML:**
- **Library**: Showdown
- **GitHub Flavored Markdown**: Extended syntax support
- **Customization**: Extensive option support for output control
- **Performance**: Optimized for workflow processing

**HTML to Markdown:**
- **Library**: node-html-markdown
- **Element Support**: Comprehensive HTML element handling
- **Formatting Options**: Multiple output formatting styles
- **Content Preservation**: Minimal information loss during conversion

#### Performance Considerations:

**Processing Efficiency:**
- **Large Documents**: Efficient processing of large markdown/HTML documents
- **Memory Usage**: Optimized memory usage for document conversion
- **Batch Processing**: Handle multiple documents efficiently
- **Stream Processing**: Support for processing large content streams

**Option Optimization:**
- **Feature Selection**: Enable only required features for better performance
- **Parser Configuration**: Optimize parser settings for specific use cases
- **Content Filtering**: Use filtering options to reduce processing overhead
- **Output Optimization**: Choose efficient output formats

#### Security Considerations:

**Content Safety:**
- **HTML Injection**: Be cautious with user-provided HTML content
- **Script Prevention**: Consider stripping script tags from HTML input
- **Link Validation**: Validate links in automatic linking features
- **Content Sanitization**: Implement content sanitization for user input

**Safe Processing:**
- **Input Validation**: Validate input format and structure
- **Output Encoding**: Proper encoding of output content
- **XSS Prevention**: Prevent cross-site scripting through proper escaping
- **Content Filtering**: Filter potentially harmful content elements

#### Templates Available:
- **AI agent that can scrape webpages**: Convert scraped HTML to Markdown for processing
- **Autonomous AI crawler**: Process web content between formats
- **Host Your Own AI Deep Research Agent**: Content format conversion for research
- **Browse Markdown integration templates**: Additional workflow examples

#### Related Nodes:
- **HTML**: Alternative for HTML generation and processing
- **Convert to File**: Save converted content as files
- **HTTP Request**: Download HTML content for conversion
- **Edit Fields (Set)**: Process and structure converted content
- **Extract From File**: Extract content from markdown/HTML files

#### Best Practices:

**1. Format Selection:**
- **Use Case Alignment**: Choose format based on end use requirements
- **Platform Compatibility**: Consider target platform format requirements
- **Content Type**: Select format appropriate for content type
- **Processing Pipeline**: Choose format that optimizes processing pipeline

**2. Option Configuration:**
- **Test Combinations**: Test option combinations for desired output
- **Minimal Configuration**: Use minimal options for better performance
- **Compatibility Focus**: Choose options for target platform compatibility
- **Content Specific**: Configure options based on content characteristics

**3. Content Processing:**
- **Input Validation**: Validate input content format and structure
- **Output Verification**: Verify converted content meets requirements
- **Error Handling**: Implement proper error handling for conversion failures
- **Quality Control**: Review converted content for accuracy

**4. Workflow Integration:**
- **Pipeline Design**: Design efficient conversion pipelines
- **Format Standardization**: Standardize on formats within workflows
- **Content Routing**: Route content based on format requirements
- **Processing Optimization**: Optimize conversion placement in workflows

#### Troubleshooting:

**Common Issues:**
- **Option Conflicts**: Some options may conflict or interact unexpectedly
- **Format Problems**: Input format may not match expected structure
- **Content Loss**: Some HTML elements may not convert perfectly to Markdown
- **Syntax Issues**: Markdown syntax may not render as expected in HTML

**Resolution Strategies:**
- **Option Testing**: Test option combinations with sample content
- **Input Validation**: Validate input content before conversion
- **Format Verification**: Verify input/output formats meet expectations
- **Incremental Testing**: Test with simple content before complex documents

#### Error Handling:

**Conversion Errors:**
- **Invalid Input**: Handle malformed HTML or Markdown gracefully
- **Unsupported Elements**: Manage elements that don't convert cleanly
- **Encoding Issues**: Handle character encoding problems
- **Size Limitations**: Manage very large document processing

**Workflow Integration:**
- **Error Workflows**: Implement error handling workflows for conversion failures
- **Fallback Processing**: Provide alternative processing paths for failures
- **Content Validation**: Validate content before and after conversion
- **Quality Assurance**: Implement quality checks for converted content

#### Advanced Use Cases:

**1. Multi-format Publishing:**
- **Content Source**: Maintain content in Markdown for easy editing
- **Multiple Outputs**: Generate HTML, PDF, and other formats from Markdown
- **Platform Optimization**: Optimize content for different platforms
- **Automated Distribution**: Distribute content in multiple formats automatically

**2. Content Migration Projects:**
- **Legacy System Migration**: Convert content from old HTML systems
- **Platform Standardization**: Standardize content formats across platforms
- **Archive Processing**: Process archived content for modern systems
- **Bulk Conversion**: Handle large-scale content conversion projects

**3. Documentation Automation:**
- **API Documentation**: Generate documentation from code comments
- **README Generation**: Automated README file generation and updates
- **Help System**: Convert documentation between help system formats
- **Version Control**: Maintain documentation in version-friendly formats

**4. Content Optimization:**
- **SEO Optimization**: Convert content for SEO-friendly formats
- **Performance Optimization**: Optimize content format for loading speed
- **Mobile Optimization**: Convert content for mobile-friendly formats
- **Accessibility**: Ensure content accessibility across formats

#### Monitoring and Analytics:

**Conversion Quality:**
- **Success Rates**: Monitor conversion success rates
- **Content Integrity**: Track content preservation quality
- **Performance Metrics**: Monitor conversion speed and efficiency
- **Error Patterns**: Analyze common conversion errors

**Usage Patterns:**
- **Format Preferences**: Track which conversion directions are most used
- **Option Usage**: Monitor which options are most valuable
- **Content Types**: Analyze what types of content are converted most
- **Performance Impact**: Measure impact of different option combinations

#### Future Considerations:

**Feature Enhancement:**
- **Extended Syntax**: Support for additional Markdown extensions
- **Better HTML Support**: Enhanced HTML element conversion
- **Performance Improvements**: Faster processing for large documents
- **Quality Improvements**: Better preservation of complex formatting

**Integration Improvements:**
- **Real-time Preview**: Live preview capabilities for content conversion
- **Batch Processing**: Enhanced batch processing for multiple documents
- **Template Support**: Template-based conversion for consistent formatting
- **Plugin System**: Support for custom conversion plugins and extensions

---

### 37. MCP Server Trigger
**Status**: ✅ Active
**Category**: Core - Trigger/AI/MCP Integration
**Purpose**: Allow n8n to act as a Model Context Protocol (MCP) server, making n8n tools and workflows available to MCP clients

#### Authentication:
- **HTTP Request Credentials**: Uses same authentication as HTTP Request node
- **Bearer Auth**: Token-based authentication for MCP clients
- **Header Auth**: Custom header-based authentication
- **Optional**: Authentication can be disabled for open access

#### Operations:
**1. MCP Server Endpoint** - Expose n8n as MCP server for external client connections

#### Key Concepts:

**Model Context Protocol (MCP):**
- **Protocol**: Standardized protocol for connecting AI tools and clients
- **Server Role**: n8n acts as MCP server providing tools to external clients
- **Tool Integration**: Exposes n8n workflows and tools to MCP-compatible clients
- **Client Support**: Works with Claude Desktop and other MCP clients

**Unique Trigger Behavior:**
- **Tool-Only Connections**: Unlike conventional triggers, only connects to tool nodes
- **No Sequential Flow**: Doesn't pass output to next connected node
- **Tool Execution**: Clients can list and call individual tools
- **Workflow Exposure**: Use Custom n8n Workflow Tool node to expose workflows

#### Core Parameters:

**1. MCP URL Configuration**
- **Dual URLs**: Separate test and production URLs available
- **Test URL**: Active during development and testing (Listen for Test Event)
- **Production URL**: Active when workflow is activated for live use
- **URL Display**: Toggle between Test URL and Production URL in node panel

**URL Behavior:**
- **Test Mode**: Displays data in workflow interface for debugging
- **Production Mode**: Doesn't display data in workflow (check Executions tab)
- **Registration**: URLs registered automatically based on workflow state
- **Access**: MCP clients connect to these URLs to access n8n tools

**2. Authentication (Optional)**
- **Bearer Auth**: Token-based authentication using Authorization header
- **Header Auth**: Custom header authentication method
- **Credential Management**: Uses HTTP Request credential system
- **Client Authentication**: MCP clients must provide valid credentials

**3. Path Configuration**
- **Default Path**: Randomly generated to avoid conflicts
- **Custom Paths**: Manually specify URL paths for consistent endpoints
- **Route Parameters**: Support for parameterized paths
- **API Prototyping**: Useful for consistent endpoint URLs during development

**Path Format Examples:**
- `/:variable` - Single variable parameter
- `/path/:variable` - Fixed path with variable
- `/:variable/path` - Variable followed by fixed path
- `/:variable1/path/:variable2` - Multiple variables with fixed path
- `/:variable1/:variable2` - Multiple consecutive variables

#### Technical Implementation:

**Transport Support:**
- **Server-Sent Events (SSE)**: Long-lived HTTP-based transport for real-time communication
- **Streamable HTTP**: HTTP-based streaming for efficient data transfer
- **Protocol Limitation**: Currently doesn't support standard input/output (stdio) transport

**Connection Management:**
- **Persistent Connections**: SSE and streamable HTTP require same server instance
- **Session Handling**: Maintains connection state for ongoing client interactions
- **Real-time Communication**: Enables real-time tool execution and response
- **Client Compatibility**: Compatible with MCP specification requirements

#### Tool Integration:

**Custom n8n Workflow Tool:**
- **Workflow Exposure**: Attach workflows using Custom n8n Workflow Tool node
- **Tool Registration**: Workflows become available tools for MCP clients
- **Parameter Mapping**: Workflow inputs mapped to tool parameters
- **Response Handling**: Workflow outputs returned as tool responses

**Tool Discovery:**
- **Tool Listing**: Clients can discover available tools from n8n server
- **Tool Information**: Tool names, descriptions, and parameters exposed to clients
- **Dynamic Tools**: Tools available based on connected workflow tools
- **Real-time Updates**: Tool availability updates as workflows change

#### Claude Desktop Integration:

**Configuration Setup:**
```json
{
  "mcpServers": {
    "n8n": {
      "command": "npx",
      "args": [
        "mcp-remote",
        "<MCP_URL>",
        "--header",
        "Authorization: Bearer ${AUTH_TOKEN}"
      ],
      "env": {
        "AUTH_TOKEN": "<MCP_BEARER_TOKEN>"
      }
    }
  }
}
```

**Integration Process:**
- **Gateway Proxy**: Uses mcp-remote to proxy SSE messages to stdio-based servers
- **Token Configuration**: Environment variable for bearer token authentication
- **URL Configuration**: Replace placeholders with actual MCP URL and token
- **Claude Access**: Makes n8n tools available within Claude Desktop interface

#### Deployment Considerations:

**Queue Mode Limitations:**
- **Single Replica**: Works normally with single webhook replica
- **Multiple Replicas**: Requires dedicated routing for `/mcp*` requests
- **Connection Persistence**: SSE requires same server instance for connections
- **Load Balancer Config**: Route all MCP traffic to single dedicated webhook instance

**Critical Queue Mode Setup:**
- **Dedicated Replica**: Create separate replica set with one webhook container for MCP
- **Ingress Configuration**: Update load balancer to route `/mcp*` to MCP replica
- **Connection Stability**: Prevents connection breaks and delivery failures
- **Production Requirement**: Essential for reliable MCP operation with multiple replicas

#### Reverse Proxy Configuration:

**Nginx Configuration:**
```nginx
location /mcp/ {
    proxy_http_version 1.1;
    proxy_buffering off;
    gzip off;
    chunked_transfer_encoding off;
    proxy_set_header Connection '';
    # Additional proxy headers and settings
}
```

**Required Settings:**
- **Proxy Buffering**: Disabled for SSE compatibility
- **HTTP Version**: Use HTTP/1.1 for proper connection handling
- **Compression**: Disable gzip (n8n handles compression)
- **Transfer Encoding**: Disable chunked encoding
- **Connection Header**: Remove Connection header from forwarded requests

#### Use Cases:

**1. AI Tool Integration:**
- **Claude Desktop**: Integrate n8n workflows as tools in Claude Desktop
- **AI Assistants**: Provide n8n capabilities to AI assistant applications
- **Custom AI Clients**: Build custom MCP clients that use n8n tools
- **Workflow Automation**: Enable AI-driven workflow execution

**2. API Development and Prototyping:**
- **Tool Development**: Prototype AI tools using n8n workflows
- **API Mockups**: Create mock APIs for testing and development
- **Integration Testing**: Test tool integrations before full implementation
- **Rapid Prototyping**: Quickly build and test AI tool concepts

**3. Enterprise AI Integration:**
- **Internal Tools**: Expose internal workflows as tools for AI systems
- **Business Process Integration**: Connect AI assistants to business processes
- **Data Access**: Provide AI systems with controlled access to business data
- **Workflow Orchestration**: Enable AI-driven business workflow execution

**4. Development Workflows:**
- **Tool Testing**: Test AI tool functionality during development
- **Client Development**: Develop and test MCP client applications
- **Integration Validation**: Validate tool integrations with various clients
- **Debugging**: Debug tool interactions and data flow

#### Advanced Features:

**Real-time Communication:**
- **Live Tool Execution**: Execute tools in real-time with immediate responses
- **Streaming Responses**: Support for streaming tool responses
- **Connection Persistence**: Maintain persistent connections for ongoing interactions
- **Event Handling**: Handle real-time events and notifications

**Security Features:**
- **Authentication Control**: Optional authentication for secure tool access
- **Access Management**: Control which clients can access which tools
- **Token Management**: Secure token-based authentication system
- **Path Security**: Secure URL path configuration and management

**Development Support:**
- **Test Environment**: Separate test URLs for development and debugging
- **Tool Discovery**: Dynamic tool discovery and listing capabilities
- **Error Handling**: Comprehensive error handling and reporting
- **Debugging Tools**: Built-in debugging and monitoring capabilities

#### Key Benefits:

**AI Integration:**
- **Seamless Integration**: Easy integration with AI assistants and clients
- **Tool Ecosystem**: Expand AI capabilities with n8n's tool ecosystem
- **Workflow Access**: Provide AI systems with access to complex workflows
- **Business Logic**: Connect AI to business processes and data

**Development Efficiency:**
- **Rapid Development**: Quickly expose workflows as AI tools
- **Testing Support**: Built-in testing and debugging capabilities
- **Flexible Configuration**: Customizable paths and authentication
- **Protocol Compliance**: Full MCP specification compliance

**Enterprise Ready:**
- **Scalable Architecture**: Support for enterprise-scale deployments
- **Security Controls**: Comprehensive authentication and access controls
- **Production Support**: Robust production deployment capabilities
- **Integration Flexibility**: Works with various MCP clients and systems

#### Templates Available:
- **Build an MCP Server with Google Calendar and Custom Functions**: Complete MCP server implementation
- **Build your own N8N Workflows MCP Server**: Custom workflow exposure example
- **Build a Personal Assistant with Google Gemini, Gmail and Calendar using MCP**: AI assistant integration
- **Browse MCP Server Trigger integration templates**: Additional examples and use cases

#### Related Nodes:
- **Custom n8n Workflow Tool**: Expose workflows as MCP tools
- **MCP Client Tool**: Connect to external MCP servers
- **HTTP Request**: Similar authentication and URL handling
- **Webhook**: Alternative HTTP-based trigger for different use cases

#### Best Practices:

**1. Authentication Security:**
- **Use Authentication**: Always enable authentication for production deployments
- **Secure Tokens**: Use strong, unique bearer tokens for authentication
- **Token Rotation**: Implement regular token rotation procedures
- **Access Control**: Limit client access to necessary tools only

**2. Deployment Configuration:**
- **Dedicated Replicas**: Use dedicated webhook replicas for MCP in multi-replica setups
- **Reverse Proxy**: Configure reverse proxy properly for SSE support
- **Load Balancing**: Ensure proper load balancer configuration for MCP traffic
- **Connection Management**: Monitor and maintain persistent connections

**3. Tool Design:**
- **Clear Tool Names**: Use descriptive names for exposed tools
- **Proper Documentation**: Document tool parameters and expected outputs
- **Error Handling**: Implement robust error handling in tool workflows
- **Response Formatting**: Format tool responses appropriately for clients

**4. Development Workflow:**
- **Test Environment**: Use test URLs during development and testing
- **Production Transition**: Properly transition from test to production URLs
- **Monitoring**: Monitor tool usage and performance in production
- **Client Testing**: Test with various MCP clients for compatibility

#### Troubleshooting:

**Connection Issues:**
- **Reverse Proxy**: Check reverse proxy configuration for SSE support
- **Queue Mode**: Verify proper routing for multiple webhook replicas
- **Authentication**: Validate authentication credentials and configuration
- **Network**: Check network connectivity and firewall settings

**Tool Access Problems:**
- **Tool Registration**: Verify tools are properly connected and registered
- **Authentication**: Check client authentication and token validity
- **Path Configuration**: Validate URL paths and parameter configuration
- **Client Compatibility**: Ensure client MCP specification compatibility

**Performance Issues:**
- **Connection Persistence**: Monitor connection stability and persistence
- **Tool Response Time**: Optimize tool workflow performance
- **Resource Usage**: Monitor server resource usage and scaling
- **Client Load**: Manage client load and concurrent connections

#### Security Considerations:

**Access Control:**
- **Authentication Required**: Always use authentication in production
- **Token Security**: Secure storage and transmission of authentication tokens
- **Client Validation**: Validate client authenticity and authorization
- **Tool Access**: Control which tools are accessible to which clients

**Network Security:**
- **HTTPS Usage**: Always use HTTPS for production MCP endpoints
- **Reverse Proxy**: Secure reverse proxy configuration
- **Firewall Rules**: Implement appropriate firewall rules for MCP traffic
- **Monitoring**: Monitor access patterns and potential security threats

#### Performance Optimization:

**Connection Management:**
- **Persistent Connections**: Optimize persistent connection handling
- **Connection Pooling**: Implement efficient connection pooling
- **Resource Management**: Monitor and manage server resources
- **Scaling**: Plan for horizontal scaling with proper routing

**Tool Performance:**
- **Workflow Optimization**: Optimize exposed workflow performance
- **Response Caching**: Implement appropriate response caching
- **Parallel Processing**: Enable parallel tool execution when possible
- **Resource Monitoring**: Monitor tool execution resource usage

---

### 38. Merge
**Status**: ✅ Active
**Category**: Core - Data Processing/Flow Control
**Purpose**: Combine data from multiple streams once data of all streams is available

#### Version History:
- **Major Changes in 0.194.0**: Complete node overhaul by n8n team
- **Minor Changes in 1.49.0**: Added support for more than two inputs and SQL Query mode
- **Legacy Versions**: Older versions only support up to two inputs (use Code node for multiple inputs)

#### Authentication:
- **No credentials required**: Direct data processing operation
- **Multi-input support**: Can handle multiple data streams (v1.49.0+)
- **Synchronization**: Waits for all connected inputs before processing

#### Operations:
**1. Data Stream Combination** - Combine data from multiple streams using various merge strategies

#### Core Parameters:

**1. Mode** - Choose how the Merge node should combine data from different streams:

**Available Modes:**
- **Append**: Keep data from all inputs, output items one after another
- **Combine**: Combine data from inputs using various strategies
- **SQL Query**: Write custom SQL queries to merge data (v1.49.0+)
- **Choose Branch**: Select which input to keep without modification

#### Mode Details:

**1. Append Mode**
- **Purpose**: Keep data from all inputs and output items of each input sequentially
- **Configuration**: Choose **Number of Inputs** to specify how many inputs to combine
- **Behavior**: Node waits for execution of all connected inputs before outputting
- **Output Order**: Items from Input 1, then Input 2, then Input 3, etc.
- **Use Case**: Simple concatenation of datasets

**Data Flow Example:**
```
Input 1: [A, B, C]
Input 2: [D, E, F]
Output:  [A, B, C, D, E, F]
```

**2. Combine Mode**
- **Purpose**: Combine data from two inputs using sophisticated merge strategies
- **Configuration**: Select option in **Combine By** to determine merge method
- **Advanced Options**: Multiple configuration options for fine-tuned merging

**Combine By Options:**

**a) Matching Fields**
- **Purpose**: Compare items by field values and merge based on matches
- **Configuration**: Enter fields to compare in **Fields to Match**
- **Field Specification**: Use dot notation for nested fields (e.g., `user.id`)

**Output Type Options:**
- **Keep Matches**: Merge items that match (inner join behavior)
- **Keep Non-Matches**: Merge items that don't match
- **Keep Everything**: Merge matching items and include non-matching items (outer join)
- **Enrich Input 1**: Keep all Input 1 data, add matching Input 2 data (left join)
- **Enrich Input 2**: Keep all Input 2 data, add matching Input 1 data (right join)

**b) Position**
- **Purpose**: Combine items based on their order/index position
- **Behavior**: Item at index 0 in Input 1 merges with item at index 0 in Input 2
- **Limitation**: Limited by smaller input size
- **Option**: **Include Any Unpaired Items** to keep items without matches

**c) All Possible Combinations**
- **Purpose**: Output all possible item combinations while merging fields
- **Behavior**: Creates cartesian product of inputs
- **Field Handling**: Merges fields with same names, preserves unique fields
- **Use Case**: Generate all combinations for analysis or testing

**3. SQL Query Mode (v1.49.0+)**
- **Purpose**: Write custom SQL queries to merge data with full control
- **SQL Engine**: Uses AlaSQL for SQL processing
- **Table References**: Input data available as `input1`, `input2`, `input3`, etc.
- **Flexibility**: Full SQL syntax support for complex merging logic

**Example SQL Query:**
```sql
SELECT * FROM input1 
LEFT JOIN input2 ON input1.name = input2.id
```

**Supported Features:**
- Standard SQL SELECT statements
- JOIN operations (INNER, LEFT, RIGHT, OUTER)
- WHERE clauses for filtering
- ORDER BY for sorting
- GROUP BY for aggregation
- Full AlaSQL syntax support

**4. Choose Branch Mode**
- **Purpose**: Select which input to keep without any data modification
- **Synchronization**: Always waits for data from all inputs before choosing
- **Output Options**: Choose to output Input 1 Data, Input 2 Data, or Single Empty Item
- **Use Case**: Conditional data routing based on execution results

#### Advanced Options (Combine Mode):

**1. Clash Handling**
- **Purpose**: Control behavior when multiple items have fields with same names
- **Default**: Input 2 takes precedence over Input 1

**When Field Values Clash:**
- **Prefer Input 1**: Use Input 1 values when fields clash
- **Prefer Input 2**: Use Input 2 values when fields clash (default)
- **Always Add Input Number to Field Names**: Keep all fields with input number suffix

**Merging Nested Fields:**
- **Deep Merge**: Merge properties at all levels including nested objects
- **Shallow Merge**: Merge only top-level properties without nested object merging

**2. Fuzzy Compare**
- **Purpose**: Control type strictness when comparing field values
- **Enabled**: Tolerates type differences (treats `"3"` and `3` as same)
- **Disabled**: Strict type matching required (default)

**3. Disable Dot Notation**
- **Purpose**: Control access to nested fields using `parent.child` syntax
- **Enabled**: Prevents dot notation access to child fields
- **Disabled**: Allows dot notation for nested field access (default)

**4. Multiple Matches**
- **Purpose**: Handle scenarios with multiple matching items
- **Include All Matches**: Output multiple items for each match
- **Include First Match Only**: Keep first match, discard subsequent matches

**5. Include Any Unpaired Items (Position mode)**
- **Purpose**: Control whether to keep items without position matches
- **Enabled**: Keep unpaired items in output
- **Disabled**: Only output paired items (default)

#### Key Features:

**Multi-Input Support (v1.49.0+):**
- **Flexible Input Count**: Support for more than two inputs in Append mode
- **Version Requirement**: Requires n8n v1.49.0 or later
- **Legacy Compatibility**: Older versions limited to two inputs

**Data Synchronization:**
- **Wait Behavior**: Node waits for all connected inputs to complete
- **Complete Processing**: Ensures all data streams are available before merging
- **Error Propagation**: Errors in any input affect entire merge operation

**SQL Integration:**
- **AlaSQL Engine**: Full-featured SQL processing engine
- **Standard SQL**: Support for complex SQL operations and syntax
- **Performance**: Efficient processing for complex merge operations
- **Flexibility**: Unlimited merge complexity with SQL queries

**Smart Merging:**
- **Type Handling**: Intelligent handling of data types and conversions
- **Nested Data**: Support for complex nested object structures
- **Field Preservation**: Maintains field relationships and data integrity
- **Conflict Resolution**: Comprehensive conflict resolution strategies

#### Use Cases:

**1. Data Enrichment:**
- **User Profile Enhancement**: Merge user data with preference data
- **Product Information**: Combine product details with pricing and availability
- **Customer 360**: Merge customer data from multiple sources
- **Reference Data**: Add lookup data to primary datasets

**2. Join Operations:**
- **Database-Style Joins**: Implement SQL-style join operations
- **Master Data Management**: Combine master data with transactional data
- **Data Warehouse Operations**: ETL-style data combination
- **Reference Table Lookups**: Add reference information to datasets

**3. Data Consolidation:**
- **Report Generation**: Combine data from multiple sources for reporting
- **Dashboard Data**: Merge datasets for dashboard display
- **Analytics Preparation**: Prepare data for analysis tools
- **Export Formatting**: Combine data for export operations

**4. Workflow Orchestration:**
- **Parallel Processing**: Combine results from parallel workflow branches
- **Multi-Source Integration**: Integrate data from multiple APIs or services
- **Batch Processing**: Combine results from batch operations
- **Error Handling**: Merge error data with successful processing results

#### Common Integration Patterns:

**1. Data Enrichment Pipeline:**
```
API Call 1 → Transform → 
                         Merge (Matching Fields) → Final Output
API Call 2 → Transform → 
```
- Enrich primary data with secondary data source
- Match on common fields like ID or email
- Combine complementary information

**2. Parallel Processing:**
```
Split Data → Process A → 
                        Merge (Append) → Continue Processing
           → Process B → 
```
- Process different aspects of data in parallel
- Combine results for downstream processing
- Optimize workflow performance

**3. Master-Detail Combination:**
```
Master Data → 
              Merge (SQL Query) → Report Generation
Detail Data → 
```
- Use SQL JOIN to combine master and detail records
- Generate comprehensive reports
- Maintain data relationships

**4. Conditional Data Routing:**
```
Primary Path → 
              Merge (Choose Branch) → Output
Fallback Path → 
```
- Execute multiple paths conditionally
- Choose optimal result based on execution success
- Implement fallback strategies

#### Special Considerations:

**Uneven Data Streams:**
- **Precedence Rule**: Input 1 items take precedence for count determination
- **Processing Limit**: Limited by smallest input size in position merging
- **Example**: 5 items in Input 1 + 10 items in Input 2 = 5 processed items
- **Mitigation**: Use "Include Any Unpaired Items" option when needed

**Legacy Execution Behavior (v0.236.0 and below):**
- **If + Merge Interaction**: Both If node outputs may execute with Merge nodes
- **Execution Order**: Affects workflows using v0 (legacy) execution order
- **Modern Behavior**: v1 execution order prevents this behavior
- **Migration**: Update workflow execution order in settings

**Version Compatibility:**
- **SQL Query Mode**: Only available in v1.49.0+
- **Multi-Input Support**: Requires v1.49.0+ for more than two inputs
- **Legacy Support**: Older versions require Code node for complex merging

#### Performance Considerations:

**Memory Usage:**
- **Data Accumulation**: Node stores all input data until processing
- **Large Datasets**: Consider memory impact with large input streams
- **Optimization**: Use appropriate merge modes for data size
- **Monitoring**: Monitor memory usage with complex merge operations

**Processing Efficiency:**
- **Mode Selection**: Choose appropriate mode for use case requirements
- **SQL Optimization**: Optimize SQL queries for performance
- **Field Selection**: Limit fields in matching operations for efficiency
- **Index Strategy**: Consider data ordering for position-based merging

#### Advanced Features:

**Expression Support:**
- **Dynamic Fields**: Use expressions in field matching specifications
- **Conditional Logic**: Implement conditional merge logic
- **Calculated Fields**: Add calculated fields during merge process
- **Context Data**: Access workflow context during merge operations

**Error Handling:**
- **Input Validation**: Validate input data structure and content
- **Type Conversion**: Handle type mismatches gracefully
- **Missing Fields**: Manage missing or null field values
- **Conflict Resolution**: Resolve data conflicts systematically

**Debugging Support:**
- **Step-by-Step Examples**: Comprehensive examples for each merge mode
- **Visual Diagrams**: Clear visual representation of merge operations
- **Sample Data**: Provided sample data for testing and learning
- **Output Inspection**: Detailed output analysis for troubleshooting

#### Templates Available:
- **Scrape and summarize webpages with AI**: Combine scraped data with AI analysis
- **Automate Multi-Platform Social Media Content Creation with AI**: Merge content across platforms
- **Telegram AI Chatbot**: Combine user input with AI responses
- **Merge greetings with users based on language**: Step-by-step tutorial example

#### Related Nodes:
- **Compare Datasets**: Alternative for comparing rather than merging datasets
- **If**: Conditional logic that can work with Merge for branching
- **Code**: Alternative for complex merge logic using JavaScript/Python
- **Split in Batches**: Opposite operation for splitting rather than merging
- **Aggregate**: Alternative for data aggregation and grouping

#### Best Practices:

**1. Mode Selection:**
- **Simple Concatenation**: Use Append mode for straightforward data combination
- **Field-Based Joining**: Use Combine with Matching Fields for database-style operations
- **Complex Logic**: Use SQL Query mode for sophisticated merge requirements
- **Performance Optimization**: Choose most efficient mode for data characteristics

**2. Data Preparation:**
- **Field Standardization**: Ensure consistent field names across inputs
- **Type Consistency**: Maintain consistent data types for matching fields
- **Data Cleaning**: Clean data before merging to prevent conflicts
- **Schema Validation**: Validate data schemas for compatibility

**3. Error Prevention:**
- **Input Validation**: Validate all input data before merge operations
- **Field Verification**: Verify field existence and types
- **Null Handling**: Handle null and undefined values appropriately
- **Conflict Planning**: Plan for field name conflicts and data clashes

**4. Performance Optimization:**
- **Memory Management**: Monitor memory usage with large datasets
- **Query Optimization**: Optimize SQL queries for performance
- **Field Limitation**: Limit fields to essential data only
- **Batch Processing**: Consider batch processing for very large datasets

#### Troubleshooting:

**Common Issues:**
- **Memory Overload**: Large datasets causing memory issues
- **Field Conflicts**: Unexpected field value conflicts during merging
- **Type Mismatches**: Data type incompatibilities in merge operations
- **Missing Data**: Expected fields missing from input streams

**Resolution Strategies:**
- **Data Size Management**: Break large datasets into smaller chunks
- **Conflict Resolution**: Use appropriate clash handling options
- **Type Standardization**: Standardize data types before merging
- **Field Validation**: Validate field existence and structure

#### Security Considerations:

**Data Privacy:**
- **Sensitive Information**: Handle sensitive data appropriately during merging
- **Access Control**: Ensure proper access controls for merged data
- **Data Retention**: Consider data retention policies for merged results
- **Audit Logging**: Log merge operations for compliance and auditing

**SQL Injection (SQL Query mode):**
- **Input Validation**: Validate all inputs used in SQL queries
- **Parameterization**: Use safe query construction practices
- **Query Review**: Review SQL queries for potential security issues
- **Access Restrictions**: Limit SQL query capabilities appropriately

#### Migration Guidance:

**Version Updates:**
- **Pre-0.194.0**: Refer to legacy documentation for older versions
- **0.194.0+**: Major changes in node behavior and options
- **1.49.0+**: New features like SQL Query mode and multi-input support
- **Testing**: Thoroughly test workflows after version updates

**Legacy Compatibility:**
- **Two-Input Limitation**: Use Code node for multiple inputs in older versions
- **Execution Order**: Update from v0 to v1 execution order for modern behavior
- **Feature Availability**: Check feature availability for your n8n version

#### Advanced Use Cases:

**1. Complex Data Warehousing:**
- **Star Schema Joins**: Implement complex database join patterns
- **Dimension Tables**: Merge fact tables with dimension tables
- **Data Mart Creation**: Create specialized data marts from multiple sources
- **ETL Operations**: Implement extract, transform, load operations

**2. Real-time Data Integration:**
- **Stream Merging**: Combine real-time data streams
- **Event Correlation**: Correlate events from multiple sources
- **Live Dashboards**: Merge data for real-time dashboard updates
- **Monitoring Systems**: Combine monitoring data from multiple systems

**3. Business Intelligence:**
- **Report Generation**: Combine data from multiple business systems
- **KPI Calculation**: Merge data for key performance indicator calculation
- **Trend Analysis**: Combine historical and current data for analysis
- **Executive Dashboards**: Merge high-level metrics from various sources

**4. Machine Learning Preparation:**
- **Feature Engineering**: Combine features from multiple data sources
- **Training Data**: Merge training datasets from different sources
- **Model Validation**: Combine prediction results with actual outcomes
- **Data Pipeline**: Create ML data pipelines with multiple data sources

---

### 39. n8n
**Status**: ✅ Active
**Category**: Core - n8n API/Workflow Management
**Purpose**: A node to integrate with n8n itself. This node allows you to consume the n8n API in your workflows.

#### Authentication:
- **API Authentication**: Uses n8n API authentication credentials
- **Credential Setup**: [API Authentication Documentation](../../../../api/authentication/)
- **SSL Limitation**: Does not support SSL - use HTTP Request node for SSL connections
- **API Reference**: [n8n REST API Documentation](../../../../api/) and [API Endpoint Reference](../../../../api/api-reference/)

#### Operations (15 total):

**1. Audit Operations**
- **Generate**: Generate a security audit

**2. Credential Operations**
- **Create**: Create a credential
- **Delete**: Delete a credential
- **Get Schema**: Get credential data schema for type

**3. Execution Operations**
- **Get**: Get an execution
- **Get Many**: Get Many executions
- **Delete**: Delete an execution

**4. Workflow Operations**
- **Activate**: Activate a workflow
- **Create**: Create a workflow
- **Deactivate**: Deactivate a workflow
- **Delete**: Delete a workflow
- **Get**: Get a workflow
- **Get Many**: Get Many workflows
- **Update**: Update a workflow

#### Operation Details:

**1. Generate Audit**
- **Purpose**: Generate security audit reports for n8n instance
- **No Parameters Required**: Simple operation with configuration options only

**Configuration Options:**
- **Categories**: Select risk categories for audit inclusion:
  - **Credentials**: Credential security analysis
  - **Database**: Database security review
  - **Filesystem**: File system security assessment
  - **Instance**: Instance configuration security
  - **Nodes**: Node security evaluation
- **Days Abandoned Workflow**: Set threshold for abandoned workflow detection (default: 90 days)

**2. Create Credential**
- **Purpose**: Programmatically create new credentials in n8n
- **Use Case**: Automated credential management and setup

**Parameters:**
- **Name**: Name of the credential to create
- **Credential Type**: Type of credential (depends on installed nodes)
  - **Built-in Types**: `githubApi`, `notionApi`, `slackApi`, etc.
  - **Dynamic Types**: Available types depend on nodes installed on n8n instance
- **Data**: Valid JSON object with required properties for the credential type
  - **Schema Reference**: Use **Get Schema** operation to see expected format

**3. Delete Credential**
- **Purpose**: Remove credentials from n8n instance
- **Use Case**: Cleanup, security management, automated maintenance

**Parameters:**
- **Credential ID**: ID of the credential to delete

**4. Get Credential Schema**
- **Purpose**: Retrieve data schema for specific credential types
- **Use Case**: Understand required fields before creating credentials

**Parameters:**
- **Credential Type**: Type of credential to get schema for
  - **Available Types**: Depends on nodes installed on n8n instance
  - **Examples**: `githubApi`, `notionApi`, `slackApi`

**5. Get Execution**
- **Purpose**: Retrieve details of specific workflow execution
- **Use Case**: Execution monitoring, debugging, audit trails

**Parameters:**
- **Execution ID**: ID of the execution to retrieve

**Options:**
- **Include Execution Details**: Control whether to include detailed execution data
  - **On**: Include complete execution details
  - **Off**: Return basic execution information only

**6. Get Many Executions**
- **Purpose**: Retrieve multiple executions with filtering and pagination
- **Use Case**: Execution analysis, reporting, monitoring

**Parameters:**
- **Return All**: Return all results vs limit to specific count
- **Limit**: Number of results to return (when Return All is off)

**Filters:**
- **Workflow**: Filter executions by specific workflow
  - **From list**: Select workflow from dropdown
  - **By URL**: Enter workflow URL
  - **By ID**: Enter workflow ID
- **Status**: Filter by execution status
  - **Error**: Failed executions
  - **Success**: Successful executions
  - **Waiting**: Pending executions

**Options:**
- **Include Execution Details**: Control level of detail in results

**7. Delete Execution**
- **Purpose**: Remove execution records from n8n
- **Use Case**: Cleanup, storage management, compliance

**Parameters:**
- **Execution ID**: ID of the execution to delete

**8. Workflow Operations (Activate, Deactivate, Delete, Get)**
- **Purpose**: Manage workflow lifecycle and state
- **Use Case**: Workflow automation, deployment, management

**Common Workflow Parameter:**
- **Workflow**: Select target workflow using one of:
  - **From list**: Select from available workflows
  - **By URL**: Enter workflow URL
  - **By ID**: Enter workflow ID

**9. Create Workflow**
- **Purpose**: Programmatically create new workflows
- **Use Case**: Workflow deployment, automation, template instantiation

**Parameters:**
- **Workflow Object**: Valid JSON object with workflow definition
  - **Required Fields**:
    - `name`: Workflow name
    - `nodes`: Workflow nodes configuration
    - `connections`: Node connections definition
    - `settings`: Workflow settings
  - **API Reference**: [Create Workflow API](../../../../api/api-reference/#tag/Workflow/paths/~1workflows/post)

**10. Get Many Workflows**
- **Purpose**: Retrieve multiple workflows with filtering
- **Use Case**: Workflow discovery, management, reporting

**Parameters:**
- **Return All**: Return all workflows vs limited results
- **Limit**: Number of workflows to return

**Filters:**
- **Return Only Active Workflows**: Filter by activation status
  - **On**: Only active workflows
  - **Off**: Both active and inactive workflows
- **Tags**: Comma-separated list of required tags

**11. Update Workflow**
- **Purpose**: Modify existing workflow configuration
- **Use Case**: Workflow maintenance, version updates, configuration changes

**Parameters:**
- **Workflow**: Target workflow selection (From list/By URL/By ID)
- **Workflow Object**: Updated workflow definition JSON
  - **Required Fields**: `name`, `nodes`, `connections`, `settings`
  - **API Reference**: [Update Workflow API](https://docs.n8n.io/api/api-reference/#tag/Workflow/paths/~1workflows~1%7Bid%7D/put)

#### Key Features:

**Self-Integration:**
- **API Access**: Full access to n8n's REST API within workflows
- **Management Operations**: Complete workflow and execution management
- **Administrative Functions**: Audit, credential, and instance management
- **Automation Capabilities**: Automate n8n administration tasks

**Comprehensive Coverage:**
- **Workflow Lifecycle**: Create, read, update, delete, activate/deactivate workflows
- **Execution Management**: Monitor, retrieve, and manage workflow executions
- **Credential Management**: Create, delete, and manage authentication credentials
- **Security Auditing**: Generate security audits and compliance reports

**Integration Flexibility:**
- **Programmatic Control**: Full programmatic control over n8n instance
- **Workflow Automation**: Automate workflow deployment and management
- **Monitoring Integration**: Build custom monitoring and alerting systems
- **Administrative Automation**: Automate administrative and maintenance tasks

#### Use Cases:

**1. Workflow Management Automation:**
- **Deployment Pipelines**: Automate workflow deployment from development to production
- **Version Control**: Manage workflow versions and updates programmatically
- **Backup and Restore**: Create automated backup systems for workflows
- **Environment Synchronization**: Sync workflows between different n8n instances

**2. Monitoring and Analytics:**
- **Execution Monitoring**: Monitor workflow execution status and performance
- **Error Analysis**: Analyze failed executions and identify patterns
- **Performance Metrics**: Collect performance data for optimization
- **Alerting Systems**: Create custom alerting based on execution status

**3. Administrative Automation:**
- **Credential Management**: Automate credential lifecycle management
- **Security Auditing**: Regular security audits and compliance reporting
- **Instance Maintenance**: Automated cleanup and maintenance tasks
- **User Management**: Workflow-based user and access management

**4. DevOps Integration:**
- **CI/CD Pipelines**: Integrate n8n workflow deployment in CI/CD
- **Infrastructure as Code**: Manage n8n configuration as code
- **Environment Management**: Automated environment setup and configuration
- **Release Management**: Automate workflow release processes

**5. Business Process Automation:**
- **Workflow Orchestration**: Coordinate complex business process workflows
- **Process Monitoring**: Monitor business process execution and compliance
- **Dynamic Workflow Creation**: Create workflows based on business requirements
- **Compliance Reporting**: Generate compliance reports from workflow executions

#### Common Integration Patterns:

**1. Workflow Deployment Pipeline:**
```
Git Webhook → Extract Workflow → n8n (Create/Update Workflow) → n8n (Activate) → Notification
```
- Automate workflow deployment from version control
- Update existing workflows or create new ones
- Activate workflows for production use

**2. Execution Monitoring:**
```
Schedule Trigger → n8n (Get Many Executions) → Filter → Send Alert
```
- Regular monitoring of workflow executions
- Filter for failed or problematic executions
- Send alerts for issues requiring attention

**3. Backup and Restore:**
```
Schedule Trigger → n8n (Get Many Workflows) → Convert to File → Cloud Storage
```
- Regular backup of all workflows
- Store workflow definitions in external storage
- Enable disaster recovery capabilities

**4. Security Auditing:**
```
Schedule Trigger → n8n (Generate Audit) → Process Results → Send Report
```
- Regular security audit generation
- Process audit results for compliance
- Generate and distribute security reports

#### Advanced Features:

**Workflow JSON Structure:**
- **Nodes Array**: Complete node definitions with parameters
- **Connections Object**: Node connection mappings and data flow
- **Settings Object**: Workflow configuration and metadata
- **Validation**: API validates workflow structure and integrity

**Execution Details:**
- **Execution Data**: Complete execution trace and node outputs
- **Performance Metrics**: Execution time and resource usage
- **Error Information**: Detailed error messages and stack traces
- **Execution Context**: Workflow state and runtime information

**Filter and Search:**
- **Advanced Filtering**: Complex filters for executions and workflows
- **Tag-Based Organization**: Use tags for workflow categorization
- **Status-Based Queries**: Filter by execution status and outcomes
- **Date Range Filtering**: Time-based execution and workflow queries

#### Security Considerations:

**API Authentication:**
- **Credential Security**: Secure storage of n8n API credentials
- **Access Control**: Ensure appropriate API access permissions
- **Token Management**: Regular rotation of API authentication tokens
- **Audit Logging**: Log all API operations for security monitoring

**SSL/TLS Limitations:**
- **No SSL Support**: n8n node does not support SSL connections
- **Alternative Solution**: Use HTTP Request node for SSL-required connections
- **Certificate Handling**: HTTP Request node supports SSL certificate provision
- **Security Best Practices**: Always use HTTPS for API communications

**Data Privacy:**
- **Execution Data**: Handle execution data according to privacy requirements
- **Credential Information**: Secure handling of credential data
- **Audit Information**: Proper handling of security audit results
- **Access Logging**: Maintain audit trails for compliance

#### Performance Considerations:

**API Efficiency:**
- **Pagination**: Use pagination for large result sets
- **Filtering**: Apply filters to reduce data transfer
- **Detail Control**: Use Include Execution Details option appropriately
- **Batch Operations**: Group multiple operations when possible

**Resource Management:**
- **Memory Usage**: Monitor memory usage with large datasets
- **Network Traffic**: Optimize API calls to reduce network overhead
- **Execution Impact**: Consider impact of API calls on n8n performance
- **Rate Limiting**: Respect API rate limits and quotas

#### Integration Examples:

**1. DevOps Workflow Management:**
- Integrate with GitLab/GitHub for workflow version control
- Automate workflow testing and deployment
- Monitor production workflow health
- Implement automated rollback procedures

**2. Business Intelligence:**
- Extract execution data for business analytics
- Create custom dashboards for workflow performance
- Generate compliance and audit reports
- Monitor business process KPIs

**3. Multi-Environment Management:**
- Sync workflows between development, staging, and production
- Automate environment-specific configuration
- Implement workflow promotion pipelines
- Manage environment-specific credentials

**4. Disaster Recovery:**
- Automated backup of workflows and configurations
- Restore procedures for system recovery
- Cross-instance replication of critical workflows
- Configuration drift detection and correction

#### Templates Available:
- **Very quick quickstart**: Basic n8n API usage examples
- **AI agent that can scrape webpages**: Advanced workflow automation
- **Pulling data from services that n8n doesn't have a pre-built integration for**: Custom integration patterns
- **Browse n8n integration templates**: Additional workflow examples

#### Related Resources:
- **n8n REST API Documentation**: [Complete API Documentation](../../../../api/)
- **API Authentication**: [Authentication Setup Guide](../../../../api/authentication/)
- **API Reference**: [Endpoint Reference Documentation](../../../../api/api-reference/)
- **Workflow API**: [Workflow Management API](../../../../api/api-reference/#tag/Workflow)

#### Related Nodes:
- **HTTP Request**: Alternative for SSL connections and custom API calls
- **Execute Sub-workflow**: Alternative for workflow orchestration
- **n8n Form**: Create interactive forms
- **n8n Trigger**: Workflow-based triggering
- **Webhook**: External workflow triggering

#### Best Practices:

**1. API Usage:**
- **Authentication Security**: Use secure API authentication methods
- **Error Handling**: Implement comprehensive error handling for API calls
- **Rate Limiting**: Respect API rate limits and implement backoff strategies
- **Data Validation**: Validate all data before API operations

**2. Workflow Management:**
- **Version Control**: Maintain workflow definitions in version control
- **Testing**: Test workflows before production deployment
- **Documentation**: Document workflow purposes and dependencies
- **Backup**: Regular backup of critical workflows

**3. Monitoring:**
- **Regular Audits**: Perform regular security and operational audits
- **Execution Monitoring**: Monitor workflow execution health
- **Performance Tracking**: Track workflow performance metrics
- **Alert Systems**: Implement appropriate alerting for issues

**4. Security:**
- **Access Control**: Implement proper access controls for API usage
- **Credential Management**: Secure management of all credentials
- **Audit Trails**: Maintain comprehensive audit trails
- **Compliance**: Ensure compliance with security requirements

#### Troubleshooting:

**Common Issues:**
- **SSL Connection Problems**: Use HTTP Request node for SSL requirements
- **Authentication Failures**: Verify API credentials and permissions
- **JSON Format Errors**: Validate workflow JSON structure
- **API Rate Limits**: Implement proper rate limit handling

**Resolution Strategies:**
- **Credential Validation**: Test API credentials independently
- **JSON Validation**: Use API schema validation for workflow objects
- **Error Analysis**: Analyze API error responses for troubleshooting
- **Performance Monitoring**: Monitor API call performance and success rates

#### Security Best Practices:

**API Security:**
- **Secure Credentials**: Store API credentials securely
- **Access Logging**: Log all API access for security monitoring
- **Permission Management**: Use minimal required API permissions
- **Regular Audits**: Perform regular security audits using the audit feature

**Data Protection:**
- **Sensitive Data**: Handle sensitive workflow and execution data appropriately
- **Encryption**: Use encryption for data in transit and at rest
- **Access Control**: Implement proper access controls for workflow data
- **Compliance**: Ensure compliance with data protection regulations

#### Advanced Use Cases:

**1. Multi-Tenant Management:**
- **Instance Management**: Manage multiple n8n instances
- **Workflow Distribution**: Distribute workflows across instances
- **Centralized Monitoring**: Centralized monitoring across instances
- **Configuration Management**: Centralized configuration management

**2. Enterprise Integration:**
- **LDAP Integration**: Integrate with enterprise directory services
- **SSO Implementation**: Implement single sign-on workflows
- **Compliance Automation**: Automate compliance reporting and auditing
- **Change Management**: Implement change management processes

**3. Advanced Analytics:**
- **Performance Analytics**: Advanced workflow performance analysis
- **Business Intelligence**: Extract business insights from workflow data
- **Predictive Analytics**: Predict workflow failures and performance issues
- **Custom Metrics**: Create custom performance and business metrics

**4. Automation Platform:**
- **Self-Healing**: Implement self-healing workflow capabilities
- **Auto-Scaling**: Automatic scaling of workflow resources
- **Load Balancing**: Distribute workflow execution across instances
- **Optimization**: Automatic workflow optimization based on performance data

---

### 40. n8n Form
**Status**: ✅ Active
**Category**: Core - Form Building/User Interface
**Purpose**: Create user-facing forms with multiple steps. Add other nodes with custom logic between to process user input. Must start workflow with n8n Form Trigger.

#### Authentication:
- **No credentials required**: Forms are accessed through unique URLs
- **Form Access**: Controlled by Form Trigger settings
- **User Authentication**: Optional authentication configurable in Form Trigger

#### Operations:
**1. Multi-Step Form Creation** - Build interactive forms with various field types and custom logic between steps

#### Core Parameters:

**1. Page Type** - Choose the type of form page:
- **Form Elements**: Standard form page with input fields
- **Form Ending**: Final page to complete form submission

**2. Define Form** - Method to create form fields:
- **Using Fields Below**: Visual interface to add and configure form fields
- **Using JSON**: Define fields programmatically with JSON array

#### Form Field Types (Using Fields Below):

**1. Text** (Default)
- **Field Label**: Display label for the field
- **Placeholder**: Placeholder text for input
- **Required Field**: Make field mandatory

**2. Email**
- **Field Label**: Display label for email field
- **Placeholder**: Placeholder text
- **Required Field**: Make field mandatory
- **Validation**: Automatic email format validation

**3. Number**
- **Field Label**: Display label for number field
- **Placeholder**: Placeholder number
- **Required Field**: Make field mandatory
- **Type**: Numeric input only

**4. Password**
- **Field Label**: Display label for password field
- **Placeholder**: Placeholder text
- **Required Field**: Make field mandatory
- **Display**: Masked input for security

**5. Date**
- **Field Label**: Display label for date field
- **Format Date**: Date format pattern (e.g., mm/dd/yyyy)
- **Required Field**: Make field mandatory
- **Date Picker**: Interactive calendar selection

**6. Textarea**
- **Field Label**: Display label for textarea
- **Placeholder**: Placeholder text
- **Required Field**: Make field mandatory
- **Multi-line**: Extended text input

**7. Dropdown**
- **Field Label**: Display label for dropdown
- **Field Options**: List of dropdown options
- **Multiselect**: Enable multiple selections
- **Required Field**: Make field mandatory

**8. File**
- **Field Label**: Display label for file upload
- **Multiple Files**: Allow multiple file selection
- **Accept File Types**: Comma-separated list of extensions (e.g., .jpg, .png)
- **Required Field**: Make field mandatory

**9. Custom HTML**
- **HTML**: Custom HTML content to display
- **Element Name**: Optional field name for output data
- **Read-only**: Display content only, no user input
- **Restrictions**: No `<script>`, `<style>`, or `<input>` elements

**10. Hidden Field**
- **Field Name**: Name of hidden field
- **Field Value**: Default value for hidden field
- **Purpose**: Pass data without user interaction
- **Query Parameters**: Can be set via URL parameters

#### JSON Field Definition:

**Required Keys:**
- **fieldLabel**: The label that appears above the input field
- **fieldType**: Type of field (text, email, number, password, date, textarea, dropdown, file)

**Optional Keys:**
- **placeholder**: Placeholder text (not for dropdown, date, file)
- **requiredField**: Make field mandatory
- **fieldOptions**: For dropdown fields - contains values array
- **multiselect**: For dropdown fields - enable multiple selection
- **multipleFiles**: For file fields - allow multiple files
- **acceptFileTypes**: For file fields - allowed extensions
- **formatDate**: For date fields - date format pattern

**Example JSON:**
```json
[
  {
    "fieldLabel": "Email Address",
    "fieldType": "email",
    "placeholder": "user@example.com",
    "requiredField": true
  },
  {
    "fieldLabel": "Select Options",
    "fieldType": "dropdown",
    "fieldOptions": {
      "values": [
        {"option": "Option 1"},
        {"option": "Option 2"}
      ]
    },
    "multiselect": true
  }
]
```

#### Form Ending Options:

**1. Show Completion Screen**
- **Completion Title**: Main heading (h1) on completion page
- **Completion Message**: Subtitle text (supports `\n` or `<br>` for line breaks)
- **Completion Page Title**: Browser tab title

**2. Redirect to URL**
- **URL**: Target URL for redirect after form submission
- **Automatic**: Immediate redirect upon completion

**3. Show Text**
- **Text**: Custom HTML or plain text content to display
- **Full Control**: Complete control over final page content

#### Node Options:

**1. Form Title**
- **Display**: Shows as webpage title and main h1 heading
- **Optional**: Not required but recommended

**2. Form Description**
- **Display**: Subtitle below main title
- **HTML Support**: Can include HTML formatting
- **Line Breaks**: Use `\n` or `<br>`
- **Meta Description**: Also populates HTML meta description

**3. Button Label**
- **Submit Button**: Custom text for form submission button
- **Default**: Standard submit text if not specified

#### Key Features:

**Query Parameter Support:**
- **Default Values**: Set initial field values via URL parameters
- **Percent Encoding**: Special characters must be encoded (@ → %40, space → %20)
- **Production Only**: Query parameters only work in production mode
- **All Pages**: Parameters available on all form pages

**Example URL with Parameters:**
```
https://my-account.n8n.cloud/form/my-form?email=jane.doe%40example.com&name=Jane%20Doe
```

**Branch Handling:**

**Mutually Exclusive Branches:**
- **Single Path**: Only one branch executes based on conditions
- **Normal Flow**: Each form page displays sequentially
- **Single Ending**: One Form Ending node displays

**Multiple Branch Execution:**
- **Sequential Processing**: Branches execute one after another
- **Form Display**: All form pages in executed branches display
- **Single Ending**: Only the final branch's Form Ending displays
- **Important**: Plan form flow carefully when multiple branches possible

#### Use Cases:

**1. Data Collection Forms:**
- **Contact Forms**: Collect user inquiries and feedback
- **Registration Forms**: User sign-up and onboarding
- **Survey Forms**: Multi-step questionnaires
- **Application Forms**: Job applications, service requests

**2. Interactive Workflows:**
- **Conditional Logic**: Show different fields based on previous answers
- **Data Validation**: Process and validate between form steps
- **Progressive Disclosure**: Reveal fields based on user selections
- **Multi-Step Processes**: Break complex forms into manageable steps

**3. File Upload Workflows:**
- **Document Collection**: Gather documents from users
- **Image Uploads**: Collect photos or graphics
- **Multi-File Support**: Handle multiple file submissions
- **Type Restrictions**: Control accepted file formats

**4. Integration Forms:**
- **API Data Collection**: Gather data for API submissions
- **CRM Integration**: Collect leads and customer data
- **Support Tickets**: Create structured support requests
- **Order Forms**: Collect order information and preferences

#### Common Integration Patterns:

**1. Simple Contact Form:**
```
Form Trigger → Form (Contact Info) → Send Email → Form Ending (Success)
```

**2. Conditional Survey:**
```
Form Trigger → Form (Initial) → Switch → Form (Path A) / Form (Path B) → Form Ending
```

**3. File Processing:**
```
Form Trigger → Form (Upload) → Extract From File → Process → Form Ending
```

**4. Multi-Step Registration:**
```
Form Trigger → Form (Basic Info) → Validate → Form (Details) → Create User → Form Ending
```

#### Best Practices:

**1. Form Design:**
- **Clear Labels**: Use descriptive, user-friendly field labels
- **Logical Flow**: Organize fields in logical sequence
- **Required Fields**: Mark only essential fields as required
- **Help Text**: Use placeholders and descriptions effectively

**2. User Experience:**
- **Progress Indication**: Show users their progress in multi-step forms
- **Error Handling**: Provide clear error messages
- **Mobile Friendly**: Test forms on mobile devices
- **Loading States**: Consider form submission feedback

**3. Data Handling:**
- **Validation**: Validate data between form steps
- **Security**: Don't expose sensitive data in URLs
- **Hidden Fields**: Use for tracking and metadata
- **File Limits**: Set appropriate file size and type restrictions

**4. Testing:**
- **Test Mode**: Use test URL during development
- **All Paths**: Test all possible form branches
- **Error Cases**: Test validation and error scenarios
- **Production**: Switch to production URL when ready

#### Security Considerations:

**Data Privacy:**
- **Sensitive Data**: Avoid exposing in query parameters
- **HTTPS**: Always use secure connections
- **Access Control**: Configure appropriate Form Trigger authentication
- **Data Storage**: Consider where form data is stored

**File Uploads:**
- **File Types**: Restrict to necessary file types only
- **File Size**: Implement reasonable size limits
- **Validation**: Validate uploaded files before processing
- **Malware**: Consider scanning uploaded files

#### Performance Considerations:

**Form Complexity:**
- **Field Count**: Balance between steps and fields per page
- **File Uploads**: Consider impact of large file uploads
- **Branching Logic**: Minimize complex conditional paths
- **Processing Time**: Keep inter-step processing quick

**User Experience:**
- **Page Load**: Optimize form page loading times
- **Validation Speed**: Provide quick field validation
- **Submit Processing**: Show progress during submission
- **Error Recovery**: Allow users to correct errors easily

#### Templates Available:
- **Automate Multi-Platform Social Media Content Creation with AI**
- **AI-Powered Social Media Content Generator & Publisher**
- **Flux AI Image Generator**
- **Browse n8n Form integration templates**

#### Related Resources:
- **Form Building Guide**: Best practices for form creation
- **Query Parameters**: URL parameter encoding reference
- **HTML in Forms**: Supported HTML elements and formatting

#### Related Nodes:
- **n8n Form Trigger**: Required trigger node to start form workflows
- **Switch**: Create conditional form paths
- **Merge**: Combine data from multiple form branches
- **Respond to Webhook**: Alternative response handling
- **HTML**: Generate custom HTML content

#### Troubleshooting:

**Common Issues:**
- **Form Not Loading**: Check Form Trigger configuration and URL
- **Query Parameters Not Working**: Ensure using production URL and proper encoding
- **Branch Issues**: Verify only one Form Ending displays per execution
- **File Upload Problems**: Check file type restrictions and size limits

**Testing Tips:**
- **Use Test URL**: During development for debugging
- **Check All Branches**: Test every possible form path
- **Mobile Testing**: Verify form works on mobile devices
- **Browser Compatibility**: Test across different browsers

---

### 41. n8n Form Trigger
**Status**: ✅ Active
**Category**: Core - Trigger/Form Building/User Interface
**Purpose**: Start a workflow when a user submits a form, taking the input data from the form. The node generates the form web page for you to use.

#### Relationship to n8n Form:
- **Form Trigger**: Starts the workflow and creates the initial form page
- **n8n Form**: Adds additional pages to continue multi-step forms
- **Required Start**: Must be first node in form-based workflows

#### Authentication:
- **Basic Auth**: Username/password authentication for form access
- **None**: No authentication required (open access)
- **Optional Security**: Authentication can be enabled/disabled as needed

#### Operations:
**1. Form Submission Trigger** - Start workflow when users submit form data

#### Core Parameters:

**1. Authentication**
- **Basic Auth**: Protect form with username/password
  - **Username**: Username for form access
  - **Password**: Password for form access
- **None**: Open access without authentication

**2. Form URLs**
- **Test URL**: Active during development and testing
  - **Usage**: Select "Test Step" or "Test Workflow" for testing
  - **Behavior**: Displays data in workflow UI for debugging
  - **Registration**: Registers test webhook when testing
- **Production URL**: Active when workflow is activated
  - **Usage**: Live form for end users
  - **Behavior**: Data not visible in UI (check Executions tab)
  - **Registration**: Registers production webhook when activated

**3. Form Path**
- **Purpose**: Set custom slug/path for the form URL
- **Default**: Auto-generated random path
- **Custom Path**: Specify memorable or branded form paths
- **URL Structure**: `https://your-instance.com/form/custom-path`

**4. Form Title**
- **Display**: Shows as webpage title and main h1 heading
- **Purpose**: Primary form identification for users
- **SEO**: Also used for HTML page title tag

**5. Form Description**
- **Display**: Subtitle below main title
- **Formatting**: Supports `\n` or `<br>` for line breaks
- **Purpose**: Explain form purpose and instructions to users

**6. Form Elements**
- **Configuration**: Click "Add Form Element" to add fields
- **Field Types**: Multiple input types available for different data

#### Form Element Types:

**1. Text**
- **Field Label**: Display text above input
- **Placeholder**: Hint text in input field
- **Required Field**: Toggle to make mandatory

**2. Email**
- **Field Label**: Display text above email input
- **Placeholder**: Email format hint
- **Required Field**: Toggle to make mandatory
- **Validation**: Automatic email format validation

**3. Number**
- **Field Label**: Display text above number input
- **Placeholder**: Numeric hint text
- **Required Field**: Toggle to make mandatory
- **Input Type**: Numeric keyboard on mobile devices

**4. Password**
- **Field Label**: Display text above password input
- **Placeholder**: Password hint text
- **Required Field**: Toggle to make mandatory
- **Security**: Input masked for privacy

**5. Date**
- **Field Label**: Display text above date picker
- **Format Date**: Date format pattern (e.g., mm/dd/yyyy)
- **Required Field**: Toggle to make mandatory
- **Interface**: Interactive calendar date picker

**6. Textarea**
- **Field Label**: Display text above textarea
- **Placeholder**: Multi-line hint text
- **Required Field**: Toggle to make mandatory
- **Use Case**: Extended text input for comments, descriptions

**7. Dropdown List**
- **Field Label**: Display text above dropdown
- **Add Field Option**: Add dropdown choices
- **Multiple Choice**: Toggle for multi-select capability
- **Required Field**: Toggle to make mandatory

**8. File**
- **Field Label**: Display text above file upload
- **Multiple Files**: Allow multiple file selection
- **Accept File Types**: Comma-separated extensions (.jpg, .png, .pdf)
- **Required Field**: Toggle to make mandatory

**9. Custom HTML**
- **Field Label**: Optional label for HTML content
- **HTML Content**: Custom HTML to insert
- **Element Name**: Include in output data (optional)
- **Restrictions**: No `<script>`, `<style>`, or `<input>` elements
- **Use Case**: Rich text, images, videos, formatting

**10. Hidden Field**
- **Field Name**: Name for hidden data field
- **Field Value**: Default value for hidden field
- **Purpose**: Pass data without user interaction
- **Query Parameters**: Can be set via URL parameters

**7. Respond When**
- **Form Is Submitted**: Send response immediately when form submitted
- **Workflow Finishes**: Wait for workflow completion before responding

#### Node Options:

**1. Append n8n Attribution**
- **Default**: Enabled ("Form automated with n8n" at bottom)
- **Toggle**: Can be disabled to hide attribution

**2. Form Response**
- **Form Submitted Text**: Show success message to user
- **Redirect URL**: Redirect user to specified URL after submission

**3. Ignore Bots**
- **Purpose**: Filter out bot requests (link previewers, crawlers)
- **Default**: Disabled
- **Use Case**: Prevent spam and automated submissions

**4. Use Workflow Timezone**
- **Default**: UTC timezone for `submittedAt` timestamp
- **Enabled**: Use workflow timezone setting instead
- **Impact**: Affects timestamp values in form submission data

#### Query Parameters Support:

**Production Only Feature:**
- **Availability**: Only works with Production URL, not Test URL
- **Purpose**: Set initial field values via URL parameters
- **Global**: Parameters available on all form pages

**Parameter Encoding:**
- **Required**: Percent-encode special characters
- **Examples**:
  - `@` becomes `%40`
  - Space becomes `%20`
- **Tools**: Use URL encode/decode tools for proper formatting

**Example Usage:**
```
Base URL: https://my-account.n8n.cloud/form/my-form
With Parameters: https://my-account.n8n.cloud/form/my-form?email=jane.doe%40example.com&name=Jane%20Doe
```

**Parameter Inheritance:**
- **All Pages**: Same query parameters sent to all form pages
- **n8n Form**: Subsequent form pages receive same parameters
- **Consistent**: Maintains field values across multi-step forms

#### Key Features:

**Form Generation:**
- **Automatic**: Generates complete web form interface
- **Responsive**: Mobile-friendly form design
- **Accessible**: Proper form accessibility features
- **Branded**: Customizable with title, description, styling

**Multi-Step Support:**
- **Integration**: Works with n8n Form nodes for multi-step forms
- **State Management**: Maintains form state across steps
- **Data Flow**: Passes data between form pages
- **User Experience**: Seamless progression through form steps

**Development Support:**
- **Test Environment**: Separate test URL for development
- **Debug Mode**: Visible data flow in test mode
- **Production Ready**: Simple switch to production mode
- **Error Handling**: Built-in error handling and user feedback

#### Use Cases:

**1. Contact and Lead Forms:**
- **Contact Forms**: Customer inquiries and feedback collection
- **Lead Generation**: Marketing lead capture forms
- **Newsletter Signup**: Email subscription forms
- **Event Registration**: Registration for events and webinars

**2. Data Collection:**
- **Survey Forms**: Customer satisfaction and market research
- **Application Forms**: Job applications, service requests
- **Feedback Forms**: Product feedback and reviews
- **Registration Forms**: User account creation and onboarding

**3. File Upload Workflows:**
- **Document Submission**: Legal documents, contracts, applications
- **Image Upload**: Photos, graphics, profile pictures
- **File Processing**: Automated file processing workflows
- **Content Submission**: User-generated content collection

**4. Business Process Automation:**
- **Order Forms**: Product orders and service requests
- **Support Tickets**: Technical support and help desk
- **Approval Workflows**: Document approval and review processes
- **Data Entry**: Structured data collection and validation

**5. Integration Workflows:**
- **CRM Integration**: Lead and customer data collection
- **Email Marketing**: Subscriber data for email campaigns
- **Database Population**: Structured data entry into databases
- **API Data Collection**: Form data for API submissions

#### Common Integration Patterns:

**1. Simple Contact Form:**
```
Form Trigger → Edit Fields (Set) → Send Email → Respond to Webhook
```
- Collect contact information
- Format email notification
- Send to appropriate recipient
- Confirm submission to user

**2. Multi-Step Registration:**
```
Form Trigger → Validate Data → n8n Form (Additional Info) → Create Account → Send Welcome Email
```
- Initial registration data
- Data validation and processing
- Additional information collection
- Account creation and welcome process

**3. File Processing Workflow:**
```
Form Trigger → Extract From File → Process Data → Database Insert → Email Confirmation
```
- File upload form
- Extract and process file contents
- Store processed data
- Confirm processing completion

**4. Lead Generation:**
```
Form Trigger → Edit Fields (Set) → HTTP Request (CRM) → Send Email → Marketing Automation
```
- Lead information collection
- Data formatting for CRM
- CRM integration
- Automated follow-up processes

#### Testing vs Production:

**Development Phase:**
1. **Use Test URL**: During form building and testing
2. **Test Step**: Test form fields and validation
3. **Test Workflow**: Test complete workflow execution
4. **Debug Mode**: View data flow in workflow UI

**Production Deployment:**
1. **Switch to Production URL**: When form is ready for users
2. **Activate Workflow**: Enable automatic execution
3. **Monitor Executions**: Check Executions tab for submission data
4. **Production Monitoring**: Monitor form performance and submissions

#### Security Considerations:

**Access Control:**
- **Authentication**: Use Basic Auth for sensitive forms
- **Bot Protection**: Enable "Ignore Bots" for public forms
- **Validation**: Implement proper data validation
- **Rate Limiting**: Consider rate limiting for high-traffic forms

**Data Privacy:**
- **Sensitive Data**: Handle personally identifiable information appropriately
- **Secure Transmission**: Always use HTTPS for form submissions
- **Data Storage**: Consider data retention and privacy policies
- **Compliance**: Ensure GDPR and other regulatory compliance

**File Upload Security:**
- **File Types**: Restrict to necessary file types only
- **File Size**: Implement reasonable size limits
- **Validation**: Validate uploaded files before processing
- **Scanning**: Consider malware scanning for uploaded files

#### Performance Considerations:

**Form Optimization:**
- **Field Count**: Balance between usability and data collection needs
- **File Uploads**: Consider impact of large file uploads on performance
- **Validation**: Implement client-side validation for better user experience
- **Loading Time**: Optimize form loading and rendering performance

**Workflow Impact:**
- **Processing Time**: Keep workflow processing time reasonable
- **Resource Usage**: Monitor memory and CPU usage for complex forms
- **Error Handling**: Implement graceful error handling and user feedback
- **Scalability**: Plan for expected form submission volume

#### Templates Available:
- **Browse n8n Form Trigger integration templates**: [Template Gallery](https://n8n.io/integrations/n8n-form-trigger/)
- **Search all templates**: [Workflow Templates](https://n8n.io/workflows/)
- **Form building examples**: Various form workflow implementations

#### Related Nodes:
- **n8n Form**: Add additional pages to forms for multi-step workflows
- **Respond to Webhook**: Send custom responses to form submissions
- **Edit Fields (Set)**: Process and format form submission data
- **Send Email**: Email notifications and confirmations
- **HTTP Request**: Integration with external APIs and services

#### Best Practices:

**1. Form Design:**
- **Clear Labels**: Use descriptive, user-friendly field labels
- **Logical Flow**: Organize fields in logical sequence
- **Required Fields**: Mark only essential fields as required
- **Help Text**: Use placeholders and descriptions effectively

**2. User Experience:**
- **Mobile Friendly**: Ensure forms work well on mobile devices
- **Progress Indication**: Show progress in multi-step forms
- **Error Messages**: Provide clear, actionable error messages
- **Confirmation**: Always confirm successful form submission

**3. Data Handling:**
- **Validation**: Validate all form data before processing
- **Security**: Never expose sensitive data in URLs or logs
- **Storage**: Consider where and how form data is stored
- **Privacy**: Implement appropriate privacy protections

**4. Development Workflow:**
- **Test Thoroughly**: Test all form fields and validation rules
- **Error Scenarios**: Test error handling and edge cases
- **Browser Testing**: Test across different browsers and devices
- **Performance**: Test with expected data volumes and file sizes

#### Troubleshooting:

**Common Issues:**
- **Form Not Loading**: Check workflow activation and URL configuration
- **Query Parameters Not Working**: Ensure using Production URL and proper encoding
- **Authentication Problems**: Verify Basic Auth credentials if enabled
- **File Upload Issues**: Check file type restrictions and size limits

**Testing Tips:**
- **Use Test URL**: Always start with Test URL during development
- **Check All Fields**: Test every form field type and validation
- **Error Testing**: Test error scenarios and validation failures
- **Mobile Testing**: Test form functionality on mobile devices

**Production Issues:**
- **Missing Data**: Check Executions tab for submission data
- **Workflow Errors**: Monitor workflow execution for errors
- **User Reports**: Implement user feedback for form issues
- **Performance**: Monitor form submission performance and response times

#### Advanced Features:

**Conditional Logic:**
- **Dynamic Forms**: Use subsequent n8n Form nodes for conditional fields
- **Branch Logic**: Implement different processing paths based on form data
- **Validation Workflows**: Add custom validation between form steps
- **User Experience**: Create personalized form experiences

**Integration Capabilities:**
- **CRM Integration**: Direct integration with customer relationship management systems
- **Email Marketing**: Automated email marketing workflows
- **Database Storage**: Direct database integration for form data
- **API Endpoints**: Custom API integrations based on form submissions

**Analytics and Monitoring:**
- **Submission Tracking**: Track form submission rates and patterns
- **Error Monitoring**: Monitor and alert on form errors
- **Performance Analytics**: Analyze form performance and user behavior
- **A/B Testing**: Test different form designs and flows

#### Migration and Updates:

**Version Compatibility:**
- **URL Changes**: Test URLs when updating n8n versions
- **Feature Updates**: Review new features and capabilities
- **Breaking Changes**: Check for any breaking changes in form functionality
- **Best Practices**: Stay updated with form building best practices

**Workflow Evolution:**
- **Form Enhancement**: Continuously improve form design and functionality
- **Process Optimization**: Optimize backend processing workflows
- **User Feedback**: Incorporate user feedback into form improvements
- **Technology Updates**: Leverage new n8n features for better forms

---

### 42. n8n Trigger
**Status**: ✅ Active
**Category**: Core - Trigger/Workflow Management
**Purpose**: Triggers when the current workflow updates or activates, or when the n8n instance starts or restarts. Use to notify when these events occur.

#### Authentication:
- **No credentials required**: Internal n8n event-based triggering
- **System events**: Responds to n8n system and workflow lifecycle events
- **Automatic activation**: Triggers automatically based on selected events

#### Operations:
**1. System Event Monitoring** - Trigger workflows based on n8n system and workflow events

#### Core Parameters:

**1. Events** - Choose which events should trigger the node:

**Available Events (Multiple Selection Supported):**

**Active Workflow Updated**:
- **Trigger Condition**: When this specific workflow is updated/modified
- **Use Case**: Workflow change notifications, audit trails, deployment tracking
- **Scope**: Only triggers for the workflow containing this node
- **Timing**: Immediately after workflow is saved with changes

**Instance Started**:
- **Trigger Condition**: When the n8n instance starts or restarts
- **Use Case**: System startup notifications, health checks, initialization tasks
- **Scope**: Global event affecting entire n8n instance
- **Timing**: During n8n instance startup/restart process

**Workflow Activated**:
- **Trigger Condition**: When this specific workflow is activated (enabled)
- **Use Case**: Activation notifications, deployment confirmations, workflow lifecycle tracking
- **Scope**: Only triggers for the workflow containing this node
- **Timing**: When workflow changes from inactive to active state

#### Key Features:

**Multiple Event Selection:**
- **Flexible Configuration**: Select one or multiple events for comprehensive monitoring
- **Event Combination**: Can monitor workflow updates AND activations simultaneously
- **Granular Control**: Choose specific events relevant to use case
- **Comprehensive Coverage**: Cover entire workflow and system lifecycle

**Self-Referential Monitoring:**
- **Workflow-Specific**: Active Workflow Updated and Workflow Activated events apply to containing workflow
- **Self-Awareness**: Workflow can monitor its own lifecycle events
- **Internal Tracking**: Track workflow changes from within the workflow itself
- **Audit Capability**: Built-in audit trail for workflow modifications

**System-Level Monitoring:**
- **Instance Events**: Monitor entire n8n instance lifecycle
- **Startup Detection**: Detect system restarts and initialization
- **Global Awareness**: System-wide event monitoring capability
- **Infrastructure Monitoring**: Track n8n infrastructure events

#### Use Cases:

**1. Workflow Lifecycle Management:**
- **Change Notifications**: Alert when workflows are modified or updated
- **Deployment Tracking**: Track workflow deployments and activations
- **Version Control**: Trigger version control operations on workflow changes
- **Audit Logging**: Create audit trails for workflow modifications

**2. System Monitoring and Health Checks:**
- **Startup Notifications**: Alert administrators when n8n instance starts
- **Health Verification**: Verify system health after restarts
- **Service Monitoring**: Monitor n8n service availability and status
- **Infrastructure Alerts**: Alert on system restart events

**3. DevOps and CI/CD Integration:**
- **Deployment Notifications**: Notify teams when workflows are deployed
- **Change Management**: Integrate with change management processes
- **Automated Testing**: Trigger tests when workflows are updated
- **Release Tracking**: Track workflow releases and activations

**4. Compliance and Governance:**
- **Change Auditing**: Track all workflow modifications for compliance
- **Approval Workflows**: Trigger approval processes for workflow changes
- **Documentation Updates**: Update documentation when workflows change
- **Regulatory Reporting**: Generate compliance reports on workflow changes

**5. Operational Automation:**
- **Backup Triggers**: Backup workflows when they're updated
- **Synchronization**: Sync workflow changes across environments
- **Notification Systems**: Notify stakeholders of important changes
- **Monitoring Integration**: Integrate with external monitoring systems

#### Common Integration Patterns:

**1. Change Notification System:**
```
n8n Trigger (Workflow Updated) → Edit Fields (Set) → Send Email → Slack Notification
```
- Monitor workflow changes
- Format change notification
- Notify development team
- Track changes in communication channels

**2. System Health Monitoring:**
```
n8n Trigger (Instance Started) → HTTP Request (Health Check) → Switch → Alert/Confirm
```
- Detect system restarts
- Perform health verification
- Route based on health status
- Alert or confirm system status

**3. Deployment Pipeline:**
```
n8n Trigger (Workflow Activated) → Git (Commit) → Send Email → Update Documentation
```
- Track workflow activations
- Commit activation to version control
- Notify deployment completion
- Update deployment documentation

**4. Audit and Compliance:**
```
n8n Trigger (Multiple Events) → Database Insert → Generate Report → Compliance System
```
- Monitor all workflow lifecycle events
- Log events to audit database
- Generate compliance reports
- Submit to compliance systems

#### Advanced Features:

**Event Combination Strategies:**
- **Comprehensive Monitoring**: Select all events for complete lifecycle tracking
- **Selective Monitoring**: Choose specific events based on requirements
- **Change Focus**: Monitor updates and activations for change management
- **System Focus**: Monitor instance starts for infrastructure management

**Self-Monitoring Capabilities:**
- **Workflow Self-Awareness**: Workflows can monitor their own state changes
- **Internal Auditing**: Build audit capabilities into workflows themselves
- **Change Detection**: Detect when containing workflow is modified
- **Lifecycle Tracking**: Track entire workflow lifecycle from within

**Integration with Workflow Management:**
- **Version Control**: Integrate with Git for workflow versioning
- **Change Management**: Support change management and approval processes
- **Documentation**: Automatic documentation updates on changes
- **Testing**: Trigger automated testing on workflow updates

#### Performance Considerations:

**Event Efficiency:**
- **Lightweight Triggers**: Events have minimal performance impact
- **Selective Monitoring**: Choose only necessary events to reduce overhead
- **Internal Events**: No external API calls or network overhead
- **Fast Response**: Immediate triggering on event occurrence

**Resource Usage:**
- **Minimal Resources**: Very low CPU and memory usage
- **No Polling**: Event-driven rather than polling-based
- **Efficient Processing**: Direct integration with n8n event system
- **Scalable**: Scales with n8n instance capabilities

#### Event Timing and Behavior:

**Active Workflow Updated:**
- **Timing**: Immediately after workflow save operation
- **Scope**: Only the specific workflow containing the trigger
- **Multiple Updates**: Each save operation triggers separately
- **Content Changes**: Triggers on any workflow modification

**Instance Started:**
- **Timing**: During n8n startup process
- **Scope**: Global event for entire n8n instance
- **Restart Detection**: Triggers on both cold starts and restarts
- **Initialization**: May trigger before all systems are fully ready

**Workflow Activated:**
- **Timing**: When workflow transitions from inactive to active
- **Scope**: Only the specific workflow containing the trigger
- **State Change**: Only triggers on activation, not deactivation
- **Manual/Automatic**: Triggers regardless of activation method

#### Security Considerations:

**Access Control:**
- **Internal Events**: No external access required
- **Workflow Permissions**: Respects workflow access permissions
- **System Events**: Instance events available to all authorized users
- **Audit Security**: Secure audit trail capabilities

**Information Security:**
- **Event Data**: Contains workflow and system event information
- **Sensitive Information**: May contain workflow metadata
- **Access Logging**: Log access to event information
- **Privacy Protection**: Protect sensitive workflow information

#### Troubleshooting:

**Common Issues:**
- **Event Not Triggering**: Verify event selection and workflow state
- **Multiple Triggers**: Understand that each event triggers separately
- **Timing Issues**: Consider event timing in workflow design
- **Scope Confusion**: Understand workflow-specific vs system-wide events

**Debugging Tips:**
- **Test Events**: Manually trigger events to test functionality
- **Event Logging**: Use execution logs to verify event triggering
- **Timing Analysis**: Analyze event timing and sequence
- **Configuration Review**: Verify event selection configuration

#### Templates Available:
- **Browse n8n Trigger integration templates**: [Template Gallery](https://n8n.io/integrations/n8n-trigger/)
- **Search all templates**: [Workflow Templates](https://n8n.io/workflows/)
- **System monitoring examples**: Various monitoring workflow implementations
- **DevOps integration patterns**: Deployment and change management examples

#### Related Nodes:
- **Workflow Trigger**: Trigger other workflows from this workflow
- **Error Trigger**: Handle workflow errors and failures
- **Schedule Trigger**: Time-based workflow triggering
- **Manual Trigger**: Manual workflow triggering for testing
- **Webhook**: External HTTP-based workflow triggering

#### Best Practices:

**1. Event Selection:**
- **Purpose-Driven**: Select events based on specific monitoring needs
- **Avoid Redundancy**: Avoid selecting unnecessary events
- **Comprehensive Coverage**: Consider all relevant lifecycle events
- **Performance Impact**: Balance monitoring needs with performance

**2. Workflow Design:**
- **Event Handling**: Design workflows to handle multiple event types
- **Error Handling**: Implement proper error handling for event processing
- **Notification Logic**: Create clear and actionable notifications
- **Testing**: Test all selected event types thoroughly

**3. Integration Strategy:**
- **External Systems**: Integrate with external monitoring and notification systems
- **Change Management**: Integrate with change management processes
- **Documentation**: Maintain documentation of monitoring workflows
- **Audit Trails**: Create comprehensive audit trails for compliance

**4. Operational Excellence:**
- **Monitoring**: Monitor the monitoring workflows themselves
- **Alerting**: Implement appropriate alerting for critical events
- **Documentation**: Document event handling procedures
- **Maintenance**: Regularly review and update monitoring configurations

#### Migration and Evolution:

**Workflow Evolution:**
- **Event Adaptation**: Adapt event monitoring as workflows evolve
- **Requirement Changes**: Update event selection based on changing requirements
- **Integration Updates**: Update integrations as systems evolve
- **Process Improvement**: Continuously improve monitoring and notification processes

**System Upgrades:**
- **Version Compatibility**: Verify event compatibility across n8n versions
- **Feature Updates**: Leverage new monitoring features in updates
- **Migration Planning**: Plan event monitoring during system migrations
- **Backward Compatibility**: Ensure monitoring works across version changes

---

42. n8n Trigger
### 43. No Operation, do nothing
**Status**: ✅ Active
**Category**: Core - Workflow Organization/Visual Clarity
**Purpose**: Use when you don't want to perform any operations. Makes workflows easier to read and understand where the flow of data stops, helping others visually get a better understanding of the workflow.

#### Authentication:
- **No credentials required**: Simple pass-through node with no external connections
- **No configuration needed**: Works immediately without any setup
- **Visual purpose**: Primarily serves as workflow documentation and organization tool

#### Operations:
**1. Pass-Through Operation** - Receives input data and passes it through unchanged without performing any operations

#### Core Parameters:
**No Parameters Required**: This node has no configurable parameters or options. It simply receives input data and passes it through unchanged.

#### Key Features:

**Visual Workflow Organization:**
- **Workflow Clarity**: Makes complex workflows easier to read and understand
- **Data Flow Visualization**: Clearly shows where data flow terminates or pauses
- **Documentation Purpose**: Serves as visual documentation for workflow logic
- **Team Collaboration**: Helps team members understand workflow structure

**Pass-Through Functionality:**
- **Data Preservation**: Passes all input data through without modification
- **Zero Processing**: Performs no operations on the data
- **Maintains Structure**: Preserves original data structure and format
- **No Side Effects**: Has no impact on workflow performance or data

**Workflow Design Support:**
- **Placeholder Functionality**: Can serve as placeholder for future development
- **Branch Termination**: Clearly marks the end of workflow branches
- **Testing Support**: Useful during workflow development and testing
- **Visual Separation**: Creates visual breaks in complex workflow logic

#### Use Cases:

**1. Workflow Documentation:**
- **Visual Clarity**: Make complex workflows easier to understand
- **Data Flow Endpoints**: Clearly mark where data processing ends
- **Branch Termination**: Show where specific workflow branches conclude
- **Team Understanding**: Help team members quickly grasp workflow logic

**2. Development and Testing:**
- **Placeholder Nodes**: Temporary placeholder during workflow development
- **Testing Breakpoints**: Create stopping points for testing and debugging
- **Incremental Development**: Mark areas for future development
- **Workflow Staging**: Show different stages of workflow completion

**3. Workflow Organization:**
- **Logical Separation**: Separate different sections of complex workflows
- **Visual Breaks**: Create visual breaks in long workflow chains
- **Flow Documentation**: Document the intended flow of data
- **Process Boundaries**: Mark boundaries between different processes

**4. Error Handling and Fallbacks:**
- **Safe Termination**: Provide safe endpoints for error handling branches
- **Fallback Routes**: Mark fallback processing routes
- **Exception Paths**: Clearly show exception handling paths
- **Graceful Endings**: Provide graceful workflow termination points

**5. Conditional Logic Support:**
- **Branch Endpoints**: Mark the end of conditional processing branches
- **Switch Termination**: Provide endpoints for Switch node branches
- **If-Then Endings**: Mark the conclusion of conditional logic paths
- **Logic Visualization**: Make conditional logic easier to understand

#### Common Integration Patterns:

**1. Workflow Branch Termination:**
```
Switch Node → Process A → No Operation, do nothing
            → Process B → No Operation, do nothing
            → Process C → Continue Processing
```
- Clearly mark which branches terminate vs continue
- Visual indication of workflow completion points
- Help team understand different processing paths

**2. Development Placeholder:**
```
Current Processing → No Operation, do nothing → [Future Development]
```
- Mark areas for future feature development
- Placeholder for planned functionality
- Visual reminder of development roadmap

**3. Testing and Debugging:**
```
Data Source → Transform → No Operation, do nothing → [Test Output]
```
- Create testing breakpoints in workflows
- Isolate sections for debugging
- Validate data at specific points

**4. Error Handling Endpoints:**
```
Try Process → Success Path → Continue
           → Error Path → No Operation, do nothing
```
- Provide safe termination for error branches
- Clear indication of error handling completion
- Visual documentation of error flows

#### Advanced Features:

**Data Pass-Through:**
- **Unchanged Data**: All input data passes through completely unchanged
- **Multiple Items**: Handles single items or arrays of items identically
- **Data Types**: Works with any data type (JSON, binary, etc.)
- **Structure Preservation**: Maintains all data structure and metadata

**Performance Characteristics:**
- **Zero Overhead**: Minimal performance impact on workflows
- **Instant Processing**: No processing delay or resource usage
- **Memory Efficient**: Uses minimal memory resources
- **Scalable**: Handles any volume of data efficiently

**Visual Design:**
- **Clear Naming**: Descriptive name makes purpose immediately clear
- **Visual Distinction**: Easily recognizable in workflow diagrams
- **Consistent Behavior**: Predictable behavior across all use cases
- **Documentation Value**: Self-documenting through clear purpose

#### Best Practices:

**1. Workflow Organization:**
- **Strategic Placement**: Use at logical endpoints and boundaries
- **Clear Purpose**: Place where visual clarity is most needed
- **Team Understanding**: Consider team member workflow comprehension
- **Documentation Value**: Use to enhance workflow documentation

**2. Development Workflow:**
- **Placeholder Strategy**: Use as placeholders during incremental development
- **Testing Points**: Create clear testing and validation points
- **Future Planning**: Mark areas for future feature development
- **Version Control**: Help track workflow evolution over time

**3. Visual Design:**
- **Logical Endpoints**: Place at natural stopping points in workflow logic
- **Branch Termination**: Use to clearly mark the end of processing branches
- **Flow Clarity**: Enhance overall workflow readability
- **Team Collaboration**: Improve team understanding of complex workflows

**4. Performance Considerations:**
- **Minimal Impact**: No performance concerns with this node
- **Liberal Usage**: Can be used liberally without performance penalty
- **Resource Efficiency**: No resource consumption concerns
- **Scalability**: Scales with any workflow size or complexity

#### Integration Examples:

**1. Complex Business Process:**
- **Process Documentation**: Use to document different stages of business processes
- **Approval Workflows**: Mark endpoints of approval chains
- **Compliance Processes**: Show completion of compliance steps
- **Quality Assurance**: Mark QA checkpoints and endpoints

**2. Data Processing Pipeline:**
- **ETL Processes**: Mark completion of extract, transform, or load phases
- **Data Validation**: Show validation endpoints and success paths
- **Error Handling**: Provide clear error handling termination points
- **Pipeline Stages**: Visually separate different pipeline stages

**3. API Integration Workflows:**
- **Request Processing**: Mark completion of request processing
- **Response Handling**: Show successful response processing endpoints
- **Error Responses**: Clearly mark error response handling completion
- **Webhook Processing**: Document webhook processing completion

**4. Automated Business Workflows:**
- **Customer Onboarding**: Mark completion of onboarding steps
- **Order Processing**: Show order processing completion points
- **Notification Workflows**: Mark successful notification delivery
- **Reporting Processes**: Show report generation completion

#### Development and Testing Support:

**Workflow Development:**
- **Incremental Building**: Mark areas for future development
- **Feature Planning**: Placeholder for planned features
- **Version Staging**: Show different development stages
- **Requirements Documentation**: Visual representation of requirements

**Testing and Debugging:**
- **Test Endpoints**: Create clear testing termination points
- **Debug Breakpoints**: Mark debugging and inspection points
- **Validation Points**: Show data validation completion
- **Quality Assurance**: Mark QA testing endpoints

**Team Collaboration:**
- **Workflow Reviews**: Help team members understand complex flows
- **Knowledge Transfer**: Aid in transferring workflow knowledge
- **Documentation**: Self-documenting workflow structure
- **Onboarding**: Help new team members understand workflows

#### Templates Available:
- **Back Up Your n8n Workflows To Github**: Workflow organization and documentation example
- **✨🩷Automated Social Media Content Publishing Factory + System Prompt Composition**: Complex workflow organization
- **Host Your Own AI Deep Research Agent with n8n, Apify and OpenAI o3**: Advanced workflow structure documentation
- **Browse No Operation, do nothing integration templates**: [Template Gallery](https://n8n.io/integrations/no-operation-do-nothing/)

#### Related Nodes:
- **Manual Trigger**: Simple workflow starting point
- **Debug Helper**: Development and testing support
- **Stop And Error**: Intentional workflow termination with error
- **If**: Conditional logic that may need clear endpoints
- **Switch**: Multiple branch logic requiring clear termination points

#### Use Case Examples:

**1. Customer Support Workflow:**
```
Ticket Received → Categorize → High Priority → Escalate → No Operation, do nothing
                            → Medium Priority → Standard Process → No Operation, do nothing
                            → Low Priority → Auto-Response → No Operation, do nothing
```
- Clear visual indication of different support path endpoints
- Team understanding of escalation outcomes
- Documentation of support process completion

**2. E-commerce Order Processing:**
```
Order Received → Validate → Valid → Process Payment → Ship → No Operation, do nothing
                         → Invalid → Send Error Email → No Operation, do nothing
```
- Visual clarity of order processing outcomes
- Clear indication of successful vs failed processing
- Team understanding of process endpoints

**3. Data Integration Pipeline:**
```
Data Source → Extract → Transform → Validate → Success → Load → No Operation, do nothing
                                            → Failure → Log Error → No Operation, do nothing
```
- Clear ETL process documentation
- Visual indication of success vs failure paths
- Team understanding of data pipeline outcomes

#### Workflow Design Principles:

**Visual Clarity:**
- **Logical Endpoints**: Place at natural conclusion points
- **Flow Documentation**: Enhance understanding of data flow
- **Team Communication**: Improve team workflow comprehension
- **Process Boundaries**: Clearly mark process completion

**Development Support:**
- **Incremental Development**: Support phased development approaches
- **Testing Integration**: Provide clear testing termination points
- **Feature Planning**: Mark areas for future development
- **Version Control**: Help track workflow evolution

**Operational Excellence:**
- **Documentation Value**: Self-documenting workflow structure
- **Maintenance Support**: Aid in workflow maintenance and updates
- **Knowledge Transfer**: Support team knowledge sharing
- **Quality Assurance**: Clear QA and validation endpoints

#### Troubleshooting:

**Common Usage Questions:**
- **When to Use**: Use when visual clarity and documentation are needed
- **Performance Impact**: No performance impact - use liberally as needed
- **Data Flow**: All data passes through unchanged
- **Team Benefits**: Improves team understanding of complex workflows

**Best Practices:**
- **Strategic Placement**: Use at logical workflow endpoints
- **Clear Documentation**: Enhance workflow documentation value
- **Team Collaboration**: Consider team member workflow understanding
- **Development Support**: Aid in development and testing processes

#### Future Considerations:

**Workflow Evolution:**
- **Documentation Updates**: Keep visual documentation current with workflow changes
- **Team Onboarding**: Continue to support new team member understanding
- **Process Improvement**: Use to identify workflow optimization opportunities
- **Standards Development**: Develop team standards for visual workflow organization

**Enhanced Features:**
- **Visual Customization**: Potential for enhanced visual customization
- **Documentation Integration**: Enhanced integration with workflow documentation
- **Team Collaboration**: Advanced team collaboration features
- **Analytics Integration**: Potential integration with workflow analytics

---

43. No Operation, do nothing
### 44. Read/Write Files from Disk
**Status**: ✅ Active (Self-hosted only)
**Category**: Core - File System/Data Storage
**Purpose**: Read and write files from/to the machine where n8n is running

#### Availability:
- **Self-hosted n8n only**: Not available on n8n Cloud
- **Local file system access**: Requires direct access to host file system
- **Container considerations**: Docker commands run in n8n container, not Docker host

#### Authentication:
- **No credentials required**: Direct file system access
- **File permissions**: Requires appropriate file system permissions
- **Path access**: Must have read/write access to specified paths

#### Operations (2 total):

**1. Read File(s) From Disk** - Retrieve one or more files from the computer that runs n8n
- **Purpose**: Load files from local file system into workflow for processing
- **Multiple files**: Support for glob patterns to read multiple files
- **Binary output**: Files are loaded as binary data for further processing

**2. Write File to Disk** - Create a binary file on the computer that runs n8n
- **Purpose**: Save workflow data as files to local file system
- **Binary input**: Accepts binary data from previous nodes
- **File creation**: Creates new files or appends to existing files

#### Operation Details:

**1. Read File(s) From Disk**

**Core Parameters:**
- **File(s) Selector**: Enter the path of the file(s) you want to read
  - **Single File**: Direct file path (e.g., `/home/user/document.pdf`)
  - **Multiple Files**: Use glob patterns for multiple file selection
  - **Expression Support**: Use n8n expressions for dynamic file paths

**Glob Pattern Support:**
- **`*`**: Matches any character zero or more times, excluding path separators
  - Example: `*.txt` matches all `.txt` files in directory
- **`**`**: Matches any character zero or more times, including path separators
  - Example: `**/*.pdf` matches all `.pdf` files in any subdirectory
- **`?`**: Matches any character except path separators one time
  - Example: `file?.txt` matches `file1.txt`, `filea.txt`
- **`[]`**: Matches any characters inside brackets
  - Example: `file[abc].txt` matches `filea.txt`, `fileb.txt`, `filec.txt`

**Read Options:**
- **File Extension**: Extension for the file in the node output
- **File Name**: Name for the file in the node output
- **MIME Type**: File's MIME type in the node output (see [Common MIME types](https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/MIME_types/Common_types))
- **Put Output File in Field**: Name of the field in output data to contain the file

**2. Write File to Disk**

**Core Parameters:**
- **File Path and Name**: Enter destination path, filename, and extension
  - **Complete Path**: Full path including directory, filename, and extension
  - **Directory Creation**: Automatically creates directories if they don't exist
  - **Expression Support**: Use n8n expressions for dynamic file paths
- **Input Binary Field**: Name of the field in input data containing the binary file
  - **Binary Data**: Field must contain binary file data
  - **Field Reference**: Must match exactly with binary field name from previous node

**Write Options:**
- **Append**: Control file creation behavior
  - **On**: Append data to existing file instead of creating new one
  - **Off**: Create new file (overwrites existing file with same name)

#### Key Features:

**File System Integration:**
- **Local File Access**: Direct access to host machine file system
- **Path Flexibility**: Support for absolute and relative file paths
- **Directory Operations**: Automatic directory creation for write operations
- **Multiple File Handling**: Process multiple files using glob patterns

**Binary Data Handling:**
- **Binary Preservation**: Maintains file integrity through binary data handling
- **File Type Support**: Works with any file type (documents, images, videos, etc.)
- **Data Flow**: Seamless integration with other n8n nodes that handle binary data
- **Metadata Management**: Proper file metadata handling (MIME types, extensions)

**Pattern Matching:**
- **Glob Support**: Advanced pattern matching using Picomatch library
- **Flexible Selection**: Select files based on names, extensions, or directory structures
- **Batch Processing**: Process multiple files in single workflow execution
- **Dynamic Patterns**: Use expressions to build patterns dynamically

#### Use Cases:

**1. File Processing Workflows:**
- **Document Processing**: Read documents for content extraction and analysis
- **Image Processing**: Load images for editing, conversion, or analysis
- **Data Import**: Import CSV, JSON, XML files for data processing
- **Log Analysis**: Read log files for monitoring and analysis

**2. Backup and Archive Operations:**
- **Automated Backups**: Create backup files from workflow data
- **File Archival**: Archive processed files with timestamps
- **Data Export**: Export workflow results to files
- **Configuration Backup**: Backup configuration files and settings

**3. Report Generation:**
- **Report Output**: Generate reports and save as PDF, Excel, or other formats
- **Data Export**: Export processed data to files for external use
- **Scheduled Reports**: Create and save reports on schedule
- **Multi-format Output**: Generate same data in multiple file formats

**4. Integration Workflows:**
- **File-based Integration**: Integrate with systems using file-based data exchange
- **Drop Folder Processing**: Process files from designated drop folders
- **File Distribution**: Distribute generated files to multiple locations
- **Legacy System Integration**: Work with systems that require file-based input/output

**5. Development and Testing:**
- **Test Data Management**: Load test data from files for workflow testing
- **Result Verification**: Save workflow outputs for verification and debugging
- **Snapshot Creation**: Create snapshots of data at various workflow stages
- **Development Assets**: Manage development files and resources

#### Common Integration Patterns:

**1. File Processing Pipeline:**
```
Local File Trigger → Read/Write Files (Read) → Extract From File → Process → Read/Write Files (Write)
```
- Monitor folder for new files
- Read files when they appear
- Extract and process content
- Save processed results

**2. Backup Workflow:**
```
Schedule Trigger → Database Query → Convert to File → Read/Write Files (Write) → Email Notification
```
- Scheduled data backup
- Export database data
- Save as file backup
- Notify completion

**3. Report Generation:**
```
API Data → Transform → Convert to File → Read/Write Files (Write) → FTP Upload
```
- Collect data from APIs
- Process and format data
- Generate report files
- Distribute to file servers

**4. Batch File Processing:**
```
Manual Trigger → Read/Write Files (Read with
### 45. Remove Duplicates
**Status**: ✅ Active
**Category**: Core - Data Processing/Quality Control
**Purpose**: Identify and delete items that are identical across all fields or a subset of fields in a single execution, or identical to/surpassed by items seen in previous executions

#### Authentication:
- **No credentials required**: Direct data processing operation
- **Internal processing**: Uses n8n's internal deduplication algorithms
- **Memory-based**: Maintains state for cross-execution deduplication

#### Operations (3 total):

**1. Remove Items Repeated Within Current Input** - Identify and remove duplicate items in the current input across all fields or a subset of fields
- **Purpose**: Clean duplicates within single workflow execution
- **Scope**: Current execution data only
- **Use Case**: Remove duplicates from API responses, uploaded files, or batch data

**2. Remove Items Processed in Previous Executions** - Compare items in current input to items from previous executions and remove duplicates
- **Purpose**: Cross-execution deduplication to prevent reprocessing
- **Scope**: Compares against historical execution data
- **Use Case**: Prevent duplicate processing in scheduled workflows, incremental data sync

**3. Clear Deduplication History** - Wipe the memory of items from previous executions
- **Purpose**: Reset deduplication state for fresh processing
- **Scope**: Clears stored deduplication data
- **Use Case**: Maintenance operations, testing, data reset scenarios

#### Core Parameters:

**1. Operation Selection**
- **Remove Items Repeated Within Current Input**: Process current execution data only
- **Remove Items Processed in Previous Executions**: Cross-execution deduplication
- **Clear Deduplication History**: Reset deduplication state

#### Remove Items Repeated Within Current Input Parameters:

**1. Compare** - Select which fields to compare for duplicate detection:

**All Fields**:
- **Purpose**: Compare all fields of input data for exact matches
- **Behavior**: Items must be identical across all properties
- **Use Case**: Complete record deduplication

**All Fields Except**:
- **Purpose**: Exclude specific fields from comparison
- **Configuration**: Enter comma-separated list of fields to exclude
- **Use Case**: Ignore timestamps, IDs, or metadata fields in comparison

**Selected Fields**:
- **Purpose**: Compare only specified fields for duplicates
- **Configuration**: Enter comma-separated list of fields to include
- **Use Case**: Focus on business key fields for deduplication

#### Remove Items Repeated Within Current Input Options:

**1. Disable Dot Notation**
- **Purpose**: Control access to nested fields using dot notation (`parent.child`)
- **Off**: Enable dot notation for nested field access (default)
- **On**: Disable dot notation, treat field names literally

**2. Remove Other Fields**
- **Purpose**: Control field retention in output (for All Fields Except/Selected Fields)
- **On**: Remove fields not used in comparison from output
- **Off**: Keep all original fields in output (default)

#### Remove Items Processed in Previous Executions Parameters:

**1. Keep Items Where** - Determine deduplication criteria:

**Value Is New**:
- **Purpose**: Remove items if their value matches items from earlier executions
- **Behavior**: Only items with new/unique values pass through
- **Use Case**: Process only new records, prevent duplicate submissions

**Value Is Higher than Any Previous Value**:
- **Purpose**: Remove items unless current value exceeds all previous values
- **Behavior**: Items pass only if they represent increases/improvements
- **Use Case**: Process only improved metrics, higher scores, increased values

**Value Is a Date Later than Any Previous Date**:
- **Purpose**: Remove items unless current date is later than previous dates
- **Behavior**: Items pass only if they represent newer timestamps
- **Use Case**: Process only recent data, chronological progression

**2. Value to Dedupe On**
- **Purpose**: Specify field(s) to use for deduplication comparison
- **Format Requirements**:
  - **Value Is New**: Field or combination with unique identifier
  - **Value Is Higher**: Field with incremental numeric values
  - **Date Later**: Field with ISO format date values

#### Remove Items Processed in Previous Executions Options:

**1. Scope** - Control deduplication data storage and sharing:

**Node** (Default):
- **Purpose**: Store deduplication data specific to this node instance
- **Isolation**: Independent from other Remove Duplicates nodes
- **Management**: Can clear history for this node without affecting others

**Workflow**:
- **Purpose**: Share deduplication data across multiple Remove Duplicates nodes
- **Sharing**: All nodes using "workflow" scope share same deduplication data
- **Coordination**: Enables coordinated deduplication across workflow

**2. History Size** (Value Is New only)
- **Purpose**: Control number of items stored for duplicate tracking
- **Default**: 10,000 items
- **Scope Impact**: Size applies per node or per workflow based on Scope setting
- **Memory**: Larger history uses more memory but provides better deduplication

#### Clear Deduplication History Parameters:

**1. Mode** - Choose history management operation:

**Clean Database**:
- **Purpose**: Delete all stored deduplication data
- **Effect**: Resets deduplication state to original condition
- **Scope**: Affects node or workflow scope based on Scope option

#### Clear Deduplication History Options:

**1. Scope** - Control which deduplication data to clear:

**Node** (Default):
- **Purpose**: Clear data specific to this Remove Duplicates node
- **Isolation**: Only affects this node instance

**Workflow**:
- **Purpose**: Clear shared workflow-level deduplication data
- **Impact**: Affects all Remove Duplicates nodes using workflow scope

#### Key Features:

**Flexible Deduplication:**
- **Field Selection**: Choose which fields to use for duplicate detection
- **Comparison Options**: Multiple comparison strategies for different use cases
- **Cross-Execution**: Prevent processing same items across multiple executions
- **State Management**: Maintains deduplication state between workflow runs

**Performance Optimization:**
- **Efficient Algorithms**: Optimized deduplication algorithms for large datasets
- **Memory Management**: Configurable history size for memory optimization
- **Scope Control**: Node vs workflow scope for resource management
- **Selective Processing**: Process only necessary fields for comparison

**Data Quality:**
- **Exact Matching**: Precise duplicate detection across fields
- **Business Logic**: Support for business-specific deduplication rules
- **Progressive Values**: Handle incremental and chronological data
- **Clean Output**: Removes duplicates while preserving data integrity

#### Use Cases:

**1. Data Quality Management:**
- **Import Deduplication**: Remove duplicates from data imports and uploads
- **API Response Cleaning**: Clean duplicate records from API responses
- **Database Sync**: Prevent duplicate entries during database synchronization
- **File Processing**: Remove duplicates from CSV, Excel, and other data files

**2. Incremental Data Processing:**
- **Changed Records Only**: Process only records that have changed since last run
- **New Data Processing**: Handle only new data items in scheduled workflows
- **Progressive Updates**: Process only improved or updated values
- **Chronological Processing**: Handle only recent timestamps and dates

**3. Business Process Automation:**
- **Customer Deduplication**: Prevent duplicate customer records creation
- **Order Processing**: Avoid processing duplicate orders or transactions
- **Email Marketing**: Prevent sending duplicate emails to same recipients
- **Inventory Management**: Handle only new or changed inventory items

**4. System Integration:**
- **Multi-Source Data**: Deduplicate data from multiple source systems
- **ETL Pipelines**: Clean duplicates in extract, transform, load processes
- **Real-time Processing**: Handle duplicates in streaming data workflows
- **Batch Processing**: Manage duplicates in large batch processing jobs

#### Common Integration Patterns:

**1. Data Import Pipeline:**
```
Data Source → Remove Duplicates (Current Input) → Validate → Database Insert
```
- Remove duplicates from imported data
- Validate cleaned data
- Insert only unique records

**2. Incremental Sync:**
```
Schedule Trigger → API Call → Remove Duplicates (Previous Executions) → Update Records
```
- Scheduled data synchronization
- Process only new or changed items
- Prevent duplicate processing

**3. Customer Data Management:**
```
Customer Data → Remove Duplicates (Selected Fields) → CRM Integration → Notification
```
- Deduplicate based on email, phone, or customer ID
- Integrate with CRM system
- Notify of new customer additions

**4. File Processing:**
```
File Upload → Extract From File → Remove Duplicates → Process Records → Generate Report
```
- Process uploaded data files
- Remove duplicate records
- Generate processing reports

#### Advanced Features:

**Cross-Execution State:**
- **Persistent Memory**: Maintains deduplication state across workflow executions
- **Configurable History**: Control how many items to remember
- **Scope Management**: Share or isolate deduplication state
- **Reset Capability**: Clear state when needed for maintenance

**Field Comparison Strategies:**
- **Exact Matching**: Precise field-by-field comparison
- **Selective Comparison**: Choose specific fields for business logic
- **Nested Field Support**: Handle complex object structures with dot notation
- **Exclusion Logic**: Exclude irrelevant fields like timestamps

**Progressive Value Handling:**
- **Incremental Values**: Handle only increasing numeric values
- **Chronological Order**: Process only later dates and timestamps
- **Business Metrics**: Handle only improved business metrics
- **Threshold Management**: Set thresholds for value progression

#### Performance Considerations:

**Memory Management:**
- **History Size**: Configure appropriate history size for memory usage
- **Scope Optimization**: Use node scope for isolated processing
- **Field Selection**: Compare only necessary fields for performance
- **Cleanup**: Regular cleanup of deduplication history

**Processing Efficiency:**
- **Selective Fields**: Compare minimal necessary fields
- **Batch Processing**: Handle large datasets efficiently
- **Algorithm Optimization**: Efficient duplicate detection algorithms
- **Resource Monitoring**: Monitor memory and CPU usage

#### Templates Available:
- **Automated Web Scraping: email a CSV, save to Google Sheets & Microsoft Excel**
- **Scrape business emails from Google Maps without third party APIs**
- **AI agent that can scrape webpages**
- **Browse Remove Duplicates integration templates**

#### Related Nodes:
- **Filter**: Alternative for removing items based on conditions
- **Compare Datasets**: Compare data between different sources
- **Aggregate**: Group and combine duplicate items
- **Sort**: Order data before deduplication
- **Code**: Custom deduplication logic

#### Best Practices:

**1. Field Selection:**
- **Business Keys**: Use relevant business identifier fields for comparison
- **Exclude Metadata**: Exclude timestamps, IDs, and system fields from comparison
- **Minimize Fields**: Compare only necessary fields for performance
- **Test Thoroughly**: Test field selection with representative data

**2. Cross-Execution Setup:**
- **Appropriate Scope**: Choose node vs workflow scope based on requirements
- **History Size**: Set appropriate history size for data volume and memory
- **Regular Cleanup**: Implement regular cleanup of deduplication history
- **Monitor Memory**: Monitor memory usage with large history sizes

**3. Performance Optimization:**
- **Efficient Comparison**: Use most selective fields for comparison
- **Resource Management**: Monitor system resources during processing
- **Batch Sizing**: Process data in appropriate batch sizes
- **Algorithm Selection**: Choose appropriate deduplication strategy

**4. Data Quality:**
- **Validation**: Validate data before and after deduplication
- **Testing**: Test with various data scenarios and edge cases
- **Monitoring**: Monitor deduplication effectiveness and performance
- **Documentation**: Document deduplication rules and business logic

#### Troubleshooting:

**Common Issues:**
- **Memory Usage**: High memory usage with large history sizes
- **Field Matching**: Incorrect field names or paths for comparison
- **Cross-Execution**: Deduplication not working across executions
- **Performance**: Slow processing with large datasets

**Resolution Strategies:**
- **History Management**: Adjust history size and cleanup frequency
- **Field Validation**: Verify field names and dot notation usage
- **Scope Configuration**: Check scope settings for cross-execution deduplication
- **Performance Tuning**: Optimize field selection and processing approach

#### Security Considerations:

**Data Privacy:**
- **Sensitive Fields**: Handle personally identifiable information appropriately
- **Field Exclusion**: Exclude sensitive fields from deduplication comparison
- **History Storage**: Consider data retention policies for deduplication history
- **Access Control**: Ensure appropriate access controls for deduplication data

**State Management:**
- **Scope Security**: Use appropriate scope for security isolation
- **History Cleanup**: Regular cleanup of stored deduplication data
- **Memory Protection**: Protect deduplication state from unauthorized access
- **Audit Logging**: Log deduplication operations for compliance

#### Advanced Use Cases:

**1. Multi-Source Integration:**
- **Source Deduplication**: Remove duplicates from multiple data sources
- **System Synchronization**: Sync data across multiple systems without duplicates
- **Master Data Management**: Maintain single source of truth for master data
- **Cross-Platform Integration**: Handle duplicates across different platforms

**2. Real-Time Processing:**
- **Stream Deduplication**: Handle duplicates in streaming data
- **Event Processing**: Deduplicate events in real-time processing
- **API Gateway**: Remove duplicate API requests
- **Message Queue**: Handle duplicate messages in queue processing

**3. Business Intelligence:**
- **Report Deduplication**: Ensure unique data in business reports
- **Analytics Preparation**: Clean data for analytics and reporting
- **KPI Calculation**: Handle duplicates in key performance indicators
- **Dashboard Data**: Provide clean data for business dashboards

**4. Compliance and Governance:**
- **Data Governance**: Implement data quality rules for governance
- **Regulatory Compliance**: Ensure data quality for regulatory reporting
- **Audit Preparation**: Prepare clean data for audit processes
- **Quality Assurance**: Implement automated data quality checks

---

45. Remove Duplicates

### 46. Rename Keys
**Status**: ✅ Active
**Category**: Core - Data Processing/Field Management
**Purpose**: Rename the keys of key-value pairs in n8n workflows to standardize field names, improve data consistency, and prepare data for downstream processing

#### Authentication:
- **No credentials required**: Direct data processing operation
- **Field-level operations**: Works with JSON object properties and nested structures
- **Expression support**: Dynamic key naming using n8n expressions

#### Operations:
**1. Key Renaming** - Rename one or multiple keys in data objects using manual specification or regular expressions

#### Core Parameters:

**1. Key Renaming Configuration** - Define which keys to rename and their new names:

**Manual Key Specification:**
- **Add New Key**: Button to add individual key rename specifications
- **Current Key Name**: The existing name of the key you want to rename
- **New Key Name**: The new name you want to assign to the key
- **Multiple Keys**: Add multiple key rename specifications as needed

**Key Naming Options:**
- **Direct Names**: Specify exact current and new key names
- **Expression Support**: Use n8n expressions for dynamic key naming
- **Nested Keys**: Support for nested object key references
- **Case Handling**: Maintain or change case as specified

#### Node Options:

**1. Regex (Regular Expression Mode)**
- **Purpose**: Use regular expressions to identify and rename keys matching patterns
- **Toggle**: Enable/disable regex-based key renaming
- **Advanced Pattern Matching**: More flexible than manual key specification

**Regex Configuration (when enabled):**

**2. Regular Expression**
- **Purpose**: Define the regex pattern to match keys for renaming
- **Pattern Syntax**: Standard JavaScript regex syntax
- **Flexible Matching**: Match multiple keys with single pattern
- **Complex Patterns**: Support for complex pattern matching scenarios

**3. Replace With**
- **Purpose**: Define the replacement pattern for matched keys
- **Static Replacement**: Use static text for replacement
- **Pattern References**: Use regex capture groups in replacement
- **Dynamic Replacement**: Combine static text with captured patterns

**Regex-Specific Options:**

**4. Case Insensitive**
- **Purpose**: Control case sensitivity in regex pattern matching
- **Off**: Case-sensitive matching (default)
- **On**: Case-insensitive matching for broader pattern matches
- **Use Case**: Handle variations in key naming conventions

**5. Max Depth**
- **Purpose**: Control depth of key replacement in nested objects
- **Values**:
  - **-1**: Unlimited depth (rename keys at any nesting level)
  - **0**: Top-level only (rename only root-level keys)
  - **Positive numbers**: Specific maximum depth levels
- **Performance**: Limiting depth improves performance with deeply nested objects

#### Key Features:

**Flexible Key Management:**
- **Multiple Strategies**: Manual specification or regex pattern matching
- **Nested Object Support**: Handle complex nested data structures
- **Batch Operations**: Rename multiple keys in single operation
- **Pattern Matching**: Advanced regex support for complex renaming scenarios

**Data Structure Preservation:**
- **Value Preservation**: Maintains all data values while changing key names
- **Structure Integrity**: Preserves object structure and relationships

### 47. Respond to Webhook
**Status**: ✅ Active
**Category**: Core - HTTP/Webhook
**Purpose**: Control the response to incoming webhooks. This node works with the Webhook node to customize the data sent back to the calling service.

#### Authentication:
- **No credentials required**: Works in conjunction with a Webhook trigger.

#### Operations:
**1. Send Custom Response** - Define the exact response to be sent back to the webhook caller.

#### Core Parameters:

**1. Respond With** - Choose the format of the response data:
- **All Incoming Items**: Respond with all the JSON items from the input.
- **Binary File**: Respond with a binary file defined in **Response Data Source**.
- **First Incoming Item**: Respond with the first incoming item's JSON.
- **JSON**: Respond with a JSON object defined in **Response Body**.
- **JWT Token**: Respond with a JSON Web Token (JWT).
- **No Data**: No response payload.
- **Redirect**: Redirect to a URL set in **Redirect URL**.
- **Text**: Respond with text set in **Response Body**.

#### Node Options:

**1. Response Code**
- **Purpose**: Set the HTTP response code to use.
- **Example**: 200 for success, 404 for not found, 500 for server error.

**2. Response Headers**
- **Purpose**: Define the response headers to send.
- **Example**: `Content-Type: application/json`

**3. Put Response in Field**
- **Purpose**: Available when you respond with **All Incoming Items** or **First Incoming Item**. Set the field name for the field containing the response data.

#### Workflow Behavior:
- If the workflow finishes without executing the Respond to Webhook node, it returns a standard message with a 200 status.
- If the workflow errors before the first Respond to Webhook node executes, the workflow returns an error message with a 500 status.
- If a second Respond to Webhook node executes after the first one, the workflow ignores it.
- If a Respond to Webhook node executes but there was no webhook, the workflow ignores the Respond to Webhook node.

#### Use Cases:
- **Custom API Responses**: Create custom API-like endpoints with specific JSON or text responses.
- **File Serving**: Respond to a webhook call with a file.
- **Redirection**: Redirect users to another page after a webhook is triggered.
- **Acknowledgement**: Send a simple "OK" or "Received" message back to the calling service.

### 48. RSS Read
**Status**: ✅ Active
**Category**: Core - Data Fetching
**Purpose**: Read data from RSS feeds published on the internet.

#### Authentication:
- **No credentials required**: Accesses public RSS feeds.

#### Operations:
**1. Read RSS Feed** - Fetches and parses an RSS feed from a given URL.

#### Core Parameters:

**1. URL**
- **Purpose**: Enter the URL for the RSS publication you want to read.
- **Example**: `https://n8n.io/blog/rss/`

#### Node Options:

**1. Ignore SSL Issues**
- **Purpose**: Choose whether n8n should ignore SSL/TLS verification.
- **On**: Ignores SSL issues (useful for feeds with self-signed certificates).
- **Off**: Enforces SSL verification (default).

#### Use Cases:
- **Content Aggregation**: Collect articles and posts from various news sources and blogs.
- **Social Media Automation**: Automatically share new blog posts to social media channels.
- **Data Enrichment**: Enrich other data with information from relevant RSS feeds.
- **Monitoring**: Monitor websites for new content.

### 49. RSS Feed Trigger
**Status**: ✅ Active
**Category**: Core - Trigger
**Purpose**: Starts a workflow when a new RSS feed item has been published.

#### Authentication:
- **No credentials required**: Accesses public RSS feeds.

#### Operations:
**1. Poll RSS Feed** - Periodically checks an RSS feed for new items.

#### Core Parameters:

**1. Poll Times**
- **Purpose**: Select a poll **Mode** to set how often to trigger the poll. Your **Mode** selection will add or remove relevant fields.
- **Modes**:
    - **Every Hour**: Triggers at a specific minute of every hour.
        - **Minute**: 0-59
    - **Every Day**: Triggers at a specific hour and minute of every day.
        - **Hour**: 0-23
        - **Minute**: 0-59
    - **Every Week**: Triggers on a specific day of the week at a specific hour and minute.
        - **Hour**: 0-23
        - **Minute**: 0-59
        - **Weekday**: Sunday-Saturday
    - **Every Month**: Triggers on a specific day of the month at a specific hour and minute.
        - **Day of the Month**: 1-31
        - **Hour**: 0-23
        - **Minute**: 0-59
    - **Every X**: Triggers at a regular interval.
        - **Value**: The interval number.
        - **Unit**: Minutes or Hours.
    - **Custom**: Use a Cron expression for more complex schedules.

**2. Feed URL**
- **Purpose**: Enter the URL of the RSS feed to poll.

#### Use Cases:
- **Content Monitoring**: Automatically trigger workflows when new articles are published on a blog or news site.
- **Social Media Automation**: Share new content from an RSS feed to social media platforms.
- **Data Aggregation**: Collect and process information from multiple feeds into a single source.
- **Alerts and Notifications**: Get notified when specific keywords appear in new RSS feed items.

### 50. Schedule Trigger
**Status**: ✅ Active
**Category**: Core - Trigger
**Purpose**: Run workflows at fixed intervals and times, similar to the Cron software utility in Unix-like systems.

#### Authentication:
- **No credentials required**: This is a time-based trigger internal to n8n.

#### Operations:
**1. Schedule Execution** - Triggers the workflow based on a defined schedule.

#### Core Parameters:

**1. Trigger Rules**
- **Purpose**: Define when the trigger should run. You can add multiple rules to run the node on different schedules.
- **Trigger Interval**: Select the time interval unit of measure to schedule the trigger for. All other parameters depend on the interval you select.
- **Modes**:
    - **Seconds**: Trigger every X seconds.
    - **Minutes**: Trigger every X minutes.
    - **Hours**: Trigger every X hours at a specific minute.
    - **Days**: Trigger every X days at a specific hour and minute.
    - **Weeks**: Trigger every X weeks on specific weekdays at a specific hour and minute.
    - **Months**: Trigger on a specific day of the month, every X months, at a specific hour and minute.
    - **Custom (Cron)**: Use a Cron expression for complex schedules.

#### Use Cases:
- **Regular Reporting**: Generate and send reports on a daily, weekly, or monthly basis.
- **Data Synchronization**: Periodically sync data between two or more systems.
- **System Maintenance**: Run cleanup or maintenance tasks at regular intervals.
- **Scheduled Tasks**: Automate any task that needs to run on a recurring schedule.

### 51. Send Email
**Status**: ✅ Active
**Category**: Core - Communication
**Purpose**: Sends emails using an SMTP email server.

#### Authentication:
- **SMTP Credential**: Required to connect to an SMTP server.

#### Operations:
**1. Send**: Send an email.
**2. Send and Wait for Response**: Send an email and pause the workflow execution until the user submits a response.

#### Core Parameters:
- **From Email**: The email address to send the email from.
- **To Email**: The email address to send the email to.
- **Subject**: The subject line for the email.
- **Email Format**:
    - **Text**: Send the email in plain-text format.
    - **HTML**: Send the email in HTML format.
    - **Both**: Send the email in both formats.

#### Node Options:
- **Append n8n Attribution**: Include "This email was sent automatically with n8n" at the end of the email.
- **Attachments**: Attach files to the email.
- **CC Email**: Send a carbon copy of the email.
- **BCC Email**: Send a blind carbon copy of the email.
- **Ignore SSL Issues**: Choose whether to ignore SSL/TLS verification.
- **Reply To**: Set a different reply-to email address.

#### Use Cases:
- **Notifications**: Send email notifications for workflow events.
- **Reporting**: Email reports generated by a workflow.
- **User Communication**: Send emails to users as part of a business process.
- **Marketing**: Send marketing emails to a list of subscribers.
52. Sort (to be documented)
53. Split Out (to be documented)
54. SSE Trigger (to be documented)
55. SSH (to be documented)
56. Stop And Error (to be documented)
57. Summarize (to be documented)
58. Switch (to be documented)
59. TOTP (to be documented)
60. Wait (to be documented)
61. Webhook (to be documented)
62. Workflow Trigger (to be documented)
63. XML (to be documented)

---

*Note: This documentation is being systematically expanded to include all core nodes with comprehensive details.*
